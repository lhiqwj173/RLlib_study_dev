{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-18 10:39:20,773\tWARNING ppo.py:295 -- You are running PPO on the new API stack! This is the new default behavior for this algorithm. If you don't want to use the new API stack, set `config.api_stack(enable_rl_module_and_learner=False,enable_env_runner_and_connector_v2=False)`. For a detailed migration guide, see here: https://docs.ray.io/en/master/rllib/new-api-stack-migration-guide.html\n",
      "d:\\programs\\miniconda3\\Lib\\site-packages\\ray\\rllib\\algorithms\\algorithm.py:569: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "`UnifiedLogger` will be removed in Ray 2.7.\n",
      "  return UnifiedLogger(config, logdir, loggers=None)\n",
      "d:\\programs\\miniconda3\\Lib\\site-packages\\ray\\tune\\logger\\unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "d:\\programs\\miniconda3\\Lib\\site-packages\\ray\\tune\\logger\\unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "d:\\programs\\miniconda3\\Lib\\site-packages\\ray\\tune\\logger\\unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "2025-03-18 10:39:25,279\tINFO worker.py:1821 -- Started a local Ray instance.\n",
      "\u001b[36m(SingleAgentEnvRunner pid=25020)\u001b[0m 2025-03-18 10:39:34,493\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
      "2025-03-18 10:39:35,007\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
      "2025-03-18 10:39:35,060\tWARNING ppo.py:295 -- You are running PPO on the new API stack! This is the new default behavior for this algorithm. If you don't want to use the new API stack, set `config.api_stack(enable_rl_module_and_learner=False,enable_env_runner_and_connector_v2=False)`. For a detailed migration guide, see here: https://docs.ray.io/en/master/rllib/new-api-stack-migration-guide.html\n",
      "2025-03-18 10:39:35,201\tWARNING rl_module.py:427 -- Could not create a Catalog object for your RLModule! If you are not using the new API stack yet, make sure to switch it off in your config: `config.api_stack(enable_rl_module_and_learner=False, enable_env_runner_and_connector_v2=False)`. Some algos already use the new stack by default. Ignore this message, if your RLModule does not use a Catalog to build its sub-components.\n",
      "2025-03-18 10:39:37,061\tINFO trainable.py:161 -- Trainable.setup took 16.238 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2025-03-18 10:39:37,065\tWARNING util.py:61 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(SingleAgentEnvRunner pid=25020)\u001b[0m on_episode_start: []\n",
      "\u001b[36m(SingleAgentEnvRunner pid=25020)\u001b[0m on_episode_start: []\n",
      "\u001b[36m(SingleAgentEnvRunner pid=25020)\u001b[0m on_episode_start: []\n",
      "\u001b[36m(SingleAgentEnvRunner pid=25020)\u001b[0m on_episode_start: []\n",
      "\u001b[36m(SingleAgentEnvRunner pid=25020)\u001b[0m on_episode_start: []\n",
      "\u001b[36m(SingleAgentEnvRunner pid=25020)\u001b[0m on_episode_start: []\n",
      "\u001b[36m(SingleAgentEnvRunner pid=25020)\u001b[0m on_episode_start: []\u001b[32m [repeated 360x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'timers': {'training_iteration': 25.57689875600394,\n",
       "  'restore_workers': 2.288194140419364e-05,\n",
       "  'training_step': 25.57650632005534,\n",
       "  'env_runner_sampling_timer': 3.7668462620675562,\n",
       "  'learner_update_timer': 21.794673148978035,\n",
       "  'synch_weights': 0.012533215032890439,\n",
       "  'synch_env_connectors': 0.013212673020316288},\n",
       " 'env_runners': {'num_agent_steps_sampled': {'default_agent': 4000},\n",
       "  'custom_episode_return_mean': 19.55495634473576,\n",
       "  'num_env_steps_sampled': 4000,\n",
       "  'episode_len_min': 10,\n",
       "  'agent_episode_returns_mean': {'default_agent': 36.56},\n",
       "  'weights_seq_no': 1.0,\n",
       "  'episode_len_mean': 36.56,\n",
       "  'episode_return_max': 108.0,\n",
       "  'custom_episode_return_min': 7.0,\n",
       "  'episode_return_mean': 36.56,\n",
       "  'num_module_steps_sampled_lifetime': {'default_policy': 8000},\n",
       "  'num_episodes': 110,\n",
       "  'num_agent_steps_sampled_lifetime': {'default_agent': 8000},\n",
       "  'module_episode_returns_mean': {'default_policy': 36.56},\n",
       "  'episode_return_min': 10.0,\n",
       "  'num_module_steps_sampled': {'default_policy': 4000},\n",
       "  'custom_episode_return_max': 108.0,\n",
       "  'episode_len_max': 108,\n",
       "  'num_env_steps_sampled_lifetime': 8000,\n",
       "  'episode_duration_sec_mean': 0.06930243200506084,\n",
       "  'custom_num_episodes': 290,\n",
       "  'sample': 3.655459398384804,\n",
       "  'num_episodes_lifetime': 290,\n",
       "  'time_between_sampling': 22.001654199964833},\n",
       " 'learners': {'__all_modules__': {'num_module_steps_trained': 4112,\n",
       "   'num_module_steps_trained_lifetime': 8294,\n",
       "   'learner_connector_timer': 4.212630487085378,\n",
       "   'num_env_steps_trained': 4112,\n",
       "   'num_trainable_parameters': 134915.0,\n",
       "   'num_env_steps_trained_lifetime': 8294,\n",
       "   'num_non_trainable_parameters': 0.0},\n",
       "  'default_policy': {'num_module_steps_trained': 4112,\n",
       "   'num_module_steps_trained_lifetime': 8294,\n",
       "   'diff_num_grad_updates_vs_sampler_policy': 0.0,\n",
       "   'entropy': 0.6200199723243713,\n",
       "   'vf_loss_unclipped': 305.1197814941406,\n",
       "   'curr_entropy_coeff': 0.0,\n",
       "   'policy_loss': -0.04922359436750412,\n",
       "   'gradients_default_optimizer_global_norm': 1.2379571199417114,\n",
       "   'curr_kl_coeff': 0.30000001192092896,\n",
       "   'total_loss': 8.307408332824707,\n",
       "   'default_optimizer_learning_rate': 5e-05,\n",
       "   'num_trainable_parameters': 134915.0,\n",
       "   'vf_loss': 8.35229206085205,\n",
       "   'num_non_trainable_parameters': 0.0,\n",
       "   'module_train_batch_size_mean': 4181.993,\n",
       "   'mean_kl_loss': 0.021698689088225365,\n",
       "   'weights_seq_no': 2.0,\n",
       "   'vf_explained_var': 0.08399474620819092}},\n",
       " 'num_training_step_calls_per_iteration': 1,\n",
       " 'num_env_steps_sampled_lifetime': 8000,\n",
       " 'fault_tolerance': {'num_healthy_workers': 2,\n",
       "  'num_remote_worker_restarts': 0},\n",
       " 'env_runner_group': {'actor_manager_num_outstanding_async_reqs': 0},\n",
       " 'done': False,\n",
       " 'training_iteration': 2,\n",
       " 'trial_id': 'default',\n",
       " 'date': '2025-03-18_10-40-24',\n",
       " 'timestamp': 1742265624,\n",
       " 'time_this_iter_s': 21.929205417633057,\n",
       " 'time_total_s': 47.56241965293884,\n",
       " 'pid': 19884,\n",
       " 'hostname': 'HASEE',\n",
       " 'node_ip': '127.0.0.1',\n",
       " 'config': {'exploration_config': {},\n",
       "  'extra_python_environs_for_driver': {},\n",
       "  'extra_python_environs_for_worker': {},\n",
       "  'placement_strategy': 'PACK',\n",
       "  'num_gpus': 0,\n",
       "  '_fake_gpus': False,\n",
       "  'num_cpus_for_main_process': 1,\n",
       "  'eager_tracing': True,\n",
       "  'eager_max_retraces': 20,\n",
       "  'tf_session_args': {'intra_op_parallelism_threads': 2,\n",
       "   'inter_op_parallelism_threads': 2,\n",
       "   'gpu_options': {'allow_growth': True},\n",
       "   'log_device_placement': False,\n",
       "   'device_count': {'CPU': 1},\n",
       "   'allow_soft_placement': True},\n",
       "  'local_tf_session_args': {'intra_op_parallelism_threads': 8,\n",
       "   'inter_op_parallelism_threads': 8},\n",
       "  'torch_compile_learner': False,\n",
       "  'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>,\n",
       "  'torch_compile_learner_dynamo_backend': 'inductor',\n",
       "  'torch_compile_learner_dynamo_mode': None,\n",
       "  'torch_compile_worker': False,\n",
       "  'torch_compile_worker_dynamo_backend': 'onnxrt',\n",
       "  'torch_compile_worker_dynamo_mode': None,\n",
       "  'torch_ddp_kwargs': {},\n",
       "  'torch_skip_nan_gradients': False,\n",
       "  'env': 'custom_cartpole',\n",
       "  'env_config': {},\n",
       "  'observation_space': None,\n",
       "  'action_space': None,\n",
       "  'clip_rewards': None,\n",
       "  'normalize_actions': True,\n",
       "  'clip_actions': False,\n",
       "  '_is_atari': None,\n",
       "  'disable_env_checking': False,\n",
       "  'env_task_fn': None,\n",
       "  'render_env': False,\n",
       "  'action_mask_key': 'action_mask',\n",
       "  'env_runner_cls': None,\n",
       "  'num_env_runners': 2,\n",
       "  'num_envs_per_env_runner': 1,\n",
       "  'num_cpus_per_env_runner': 1,\n",
       "  'num_gpus_per_env_runner': 0,\n",
       "  'custom_resources_per_env_runner': {},\n",
       "  'validate_env_runners_after_construction': True,\n",
       "  'max_requests_in_flight_per_env_runner': 1,\n",
       "  'sample_timeout_s': 60.0,\n",
       "  '_env_to_module_connector': None,\n",
       "  'add_default_connectors_to_env_to_module_pipeline': True,\n",
       "  '_module_to_env_connector': None,\n",
       "  'add_default_connectors_to_module_to_env_pipeline': True,\n",
       "  'episode_lookback_horizon': 1,\n",
       "  'rollout_fragment_length': 'auto',\n",
       "  'batch_mode': 'truncate_episodes',\n",
       "  'compress_observations': False,\n",
       "  'remote_worker_envs': False,\n",
       "  'remote_env_batch_wait_ms': 0,\n",
       "  'enable_tf1_exec_eagerly': False,\n",
       "  'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
       "  'preprocessor_pref': 'deepmind',\n",
       "  'observation_filter': 'NoFilter',\n",
       "  'update_worker_filter_stats': True,\n",
       "  'use_worker_filter_stats': True,\n",
       "  'sampler_perf_stats_ema_coef': None,\n",
       "  'num_learners': 0,\n",
       "  'num_gpus_per_learner': 0,\n",
       "  'num_cpus_per_learner': 1,\n",
       "  'local_gpu_idx': 0,\n",
       "  'max_requests_in_flight_per_learner': 3,\n",
       "  'gamma': 0.99,\n",
       "  'lr': 5e-05,\n",
       "  'grad_clip': None,\n",
       "  'grad_clip_by': 'global_norm',\n",
       "  'train_batch_size_per_learner': None,\n",
       "  'train_batch_size': 4000,\n",
       "  'num_epochs': 30,\n",
       "  'minibatch_size': 128,\n",
       "  'shuffle_batch_per_epoch': True,\n",
       "  'model': {'fcnet_hiddens': [256, 256],\n",
       "   'fcnet_activation': 'tanh',\n",
       "   'fcnet_weights_initializer': None,\n",
       "   'fcnet_weights_initializer_config': None,\n",
       "   'fcnet_bias_initializer': None,\n",
       "   'fcnet_bias_initializer_config': None,\n",
       "   'conv_filters': None,\n",
       "   'conv_activation': 'relu',\n",
       "   'conv_kernel_initializer': None,\n",
       "   'conv_kernel_initializer_config': None,\n",
       "   'conv_bias_initializer': None,\n",
       "   'conv_bias_initializer_config': None,\n",
       "   'conv_transpose_kernel_initializer': None,\n",
       "   'conv_transpose_kernel_initializer_config': None,\n",
       "   'conv_transpose_bias_initializer': None,\n",
       "   'conv_transpose_bias_initializer_config': None,\n",
       "   'post_fcnet_hiddens': [],\n",
       "   'post_fcnet_activation': 'relu',\n",
       "   'post_fcnet_weights_initializer': None,\n",
       "   'post_fcnet_weights_initializer_config': None,\n",
       "   'post_fcnet_bias_initializer': None,\n",
       "   'post_fcnet_bias_initializer_config': None,\n",
       "   'free_log_std': False,\n",
       "   'log_std_clip_param': 20.0,\n",
       "   'no_final_linear': False,\n",
       "   'vf_share_layers': False,\n",
       "   'use_lstm': False,\n",
       "   'max_seq_len': 20,\n",
       "   'lstm_cell_size': 256,\n",
       "   'lstm_use_prev_action': False,\n",
       "   'lstm_use_prev_reward': False,\n",
       "   'lstm_weights_initializer': None,\n",
       "   'lstm_weights_initializer_config': None,\n",
       "   'lstm_bias_initializer': None,\n",
       "   'lstm_bias_initializer_config': None,\n",
       "   '_time_major': False,\n",
       "   'use_attention': False,\n",
       "   'attention_num_transformer_units': 1,\n",
       "   'attention_dim': 64,\n",
       "   'attention_num_heads': 1,\n",
       "   'attention_head_dim': 32,\n",
       "   'attention_memory_inference': 50,\n",
       "   'attention_memory_training': 50,\n",
       "   'attention_position_wise_mlp_dim': 32,\n",
       "   'attention_init_gru_gate_bias': 2.0,\n",
       "   'attention_use_n_prev_actions': 0,\n",
       "   'attention_use_n_prev_rewards': 0,\n",
       "   'framestack': True,\n",
       "   'dim': 84,\n",
       "   'grayscale': False,\n",
       "   'zero_mean': True,\n",
       "   'custom_model': None,\n",
       "   'custom_model_config': {},\n",
       "   'custom_action_dist': None,\n",
       "   'custom_preprocessor': None,\n",
       "   'encoder_latent_dim': None,\n",
       "   'always_check_shapes': False,\n",
       "   'lstm_use_prev_action_reward': -1,\n",
       "   '_use_default_native_models': -1,\n",
       "   '_disable_preprocessor_api': False,\n",
       "   '_disable_action_flattening': False},\n",
       "  '_learner_connector': None,\n",
       "  'add_default_connectors_to_learner_pipeline': True,\n",
       "  'learner_config_dict': {},\n",
       "  'optimizer': {},\n",
       "  '_learner_class': None,\n",
       "  'explore': True,\n",
       "  'enable_rl_module_and_learner': True,\n",
       "  'enable_env_runner_and_connector_v2': True,\n",
       "  '_prior_exploration_config': {'type': 'StochasticSampling'},\n",
       "  'count_steps_by': 'env_steps',\n",
       "  'policy_map_capacity': 100,\n",
       "  'policy_mapping_fn': <function ray.rllib.algorithms.algorithm_config.AlgorithmConfig.DEFAULT_POLICY_MAPPING_FN(aid, episode, worker, **kwargs)>,\n",
       "  'policies_to_train': None,\n",
       "  'policy_states_are_swappable': False,\n",
       "  'observation_fn': None,\n",
       "  'input_read_method': 'read_parquet',\n",
       "  'input_read_method_kwargs': {},\n",
       "  'input_read_schema': {},\n",
       "  'input_read_episodes': False,\n",
       "  'input_read_sample_batches': False,\n",
       "  'input_read_batch_size': None,\n",
       "  'input_filesystem': None,\n",
       "  'input_filesystem_kwargs': {},\n",
       "  'input_compress_columns': ['obs', 'new_obs'],\n",
       "  'input_spaces_jsonable': True,\n",
       "  'materialize_data': False,\n",
       "  'materialize_mapped_data': True,\n",
       "  'map_batches_kwargs': {},\n",
       "  'iter_batches_kwargs': {},\n",
       "  'prelearner_class': None,\n",
       "  'prelearner_buffer_class': None,\n",
       "  'prelearner_buffer_kwargs': {},\n",
       "  'prelearner_module_synch_period': 10,\n",
       "  'dataset_num_iters_per_learner': None,\n",
       "  'input_config': {},\n",
       "  'actions_in_input_normalized': False,\n",
       "  'postprocess_inputs': False,\n",
       "  'shuffle_buffer_size': 0,\n",
       "  'output': None,\n",
       "  'output_config': {},\n",
       "  'output_compress_columns': ['obs', 'new_obs'],\n",
       "  'output_max_file_size': 67108864,\n",
       "  'output_max_rows_per_file': None,\n",
       "  'output_write_method': 'write_parquet',\n",
       "  'output_write_method_kwargs': {},\n",
       "  'output_filesystem': None,\n",
       "  'output_filesystem_kwargs': {},\n",
       "  'output_write_episodes': True,\n",
       "  'offline_sampling': False,\n",
       "  'evaluation_interval': None,\n",
       "  'evaluation_duration': 10,\n",
       "  'evaluation_duration_unit': 'episodes',\n",
       "  'evaluation_sample_timeout_s': 120.0,\n",
       "  'evaluation_parallel_to_training': False,\n",
       "  'evaluation_force_reset_envs_before_iteration': True,\n",
       "  'evaluation_config': None,\n",
       "  'off_policy_estimation_methods': {},\n",
       "  'ope_split_batch_by_episode': True,\n",
       "  'evaluation_num_env_runners': 0,\n",
       "  'in_evaluation': False,\n",
       "  'sync_filters_on_rollout_workers_timeout_s': 10.0,\n",
       "  'keep_per_episode_custom_metrics': False,\n",
       "  'metrics_episode_collection_timeout_s': 60.0,\n",
       "  'metrics_num_episodes_for_smoothing': 100,\n",
       "  'min_time_s_per_iteration': None,\n",
       "  'min_train_timesteps_per_iteration': 0,\n",
       "  'min_sample_timesteps_per_iteration': 0,\n",
       "  'log_gradients': True,\n",
       "  'export_native_model_files': False,\n",
       "  'checkpoint_trainable_policies_only': False,\n",
       "  'logger_creator': None,\n",
       "  'logger_config': None,\n",
       "  'log_level': 'WARN',\n",
       "  'log_sys_usage': True,\n",
       "  'fake_sampler': False,\n",
       "  'seed': None,\n",
       "  '_run_training_always_in_thread': False,\n",
       "  '_evaluation_parallel_to_training_wo_thread': False,\n",
       "  'restart_failed_env_runners': True,\n",
       "  'ignore_env_runner_failures': False,\n",
       "  'max_num_env_runner_restarts': 1000,\n",
       "  'delay_between_env_runner_restarts_s': 60.0,\n",
       "  'restart_failed_sub_environments': False,\n",
       "  'num_consecutive_env_runner_failures_tolerance': 100,\n",
       "  'env_runner_health_probe_timeout_s': 30.0,\n",
       "  'env_runner_restore_timeout_s': 1800.0,\n",
       "  '_model_config': {},\n",
       "  '_rl_module_spec': None,\n",
       "  'algorithm_config_overrides_per_module': {},\n",
       "  '_per_module_overrides': {},\n",
       "  '_torch_grad_scaler_class': None,\n",
       "  '_torch_lr_scheduler_classes': None,\n",
       "  '_tf_policy_handles_more_than_one_loss': False,\n",
       "  '_disable_preprocessor_api': False,\n",
       "  '_disable_action_flattening': False,\n",
       "  '_disable_initialize_loss_from_dummy_batch': False,\n",
       "  '_dont_auto_sync_env_runner_states': False,\n",
       "  'enable_connectors': -1,\n",
       "  'simple_optimizer': False,\n",
       "  'policy_map_cache': -1,\n",
       "  'worker_cls': -1,\n",
       "  'synchronize_filters': -1,\n",
       "  'enable_async_evaluation': -1,\n",
       "  'custom_async_evaluation_function': -1,\n",
       "  '_enable_rl_module_api': -1,\n",
       "  'auto_wrap_old_gym_envs': -1,\n",
       "  'always_attach_evaluation_results': -1,\n",
       "  'replay_sequence_length': None,\n",
       "  '_disable_execution_plan_api': -1,\n",
       "  'use_critic': True,\n",
       "  'use_gae': True,\n",
       "  'use_kl_loss': True,\n",
       "  'kl_coeff': 0.2,\n",
       "  'kl_target': 0.01,\n",
       "  'vf_loss_coeff': 1.0,\n",
       "  'entropy_coeff': 0.0,\n",
       "  'clip_param': 0.3,\n",
       "  'vf_clip_param': 10.0,\n",
       "  'entropy_coeff_schedule': None,\n",
       "  'lr_schedule': None,\n",
       "  'sgd_minibatch_size': -1,\n",
       "  'vf_share_layers': -1,\n",
       "  'class': ray.rllib.algorithms.ppo.ppo.PPOConfig,\n",
       "  'lambda': 1.0,\n",
       "  'input': 'sampler',\n",
       "  'policies': {'default_policy': (None, None, None, None)},\n",
       "  'callbacks': __main__.CustomMetricsCallback,\n",
       "  'create_env_on_driver': False,\n",
       "  'custom_eval_function': None,\n",
       "  'framework': 'torch'},\n",
       " 'time_since_restore': 47.56241965293884,\n",
       " 'iterations_since_restore': 2,\n",
       " 'perf': {'cpu_util_percent': 90.3483870967742,\n",
       "  'ram_util_percent': 85.05161290322582},\n",
       " 'custom_metrics': {'custom_episode_return_mean': 19.55495634473576,\n",
       "  'custom_num_episodes': 290,\n",
       "  'custom_episode_return_max': 108.0,\n",
       "  'custom_episode_return_min': 7.0}}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import gymnasium as gym\n",
    "import ale_py\n",
    "gym.register_envs(ale_py)\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.rllib.algorithms.callbacks import DefaultCallbacks\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# 1. 创建自定义环境，在info中返回指标\n",
    "class CustomCartPole(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "        \n",
    "        # 在info中添加自定义指标\n",
    "        info[\"reward\"] = reward\n",
    "\n",
    "        if terminated or truncated:\n",
    "            info[\"episode_done\"] = 1\n",
    "        \n",
    "        return obs, reward, terminated, truncated, info\n",
    "\n",
    "# 2. 创建自定义回调函数，注意参数签名\n",
    "class CustomMetricsCallback(DefaultCallbacks):\n",
    "    def on_episode_step(\n",
    "        self,\n",
    "        *,\n",
    "        episode,\n",
    "        env_runner=None,\n",
    "        metrics_logger=None,\n",
    "        env=None,\n",
    "        env_index,\n",
    "        rl_module=None,\n",
    "        worker=None,\n",
    "        base_env=None,\n",
    "        policies=None,\n",
    "        **kwargs,\n",
    "    ) -> None:\n",
    "        # 获取最新的info\n",
    "        info = episode.get_infos(-1)\n",
    "        episode.add_temporary_timestep_data(\"reward\", info[\"reward\"])\n",
    "\n",
    "        if 'episode_done' in info:\n",
    "            episode.add_temporary_timestep_data(\"episode\", 1)\n",
    "\n",
    "    def on_episode_start(self, *, episode, metrics_logger, **kwargs):\n",
    "        rewards = episode.get_temporary_timestep_data(\"reward\")\n",
    "        episodes = episode.get_temporary_timestep_data(\"episode\")\n",
    "        print(f'on_episode_start: {rewards}')\n",
    "        print(f'on_episode_start: {episodes}')\n",
    "\n",
    "    def on_episode_end(self, *, episode, metrics_logger, **kwargs):\n",
    "        rewards = episode.get_temporary_timestep_data(\"reward\")\n",
    "        episodes = episode.get_temporary_timestep_data(\"episode\")\n",
    "        metrics_logger.log_value(\n",
    "            \"custom_episode_return_mean\", np.sum(rewards)\n",
    "        )\n",
    "\n",
    "        metrics_logger.log_value(\n",
    "            \"custom_episode_return_max\", np.sum(rewards), reduce='max'\n",
    "        )\n",
    "\n",
    "        metrics_logger.log_value(\n",
    "            \"custom_episode_return_min\", np.sum(rewards), reduce='min'\n",
    "        )\n",
    "\n",
    "        assert np.sum(episodes) == 1, f'episodes: {np.sum(episodes)}'\n",
    "        metrics_logger.log_value(\n",
    "            \"custom_num_episodes\", np.sum(episodes), reduce='sum'\n",
    "        )\n",
    "\n",
    "    def on_train_result(\n",
    "        self, *, algorithm, result, metrics_logger, **kwargs\n",
    "    ):\n",
    "        result.setdefault(\"custom_metrics\", {})\n",
    "        result[\"custom_metrics\"][\"custom_episode_return_mean\"] = result[\"env_runners\"][\"custom_episode_return_mean\"]\n",
    "        result[\"custom_metrics\"][\"custom_num_episodes\"] = result[\"env_runners\"][\"custom_num_episodes\"]\n",
    "        result[\"custom_metrics\"][\"custom_episode_return_max\"] = result[\"env_runners\"][\"custom_episode_return_max\"]\n",
    "        result[\"custom_metrics\"][\"custom_episode_return_min\"] = result[\"env_runners\"][\"custom_episode_return_min\"]\n",
    "\n",
    "# 3. 配置\n",
    "def env_creator(env_config):\n",
    "    env = gym.make(\"CartPole-v1\")\n",
    "    return CustomCartPole(env)\n",
    "\n",
    "from ray.tune.registry import register_env\n",
    "register_env(\"custom_cartpole\", env_creator)\n",
    "\n",
    "config = (\n",
    "    PPOConfig()\n",
    "    .api_stack(\n",
    "        enable_rl_module_and_learner=True,\n",
    "        enable_env_runner_and_connector_v2=True,\n",
    "    )\n",
    "    .environment(\"custom_cartpole\")\n",
    "    .callbacks(CustomMetricsCallback)\n",
    ")\n",
    "\n",
    "# 4. 训练并查看结果\n",
    "from ray.rllib.algorithms.ppo import PPO\n",
    "\n",
    "algo = PPO(config=config)\n",
    "\n",
    "algo.train()\n",
    "result = algo.train()\n",
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
