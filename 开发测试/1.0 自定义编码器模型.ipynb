{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 准备环境 pip install ale_py gymnasium[accept-rom-license,atari]==1.0.0\n",
    "# !wget https://raw.githubusercontent.com/lhiqwj173/dl_helper/master/envs/rl.py > /dev/null 2>&1\n",
    "# !python rl.py not_install_dl_helper > /dev/null 2>&1\n",
    "# !pip install /kaggle/working/3rd/dl_helper > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ray 版本: 2.40.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import ale_py\n",
    "gym.register_envs(ale_py)\n",
    "import ray\n",
    "print(\"ray 版本:\", ray.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RLlib 中的模型的生成逻辑\n",
    "1. 生成配置对象 config \n",
    "2. 根据配置生成模型 config.build()\n",
    "\n",
    "## 模型类型\n",
    "- PPO(通用IMPALA/APPO)\n",
    "- DQN(以及变种)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 相关类\n",
    "from ray.rllib.core.rl_module.rl_module import RLModule\n",
    "from ray.rllib.core.rl_module.torch import TorchRLModule\n",
    "from ray.rllib.algorithms.ppo.ppo_rl_module import PPORLModule\n",
    "from ray.rllib.algorithms.ppo.torch.ppo_torch_rl_module import PPOTorchRLModule\n",
    "# API 类\n",
    "from ray.rllib.core.rl_module.apis import InferenceOnlyAPI, ValueFunctionAPI\n",
    "# 配置类，用于生成模型\n",
    "from ray.rllib.algorithms.ppo.ppo_catalog import PPOCatalog\n",
    "from ray.rllib.core.models.catalog import Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.ppo.ppo_catalog import PPOCatalog\n",
    "from ray.rllib.core.models.catalog import Catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Catalog\n",
    "配置的基类，通用所有的强化学习算法  \n",
    "主要负责通用的编码器生成\n",
    "- _get_encoder_config方法生成编码器配置\n",
    "    会自动针对输入的维度选择编码器\n",
    "    - use_lstm > RecurrentEncoderConfig\n",
    "    - 1D-Box > MLPEncoderConfig\n",
    "    - 3D-Box > CNNEncoderConfig\n",
    "- build_encoder方法生成编码器\n",
    "\n",
    "自动生成的编码器涵盖 MLP/CNN/LSTM  \n",
    "基本上都可以通过配置调整满足需求,如下示意  \n",
    "详细的配置参考 Catalog._get_encoder_config 方法中对配置字段的使用  \n",
    "（或官方文档 https://docs.ray.io/en/latest/rllib/rllib-catalogs.html MODEL_DEFAULTS）\n",
    "\n",
    "```\n",
    "config.rl_module(\n",
    "    model_config={\n",
    "        # MLPEncoderConfig\n",
    "        \"fcnet_hiddens\": [5, 3, 3],\n",
    "        \"fcnet_kernel_initializer\": None,\n",
    "        \"fcnet_kernel_initializer_kwargs\": {},\n",
    "        \"fcnet_bias_initializer\": None,\n",
    "        \"fcnet_bias_initializer_kwargs\": {},\n",
    "\n",
    "        # CNNEncoderConfig\n",
    "        \"conv_filters\": [\n",
    "            [32, [8, 8], 4],  # [输出通道数, [kernel_size_h, kernel_size_w], stride]\n",
    "            [64, [4, 4], 2],  # [64个通道, 4x4卷积核, stride=2]\n",
    "            [64, [3, 3], 1],  # [64个通道, 3x3卷积核, stride=1] \n",
    "        ],\n",
    "\n",
    "        # RecurrentEncoderConfig\n",
    "        ...\n",
    "    },\n",
    ")\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 相关源码（截取）\n",
    "class Catalog:\n",
    "    \"\"\"描述用于 RL 模块的子模块架构。\n",
    "\n",
    "    RLlib 的原生 RL 模块从 Catalog 对象获取其模型。\n",
    "    默认情况下，该 Catalog 会构建其作为属性拥有的配置。\n",
    "    此组件被构建为可hack和可扩展的。您可以通过重写此类的 `build_xxx` 方法，\n",
    "    将自定义组件注入到 RL 模块中。\n",
    "    请注意，建议为单个用例编写自定义 RL 模块。\n",
    "    对 Catalog 的修改主要在您想要为不同的 RL 模块重用相同的 Catalog 时才有意义。\n",
    "    例如，如果您编写了一个自定义编码器并希望将其注入到不同的 RL 模块\n",
    "    （例如，PPO、DQN 等）。您可以通过修改\n",
    "    `Catalog._determine_components_hook` 来影响决定子组件的决策树。\n",
    "\n",
    "    使用示例：\n",
    "\n",
    "    # 定义一个自定义的 catalog\n",
    "\n",
    "    .. testcode::\n",
    "\n",
    "        import torch\n",
    "        import gymnasium as gym\n",
    "        from ray.rllib.core.models.configs import MLPHeadConfig\n",
    "        from ray.rllib.core.models.catalog import Catalog\n",
    "\n",
    "        class MyCatalog(Catalog):\n",
    "            def __init__(\n",
    "                self,\n",
    "                observation_space: gym.Space,\n",
    "                action_space: gym.Space,\n",
    "                model_config_dict: dict,\n",
    "            ):\n",
    "                super().__init__(observation_space, action_space, model_config_dict)\n",
    "                self.my_model_config = MLPHeadConfig(\n",
    "                    hidden_layer_dims=[64, 32],\n",
    "                    input_dims=[self.observation_space.shape[0]],\n",
    "                )\n",
    "\n",
    "            def build_my_head(self, framework: str):\n",
    "                return self.my_model_config.build(framework=framework)\n",
    "\n",
    "        # 有了这个，RLlib 可以像这样从这个 catalog 构建和使用模型：\n",
    "        catalog = MyCatalog(gym.spaces.Box(0, 1), gym.spaces.Box(0, 1), {})\n",
    "        my_head = catalog.build_my_head(framework=\"torch\")\n",
    "\n",
    "        # 对构建的模型进行调用。\n",
    "        out = my_head(torch.Tensor([[1]]))\n",
    "    \"\"\"\n",
    "    @OverrideToImplementCustomLogic_CallToSuperRecommended\n",
    "    def _determine_components_hook(self):\n",
    "        \"\"\"Decision tree hook for subclasses to override.\n",
    "\n",
    "        By default, this method executes the decision tree that determines the\n",
    "        components that a Catalog builds. You can extend the components by overriding\n",
    "        this or by adding to the constructor of your subclass.\n",
    "\n",
    "        Override this method if you don't want to use the default components\n",
    "        determined here. If you want to use them but add additional components, you\n",
    "        should call `super()._determine_components()` at the beginning of your\n",
    "        implementation.\n",
    "\n",
    "        This makes it so that subclasses are not forced to create an encoder config\n",
    "        if the rest of their catalog is not dependent on it or if it breaks.\n",
    "        At the end of this method, an attribute `Catalog.latent_dims`\n",
    "        should be set so that heads can be built using that information.\n",
    "        \"\"\"\n",
    "        self._encoder_config = self._get_encoder_config(\n",
    "            observation_space=self.observation_space,\n",
    "            action_space=self.action_space,\n",
    "            model_config_dict=self._model_config_dict,\n",
    "        )\n",
    "\n",
    "        # Create a function that can be called when framework is known to retrieve the\n",
    "        # class type for action distributions\n",
    "        self._action_dist_class_fn = functools.partial(\n",
    "            self._get_dist_cls_from_action_space, action_space=self.action_space\n",
    "        )\n",
    "\n",
    "        # The dimensions of the latent vector that is output by the encoder and fed\n",
    "        # to the heads.\n",
    "        self.latent_dims = self._encoder_config.output_dims\n",
    "\n",
    "    @OverrideToImplementCustomLogic\n",
    "    def build_encoder(self, framework: str) -> Encoder:\n",
    "        \"\"\"Builds the encoder.\n",
    "\n",
    "        By default, this method builds an encoder instance from Catalog._encoder_config.\n",
    "\n",
    "        You should override this if you want to use RLlib's default RL Modules but\n",
    "        only want to change the encoder. For example, if you want to use a custom\n",
    "        encoder, but want to use RLlib's default heads, action distribution and how\n",
    "        tensors are routed between them. If you want to have full control over the\n",
    "        RL Module, we recommend writing your own RL Module by inheriting from one of\n",
    "        RLlib's RL Modules instead.\n",
    "\n",
    "        Args:\n",
    "            framework: The framework to use. Either \"torch\" or \"tf2\".\n",
    "\n",
    "        Returns:\n",
    "            The encoder.\n",
    "        \"\"\"\n",
    "        \"\"\"构建编码器。\n",
    "\n",
    "        默认情况下，此方法从 Catalog._encoder_config 构建一个编码器实例。\n",
    "\n",
    "        如果您想使用 RLlib 的默认 RL 模块，但只想更改编码器，则应重写此方法。\n",
    "        例如，如果您想使用自定义编码器，但想使用 RLlib 的默认头、动作分布以及张量如何在它们之间路由。\n",
    "        如果您想完全控制 RL 模块，\n",
    "        我们建议您通过继承 RLlib 的 RL 模块之一来编写自己的 RL 模块。\n",
    "\n",
    "        Args:\n",
    "        framework: 要使用的框架。可以是 \"torch\" 或 \"tf2\"。\n",
    "\n",
    "        Returns:\n",
    "        编码器。\n",
    "        \"\"\"\n",
    "        assert hasattr(self, \"_encoder_config\"), (\n",
    "            \"You must define a `Catalog._encoder_config` attribute in your Catalog \"\n",
    "            \"subclass or override the `Catalog.build_encoder` method. By default, \"\n",
    "            \"an encoder_config is created in the __post_init__ method.\"\n",
    "        )\n",
    "        return self._encoder_config.build(framework=framework)\n",
    "\n",
    "    @classmethod\n",
    "    def _get_encoder_config(\n",
    "        cls,\n",
    "        observation_space: gym.Space,\n",
    "        model_config_dict: dict,\n",
    "        action_space: gym.Space = None,\n",
    "    ) -> ModelConfig:\n",
    "        \"\"\"Returns an EncoderConfig for the given input_space and model_config_dict.\n",
    "\n",
    "        Encoders are usually used in RLModules to transform the input space into a\n",
    "        latent space that is then fed to the heads. The returned EncoderConfig\n",
    "        objects correspond to the built-in Encoder classes in RLlib.\n",
    "        For example, for a simple 1D-Box input_space, RLlib offers an\n",
    "        MLPEncoder, hence this method returns the MLPEncoderConfig. You can overwrite\n",
    "        this method to produce specific EncoderConfigs for your custom Models.\n",
    "\n",
    "        The following input spaces lead to the following configs:\n",
    "        - 1D-Box: MLPEncoderConfig\n",
    "        - 3D-Box: CNNEncoderConfig\n",
    "        # TODO (Artur): Support more spaces here\n",
    "        # ...\n",
    "\n",
    "        Args:\n",
    "            observation_space: The observation space to use.\n",
    "            model_config_dict: The model config to use.\n",
    "            action_space: The action space to use if actions are to be encoded. This\n",
    "                is commonly the case for LSTM models.\n",
    "\n",
    "        Returns:\n",
    "            The encoder config.\n",
    "        \"\"\"\n",
    "        \"\"\"返回给定 input_space 和 model_config_dict 的 EncoderConfig。\n",
    "\n",
    "        编码器通常在 RLModules 中使用，用于将输入空间转换为一个潜在空间，然后将其传递给头部。\n",
    "        返回的 EncoderConfig 对象对应于 RLlib 中的内置编码器类。\n",
    "        例如，对于一个简单的 1D-Box 输入空间，RLlib 提供了 MLPEncoder\n",
    "        ，因此此方法返回 MLPEncoderConfig。\n",
    "        您可以重写此方法以生成特定的 EncoderConfig 以用于您的自定义模型。\n",
    "\n",
    "        以下输入空间会导致以下配置：\n",
    "\n",
    "        1D-Box: MLPEncoderConfig\n",
    "        3D-Box: CNNEncoderConfig\n",
    "        TODO (Artur): 在此处支持更多空间\n",
    "        ...\n",
    "        Args:\n",
    "        observation_space: 要使用的观测空间。\n",
    "        model_config_dict: 要使用的模型配置。\n",
    "        action_space: 如果动作需要编码，则要使用的动作空间。这在 LSTM 模型的情况下通常是这样。\n",
    "\n",
    "        Returns:\n",
    "        编码器配置。\n",
    "        \"\"\"\n",
    "\n",
    "        activation = model_config_dict[\"fcnet_activation\"]\n",
    "        output_activation = model_config_dict[\"fcnet_activation\"]\n",
    "        use_lstm = model_config_dict[\"use_lstm\"]\n",
    "\n",
    "        if use_lstm:\n",
    "            encoder_config = RecurrentEncoderConfig(\n",
    "                input_dims=observation_space.shape,\n",
    "                recurrent_layer_type=\"lstm\",\n",
    "                hidden_dim=model_config_dict[\"lstm_cell_size\"],\n",
    "                hidden_weights_initializer=model_config_dict[\"lstm_kernel_initializer\"],\n",
    "                hidden_weights_initializer_config=model_config_dict[\n",
    "                    \"lstm_kernel_initializer_kwargs\"\n",
    "                ],\n",
    "                hidden_bias_initializer=model_config_dict[\"lstm_bias_initializer\"],\n",
    "                hidden_bias_initializer_config=model_config_dict[\n",
    "                    \"lstm_bias_initializer_kwargs\"\n",
    "                ],\n",
    "                batch_major=True,\n",
    "                num_layers=1,\n",
    "                tokenizer_config=cls.get_tokenizer_config(\n",
    "                    observation_space,\n",
    "                    model_config_dict,\n",
    "                ),\n",
    "            )\n",
    "        else:\n",
    "            # TODO (Artur): Maybe check for original spaces here\n",
    "            # input_space is a 1D Box\n",
    "            if isinstance(observation_space, Box) and len(observation_space.shape) == 1:\n",
    "                # In order to guarantee backward compatability with old configs,\n",
    "                # we need to check if no latent dim was set and simply reuse the last\n",
    "                # fcnet hidden dim for that purpose.\n",
    "                hidden_layer_dims = model_config_dict[\"fcnet_hiddens\"][:-1]\n",
    "                encoder_latent_dim = model_config_dict[\"fcnet_hiddens\"][-1]\n",
    "                encoder_config = MLPEncoderConfig(\n",
    "                    input_dims=observation_space.shape,\n",
    "                    hidden_layer_dims=hidden_layer_dims,\n",
    "                    hidden_layer_activation=activation,\n",
    "                    hidden_layer_weights_initializer=model_config_dict[\n",
    "                        \"fcnet_kernel_initializer\"\n",
    "                    ],\n",
    "                    hidden_layer_weights_initializer_config=model_config_dict[\n",
    "                        \"fcnet_kernel_initializer_kwargs\"\n",
    "                    ],\n",
    "                    hidden_layer_bias_initializer=model_config_dict[\n",
    "                        \"fcnet_bias_initializer\"\n",
    "                    ],\n",
    "                    hidden_layer_bias_initializer_config=model_config_dict[\n",
    "                        \"fcnet_bias_initializer_kwargs\"\n",
    "                    ],\n",
    "                    output_layer_dim=encoder_latent_dim,\n",
    "                    output_layer_activation=output_activation,\n",
    "                    output_layer_weights_initializer=model_config_dict[\n",
    "                        \"fcnet_kernel_initializer\"\n",
    "                    ],\n",
    "                    output_layer_weights_initializer_config=model_config_dict[\n",
    "                        \"fcnet_kernel_initializer_kwargs\"\n",
    "                    ],\n",
    "                    output_layer_bias_initializer=model_config_dict[\n",
    "                        \"fcnet_bias_initializer\"\n",
    "                    ],\n",
    "                    output_layer_bias_initializer_config=model_config_dict[\n",
    "                        \"fcnet_bias_initializer_kwargs\"\n",
    "                    ],\n",
    "                )\n",
    "\n",
    "            # input_space is a 3D Box\n",
    "            elif (\n",
    "                isinstance(observation_space, Box) and len(observation_space.shape) == 3\n",
    "            ):\n",
    "                if not model_config_dict.get(\"conv_filters\"):\n",
    "                    model_config_dict[\"conv_filters\"] = get_filter_config(\n",
    "                        observation_space.shape\n",
    "                    )\n",
    "\n",
    "                encoder_config = CNNEncoderConfig(\n",
    "                    input_dims=observation_space.shape,\n",
    "                    cnn_filter_specifiers=model_config_dict[\"conv_filters\"],\n",
    "                    cnn_activation=model_config_dict[\"conv_activation\"],\n",
    "                    cnn_kernel_initializer=model_config_dict[\"conv_kernel_initializer\"],\n",
    "                    cnn_kernel_initializer_config=model_config_dict[\n",
    "                        \"conv_kernel_initializer_kwargs\"\n",
    "                    ],\n",
    "                    cnn_bias_initializer=model_config_dict[\"conv_bias_initializer\"],\n",
    "                    cnn_bias_initializer_config=model_config_dict[\n",
    "                        \"conv_bias_initializer_kwargs\"\n",
    "                    ],\n",
    "                )\n",
    "            # input_space is a 2D Box\n",
    "            elif (\n",
    "                isinstance(observation_space, Box) and len(observation_space.shape) == 2\n",
    "            ):\n",
    "                # RLlib used to support 2D Box spaces by silently flattening them\n",
    "                raise ValueError(\n",
    "                    f\"No default encoder config for obs space={observation_space},\"\n",
    "                    f\" lstm={use_lstm} found. 2D Box \"\n",
    "                    f\"spaces are not supported. They should be either flattened to a \"\n",
    "                    f\"1D Box space or enhanced to be a 3D box space.\"\n",
    "                )\n",
    "            # input_space is a possibly nested structure of spaces.\n",
    "            else:\n",
    "                # NestedModelConfig\n",
    "                raise ValueError(\n",
    "                    f\"No default encoder config for obs space={observation_space},\"\n",
    "                    f\" lstm={use_lstm} found.\"\n",
    "                )\n",
    "\n",
    "        return encoder_config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 默认编码器使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TorchMLPEncoder(\n",
      "  (net): TorchMLP(\n",
      "    (mlp): Sequential(\n",
      "      (0): Linear(in_features=2, out_features=8, bias=False)\n",
      "      (1): LayerNorm((8,), eps=0.001, elementwise_affine=True)\n",
      "      (2): SiLU()\n",
      "      (3): Linear(in_features=8, out_features=8, bias=False)\n",
      "      (4): LayerNorm((8,), eps=0.001, elementwise_affine=True)\n",
      "      (5): SiLU()\n",
      "      (6): Linear(in_features=8, out_features=4, bias=False)\n",
      "      (7): Tanh()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "TorchCNNEncoder(\n",
      "  (net): Sequential(\n",
      "    (0): TorchCNN(\n",
      "      (cnn): Sequential(\n",
      "        (0): ZeroPad2d((2, 2, 2, 2))\n",
      "        (1): Conv2d(3, 16, kernel_size=(8, 8), stride=(4, 4))\n",
      "        (2): ReLU()\n",
      "        (3): ZeroPad2d((1, 2, 1, 2))\n",
      "        (4): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2))\n",
      "        (5): ReLU()\n",
      "      )\n",
      "    )\n",
      "    (1): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      ")\n",
      "torch.Size([2, 84, 84, 3])\n",
      "{'encoder_out': tensor([[0.1949, 0.0000, 0.0301,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3109, 0.0576,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<ViewBackward0>)}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from ray.rllib.core.models.configs import (\n",
    "    CNNEncoderConfig,\n",
    "    MLPEncoderConfig,\n",
    "    RecurrentEncoderConfig,\n",
    ")\n",
    "\n",
    "# MLP\n",
    "config = MLPEncoderConfig(\n",
    "    input_dims=[2],\n",
    "    hidden_layer_dims=[8, 8],\n",
    "    hidden_layer_activation=\"silu\",\n",
    "    hidden_layer_use_layernorm=True,\n",
    "    hidden_layer_use_bias=False,\n",
    "    output_layer_dim=4,\n",
    "    output_layer_activation=\"tanh\",\n",
    "    output_layer_use_bias=False,\n",
    ")\n",
    "model = config.build(framework=\"torch\")\n",
    "print(model)\n",
    "\n",
    "# CNN\n",
    "config = CNNEncoderConfig(\n",
    "    input_dims=[84, 84, 3],  # must be 3D tensor (image: w x h x C)\n",
    "    cnn_filter_specifiers=[\n",
    "        [16, [8, 8], 4],\n",
    "        [32, [4, 4], 2],\n",
    "    ],\n",
    "    cnn_activation=\"relu\",\n",
    "    cnn_use_layernorm=False,\n",
    "    cnn_use_bias=True,\n",
    ")\n",
    "model = config.build(framework=\"torch\")\n",
    "print(model)\n",
    "\n",
    "# 创建一个batch的输入数据\n",
    "batch_size = 2\n",
    "input_tensor = torch.randn(batch_size, 84, 84, 3)  # PyTorch格式\n",
    "print(input_tensor.shape)\n",
    "\n",
    "# 前向传播\n",
    "output = model({'obs': input_tensor})\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-08 14:24:16,079\tWARNING ppo.py:295 -- You are running PPO on the new API stack! This is the new default behavior for this algorithm. If you don't want to use the new API stack, set `config.api_stack(enable_rl_module_and_learner=False,enable_env_runner_and_connector_v2=False)`. For a detailed migration guide, see here: https://docs.ray.io/en/master/rllib/new-api-stack-migration-guide.html\n",
      "d:\\programs\\miniconda3\\Lib\\site-packages\\ray\\rllib\\algorithms\\algorithm.py:569: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "`UnifiedLogger` will be removed in Ray 2.7.\n",
      "  return UnifiedLogger(config, logdir, loggers=None)\n",
      "d:\\programs\\miniconda3\\Lib\\site-packages\\ray\\tune\\logger\\unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "d:\\programs\\miniconda3\\Lib\\site-packages\\ray\\tune\\logger\\unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "d:\\programs\\miniconda3\\Lib\\site-packages\\ray\\tune\\logger\\unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "2025-01-08 14:24:25,358\tINFO worker.py:1821 -- Started a local Ray instance.\n",
      "\u001b[36m(SingleAgentEnvRunner pid=204)\u001b[0m 2025-01-08 14:24:33,532\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
      "2025-01-08 14:24:34,081\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
      "2025-01-08 14:24:34,128\tWARNING ppo.py:295 -- You are running PPO on the new API stack! This is the new default behavior for this algorithm. If you don't want to use the new API stack, set `config.api_stack(enable_rl_module_and_learner=False,enable_env_runner_and_connector_v2=False)`. For a detailed migration guide, see here: https://docs.ray.io/en/master/rllib/new-api-stack-migration-guide.html\n",
      "2025-01-08 14:24:34,355\tWARNING rl_module.py:427 -- Could not create a Catalog object for your RLModule! If you are not using the new API stack yet, make sure to switch it off in your config: `config.api_stack(enable_rl_module_and_learner=False, enable_env_runner_and_connector_v2=False)`. Some algos already use the new stack by default. Ignore this message, if your RLModule does not use a Catalog to build its sub-components.\n",
      "2025-01-08 14:24:45,387\tINFO trainable.py:161 -- Trainable.setup took 28.843 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2025-01-08 14:24:45,387\tWARNING util.py:61 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PPOTorchRLModule(\n",
       "  (encoder): TorchActorCriticEncoder(\n",
       "    (actor_encoder): TorchMLPEncoder(\n",
       "      (net): TorchMLP(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=4, out_features=5, bias=True)\n",
       "          (1): Tanh()\n",
       "          (2): Linear(in_features=5, out_features=3, bias=True)\n",
       "          (3): Tanh()\n",
       "          (4): Linear(in_features=3, out_features=3, bias=True)\n",
       "          (5): Tanh()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pi): TorchMLPHead(\n",
       "    (net): TorchMLP(\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=3, out_features=2, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (vf): TorchMLPHead(\n",
       "    (net): TorchMLP(\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=3, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "\n",
    "config = (\n",
    "    PPOConfig()\n",
    "    .api_stack(# 使用新的api\n",
    "        enable_rl_module_and_learner=True,\n",
    "        enable_env_runner_and_connector_v2=True,\n",
    "    )\n",
    "    .environment(\"CartPole-v1\")# 输入的是1d，自动采用 MLP 编码器\n",
    "    .environment(\"pong\")# 输入的是3d(4, 42, 42)，自动采用 CNN 编码器\n",
    "    .rl_module(\n",
    "        model_config={\n",
    "            # MLP 参数\n",
    "            \"fcnet_hiddens\": [5, 3, 3],\n",
    "\n",
    "            # CNN 参数, 会自动计算输出维度\n",
    "            # \"conv_filters\": [\n",
    "            #     [32, [8, 8], 4],  # [输出通道数, [kernel_size_h, kernel_size_w], stride]\n",
    "            #     [64, [4, 4], 2],  # [64个通道, 4x4卷积核, stride=2]\n",
    "            #     [64, [3, 3], 1],  # [64个通道, 3x3卷积核, stride=1] \n",
    "            # ],\n",
    "        },\n",
    "    )\n",
    ")\n",
    "\n",
    "# 构建算法\n",
    "algo = config.build()\n",
    "\n",
    "# 查看模型\n",
    "algo.get_module()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自定义编码器\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvFCNetEncoder(\n",
      "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (fc): Linear(in_features=65536, out_features=10, bias=True)\n",
      ")\n",
      "torch.Size([2, 3, 64, 64])\n",
      "{'encoder_out': tensor([[ 0.0140,  0.0622,  0.1498, -0.0791, -0.1749, -0.1062,  0.1390,  0.1778,\n",
      "          0.1347, -0.2741],\n",
      "        [-0.2605,  0.0714, -0.2527, -0.2216, -0.1182, -0.3389,  0.1329,  0.6081,\n",
      "         -0.1720,  0.1413]], grad_fn=<AddmmBackward0>)}\n"
     ]
    }
   ],
   "source": [
    "# 自定义 ModelConfig\n",
    "from ray.rllib.core.models.configs import ModelConfig\n",
    "from ray.rllib.core.models.torch.encoder import TorchModel, Encoder\n",
    "from dataclasses import dataclass\n",
    "from ray.rllib.core.models.base import ENCODER_OUT\n",
    "from ray.rllib.core.columns import Columns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ConvFCNetEncoder(TorchModel, Encoder):\n",
    "    \"\"\"\n",
    "    自定义的编码器\n",
    "    - 继承 TorchModel, Encoder\n",
    "    - 初始化函数接收参数 config（也就是 ModelConfig重写类 的示例本身）\n",
    "        TorchModel.__init__(self, config)\n",
    "        Encoder.__init__(self, config)\n",
    "    - 重写方法 def _forward(self, inputs: dict, **kwargs) -> dict:\n",
    "        注意输入 / 输出\n",
    "    \"\"\"\n",
    "    def __init__(self, config) -> None:\n",
    "        TorchModel.__init__(self, config)\n",
    "        Encoder.__init__(self, config)\n",
    "    \n",
    "        self.conv1 = nn.Conv2d(in_channels=config.input_dims[0], out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        conv1_out = (config.input_dims[1] // 1) * (config.input_dims[2] // 1) * 16  # 64 * 64 * 16\n",
    "        self.fc = nn.Linear(conv1_out, config.out_dim)\n",
    "\n",
    "    def _forward(self, inputs: dict, **kwargs) -> dict:\n",
    "        x = F.relu(self.conv1(inputs[Columns.OBS]))\n",
    "        x = x.view(x.size(0), -1)  # Flatten the output of convolutional layer\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return {ENCODER_OUT: x}\n",
    "\n",
    "@dataclass# input_dims / out_dim 为实例参数\n",
    "class test_EncoderConfig(ModelConfig):\n",
    "    \"\"\"\n",
    "    output_dims函数 返回编码器输出的维度，用于其他构造 head模型 的输入\n",
    "    \"\"\"\n",
    "    input_dims = None\n",
    "    out_dim = 10\n",
    "    def build(self, framework: str = \"torch\"):\n",
    "        if framework == \"torch\":\n",
    "            # 一个卷积层 + 全连接层\n",
    "            return ConvFCNetEncoder(self)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f'only torch ModelConfig')\n",
    "\n",
    "    @property\n",
    "    def output_dims(self):\n",
    "        \"\"\"Read-only `output_dims` are inferred automatically from other settings.\"\"\"\n",
    "        return (int(self.out_dim),)# 注意返回的是维度，不是int\n",
    "    \n",
    "net = test_EncoderConfig([3, 64, 64], 10).build()\n",
    "print(net)\n",
    "\n",
    "batch_size = 2\n",
    "input_tensor = torch.randn(batch_size, 3, 64, 64)  # PyTorch格式\n",
    "print(input_tensor.shape)\n",
    "\n",
    "# 前向传播\n",
    "output = net({Columns.OBS: input_tensor})# 与默认编码器一致输入/输出\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-08 15:32:32,432\tWARNING ppo.py:295 -- You are running PPO on the new API stack! This is the new default behavior for this algorithm. If you don't want to use the new API stack, set `config.api_stack(enable_rl_module_and_learner=False,enable_env_runner_and_connector_v2=False)`. For a detailed migration guide, see here: https://docs.ray.io/en/master/rllib/new-api-stack-migration-guide.html\n",
      "2025-01-08 15:32:39,276\tWARNING ppo.py:295 -- You are running PPO on the new API stack! This is the new default behavior for this algorithm. If you don't want to use the new API stack, set `config.api_stack(enable_rl_module_and_learner=False,enable_env_runner_and_connector_v2=False)`. For a detailed migration guide, see here: https://docs.ray.io/en/master/rllib/new-api-stack-migration-guide.html\n",
      "2025-01-08 15:32:39,459\tWARNING rl_module.py:427 -- Could not create a Catalog object for your RLModule! If you are not using the new API stack yet, make sure to switch it off in your config: `config.api_stack(enable_rl_module_and_learner=False, enable_env_runner_and_connector_v2=False)`. Some algos already use the new stack by default. Ignore this message, if your RLModule does not use a Catalog to build its sub-components.\n",
      "2025-01-08 15:32:39,631\tWARNING util.py:61 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PPOTorchRLModule(\n",
       "  (encoder): TorchActorCriticEncoder(\n",
       "    (actor_encoder): ConvFCNetEncoder(\n",
       "      (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (fc): Linear(in_features=537600, out_features=10, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (pi): TorchMLPHead(\n",
       "    (net): TorchMLP(\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=10, out_features=6, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (vf): TorchMLPHead(\n",
       "    (net): TorchMLP(\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=10, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import functools\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.rllib.core.rl_module.rl_module import RLModuleSpec\n",
    "from ray.tune.registry import get_trainable_cls, register_env\n",
    "from ray.rllib.env.wrappers.atari_wrappers import wrap_atari_for_new_api_stack\n",
    "\n",
    "from ray.rllib.algorithms.ppo.ppo_catalog import PPOCatalog\n",
    "class custom_PPOCatalog(PPOCatalog):\n",
    "    \"\"\"\n",
    "    - 重写 _determine_components_hook 生成配置\n",
    "    \"\"\"\n",
    "    def _determine_components_hook(self):\n",
    "        # 获取输入参数 可设置参数 input_dims / out_dim\n",
    "        input_dims = self._model_config_dict[\"input_dims\"]\n",
    "        out_dim = self._model_config_dict[\"out_dim\"]\n",
    "        # 生成配置\n",
    "        self._encoder_config = test_EncoderConfig(input_dims, out_dim)\n",
    "\n",
    "        # 不变\n",
    "        # Create a function that can be called when framework is known to retrieve the\n",
    "        # class type for action distributions\n",
    "        self._action_dist_class_fn = functools.partial(\n",
    "            self._get_dist_cls_from_action_space, action_space=self.action_space\n",
    "        )\n",
    "\n",
    "        # 不变\n",
    "        # The dimensions of the latent vector that is output by the encoder and fed\n",
    "        # to the heads.\n",
    "        self.latent_dims = self._encoder_config.output_dims\n",
    "\n",
    "\n",
    "register_env(\n",
    "    \"pong\",\n",
    "    lambda cfg: wrap_atari_for_new_api_stack(\n",
    "        gym.make(\"ale_py:ALE/Pong-v5\", **cfg),\n",
    "        dim=42,  # <- need images to be \"tiny\" for our custom model\n",
    "        framestack=4,\n",
    "    ),\n",
    ")\n",
    "\n",
    "config = (\n",
    "    PPOConfig()\n",
    "    .api_stack(# 使用新的api\n",
    "        enable_rl_module_and_learner=True,\n",
    "        enable_env_runner_and_connector_v2=True,\n",
    "    )\n",
    "    .environment(\"pong\")# 输入的是3d(4, 42, 42)\n",
    "    .rl_module(\n",
    "        rl_module_spec=RLModuleSpec(catalog_class=custom_PPOCatalog),# 使用自定义配置\n",
    "        model_config={\n",
    "            # # MLP 参数\n",
    "            # \"fcnet_hiddens\": [5, 3, 3],\n",
    "\n",
    "            # CNN 参数\n",
    "            # \"conv_filters\": [\n",
    "            #     [32, [8, 8], 4],  # [输出通道数, [kernel_size_h, kernel_size_w], stride]\n",
    "            #     [64, [4, 4], 2],  # [64个通道, 4x4卷积核, stride=2]\n",
    "            #     [64, [3, 3], 1],  # [64个通道, 3x3卷积核, stride=1] \n",
    "            # ],\n",
    "\n",
    "            # 自定义编码器参数\n",
    "            'input_dims' : [3, 210, 160],\n",
    "            'out_dim' : 10,\n",
    "        },\n",
    "    )\n",
    ")\n",
    "\n",
    "# 构建算法\n",
    "algo = config.build()\n",
    "\n",
    "# 查看模型\n",
    "algo.get_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
