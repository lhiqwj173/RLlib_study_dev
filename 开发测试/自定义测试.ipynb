{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"trusted":true},"outputs":[],"source":["# # 准备环境\n","# !wget https://raw.githubusercontent.com/lhiqwj173/dl_helper/master/envs/rl.py > /dev/null 2>&1\n","# !python rl.py not_install_dl_helper > /dev/null 2>&1\n","# !pip install /kaggle/working/3rd/dl_helper > /dev/null 2>&1"]},{"cell_type":"code","execution_count":2,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["ray 版本: 2.40.0\n"]}],"source":["from pprint import pprint\n","import numpy as np\n","import gymnasium as gym\n","import ale_py\n","gym.register_envs(ale_py)\n","import ray\n","print(\"ray 版本:\", ray.__version__)"]},{"cell_type":"markdown","metadata":{},"source":["# 配置\n","\n","- **PPO**\n","- DQN\n","- **Rainbow_DQN**\n","- Double_DQN\n","- Dueling_DQN\n","- DQN_PER\n","- Noisy_DQN\n","- DQN_C51\n","- **IMPALA**\n","- **APPO**\n"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["# 算法\n","algo = \"PPO\"\n","# algo = \"DQN\"\n","# algo = \"Rainbow_DQN\"\n","# algo = \"DQN_C51\"\n","# algo = \"IMPALA\"\n","# algo = \"APPO\""]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["class algo_base:\n","    def __init__(self, **kwargs):\n","        self._training_kwargs = kwargs\n","\n","    @property\n","    def training_kwargs(self):\n","        return self._training_kwargs\n","    \n","    def _update_kwargs(self, kwargs):\n","        for k, v in kwargs.items():\n","            self._training_kwargs[k] = v\n","\n","class PPO(algo_base):\n","    @property\n","    def algo(self):\n","        return \"PPO\"\n","\n","class IMPALA(algo_base):\n","    @property\n","    def algo(self):\n","        return \"IMPALA\"\n","\n","class APPO(algo_base):\n","    def __init__(self, **kwargs):\n","        self._training_kwargs = {\n","            'grad_clip': 30.0,\n","        }\n","\n","        self._update_kwargs(kwargs)\n","\n","    @property\n","    def algo(self):\n","        return \"APPO\"\n","\n","class DQN(algo_base):\n","\n","    @property\n","    def algo(self):\n","        return \"DQN\"\n","    \n","class Rainbow_DQN(DQN):\n","    def __init__(self, **kwargs):\n","        self._training_kwargs = {\n","            \"target_network_update_freq\": 500,\n","            'replay_buffer_config': {\n","                \"type\": \"PrioritizedEpisodeReplayBuffer\",\n","                \"capacity\": 60000,\n","                \"alpha\": 0.5,\n","                \"beta\": 0.5,\n","            },\n","            # \"replay_buffer_config\": {\n","            #     \"_enable_replay_buffer_api\": False,\n","            #     \"type\": \"ReplayBuffer\",\n","            #     \"type\": \"PrioritizedReplayBuffer\",\n","            #     \"capacity\": 50000,\n","            #     \"prioritized_replay_alpha\": 0.6,\n","            #     \"prioritized_replay_beta\": 0.4,\n","            #     \"prioritized_replay_eps\": 1e-6,\n","            #     \"replay_sequence_length\": 1\n","            # },\n","            \"epsilon\": [[0, 1.0], [1000000, 0.1]],\n","            \"adam_epsilon\": 1e-8,\n","            \"grad_clip\": 40.0,\n","            \"num_steps_sampled_before_learning_starts\": 10000,\n","            \"tau\": 1,\n","            \"num_atoms\": 51,\n","            \"v_min\": -10.0,\n","            \"v_max\": 10.0,\n","            \"noisy\": True,\n","            \"sigma0\": 0.5,\n","            \"dueling\": True,\n","            \"hiddens\": [512],\n","            \"double_q\": True,\n","            \"n_step\": 3,\n","        }\n","\n","        self._update_kwargs(kwargs)\n","\n","class Double_DQN(DQN):\n","    def __init__(self, **kwargs):\n","        self._training_kwargs = {\n","            'double_q': True,\n","        }\n","\n","        self._update_kwargs(kwargs)\n","\n","class Dueling_DQN(DQN):\n","    def __init__(self, **kwargs):\n","        self._training_kwargs = {\n","            'dueling': True,\n","        }\n","\n","        self._update_kwargs(kwargs)\n","\n","class DQN_PER(DQN):\n","    def __init__(self, **kwargs):\n","        self._training_kwargs = {\n","            'replay_buffer_config': {\n","                \"type\": \"PrioritizedEpisodeReplayBuffer\",\n","                \"capacity\": 60000,\n","                \"alpha\": 0.5,\n","                \"beta\": 0.5,\n","            },\n","        }\n","\n","        self._update_kwargs(kwargs)\n","\n","class Noisy_DQN(DQN):\n","    def __init__(self, **kwargs):\n","        self._training_kwargs = {\n","            'noisy': True,\n","        }\n","        \n","        self._update_kwargs(kwargs)\n","\n","class DQN_C51(DQN):\n","    def __init__(self, **kwargs):\n","        self._training_kwargs = {\n","            'num_atoms': 51,\n","            'v_min': -10.0,\n","            'v_max': 10.0,\n","        }\n","        \n","        self._update_kwargs(kwargs)\n"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["d:\\programs\\miniconda3\\Lib\\site-packages\\gymnasium\\spaces\\box.py:235: UserWarning: \u001b[33mWARN: Box low's precision lowered by casting to float32, current low.dtype=float64\u001b[0m\n","  gym.logger.warn(\n","d:\\programs\\miniconda3\\Lib\\site-packages\\gymnasium\\spaces\\box.py:305: UserWarning: \u001b[33mWARN: Box high's precision lowered by casting to float32, current high.dtype=float64\u001b[0m\n","  gym.logger.warn(\n","d:\\programs\\miniconda3\\Lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:134: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n","  logger.warn(\n","d:\\programs\\miniconda3\\Lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:158: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n","  logger.warn(f\"{pre} is not within the observation space.\")\n"]}],"source":["from ray.tune.registry import get_trainable_cls\n","from dl_helper.rl.rl_env.breakout_env import BreakoutEnv# 自定义环境\n","from ray.tune.registry import register_env\n","\n","# 注册环境 不是必须\n","register_env(\"breakout\", lambda config: BreakoutEnv())\n","\n","# 实例化简单算法配置类\n","simple_algo = globals()[algo]()\n","\n","config = (\n","    get_trainable_cls(simple_algo.algo)\n","    .get_default_config()\n","    .api_stack(\n","        enable_rl_module_and_learner=True,\n","        enable_env_runner_and_connector_v2=True,\n","    )\n","    .environment(\"breakout\")\n","    # .environment(\"CartPole-v1\")\n","    .env_runners(num_env_runners=0)\n","    .evaluation(\n","        evaluation_interval=10,\n","        evaluation_duration=3,\n","    )\n","    .rl_module(\n","        model_config={\n","            \"conv_filters\": [\n","                [32, [8, 8], 4],  # [输出通道数, [kernel_size_h, kernel_size_w], stride]\n","                [64, [4, 4], 2],  # [64个通道, 4x4卷积核, stride=2]\n","                [64, [3, 3], 1],  # [64个通道, 3x3卷积核, stride=1] \n","            ],\n","        },\n","    )\n","    .training(**simple_algo.training_kwargs)\n",")"]},{"cell_type":"markdown","metadata":{},"source":["# 训练"]},{"cell_type":"code","execution_count":6,"metadata":{"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2025-01-07 22:19:31,310\tWARNING deprecation.py:50 -- DeprecationWarning: `_enable_new_api_stack` has been deprecated. Use `AlgorithmConfig._enable_new_api_stack` instead. This will raise an error in the future!\n","2025-01-07 22:19:31,312\tWARNING deprecation.py:50 -- DeprecationWarning: `custom_resources_per_worker` has been deprecated. Use `AlgorithmConfig.custom_resources_per_env_runner` instead. This will raise an error in the future!\n","2025-01-07 22:19:31,313\tWARNING deprecation.py:50 -- DeprecationWarning: `delay_between_worker_restarts_s` has been deprecated. Use `AlgorithmConfig.delay_between_env_runner_restarts_s` instead. This will raise an error in the future!\n","2025-01-07 22:19:31,314\tWARNING deprecation.py:50 -- DeprecationWarning: `evaluation_num_workers` has been deprecated. Use `AlgorithmConfig.evaluation_num_workers` instead. This will raise an error in the future!\n","2025-01-07 22:19:31,321\tWARNING deprecation.py:50 -- DeprecationWarning: `ignore_worker_failures` has been deprecated. Use `AlgorithmConfig.ignore_env_runner_failures` instead. This will raise an error in the future!\n","2025-01-07 22:19:31,370\tWARNING deprecation.py:50 -- DeprecationWarning: `max_num_worker_restarts` has been deprecated. Use `AlgorithmConfig.max_num_env_runner_restarts` instead. This will raise an error in the future!\n","2025-01-07 22:19:31,371\tWARNING deprecation.py:50 -- DeprecationWarning: `num_consecutive_worker_failures_tolerance` has been deprecated. Use `AlgorithmConfig.num_consecutive_env_runner_failures_tolerance` instead. This will raise an error in the future!\n","2025-01-07 22:19:31,374\tWARNING deprecation.py:50 -- DeprecationWarning: `num_cpus_for_local_worker` has been deprecated. Use `AlgorithmConfig.num_cpus_for_local_worker` instead. This will raise an error in the future!\n","2025-01-07 22:19:31,376\tWARNING deprecation.py:50 -- DeprecationWarning: `num_cpus_per_learner_worker` has been deprecated. Use `AlgorithmConfig.num_cpus_per_learner` instead. This will raise an error in the future!\n","2025-01-07 22:19:31,377\tWARNING deprecation.py:50 -- DeprecationWarning: `num_cpus_per_worker` has been deprecated. Use `AlgorithmConfig.num_cpus_per_env_runner` instead. This will raise an error in the future!\n","2025-01-07 22:19:31,379\tWARNING deprecation.py:50 -- DeprecationWarning: `num_envs_per_worker` has been deprecated. Use `AlgorithmConfig.num_envs_per_env_runner` instead. This will raise an error in the future!\n","2025-01-07 22:19:31,380\tWARNING deprecation.py:50 -- DeprecationWarning: `num_gpus_per_learner_worker` has been deprecated. Use `AlgorithmConfig.num_gpus_per_learner` instead. This will raise an error in the future!\n","2025-01-07 22:19:31,381\tWARNING deprecation.py:50 -- DeprecationWarning: `num_gpus_per_worker` has been deprecated. Use `AlgorithmConfig.num_gpus_per_env_runner` instead. This will raise an error in the future!\n","2025-01-07 22:19:31,384\tWARNING deprecation.py:50 -- DeprecationWarning: `num_learner_workers` has been deprecated. Use `AlgorithmConfig.num_learners` instead. This will raise an error in the future!\n","2025-01-07 22:19:31,386\tWARNING deprecation.py:50 -- DeprecationWarning: `num_rollout_workers` has been deprecated. Use `AlgorithmConfig.num_env_runners` instead. This will raise an error in the future!\n","2025-01-07 22:19:31,388\tWARNING deprecation.py:50 -- DeprecationWarning: `recreate_failed_env_runners` has been deprecated. Use `AlgorithmConfig.fault_tolerance(restart_failed_env_runners=..)` instead. This will raise an error in the future!\n","2025-01-07 22:19:31,390\tWARNING deprecation.py:50 -- DeprecationWarning: `recreate_failed_workers` has been deprecated. Use `AlgorithmConfig.restart_failed_env_runners` instead. This will raise an error in the future!\n","2025-01-07 22:19:31,430\tWARNING deprecation.py:50 -- DeprecationWarning: `validate_workers_after_construction` has been deprecated. Use `AlgorithmConfig.validate_env_runners_after_construction` instead. This will raise an error in the future!\n","2025-01-07 22:19:31,432\tWARNING deprecation.py:50 -- DeprecationWarning: `worker_health_probe_timeout_s` has been deprecated. Use `AlgorithmConfig.env_runner_health_probe_timeout_s` instead. This will raise an error in the future!\n","2025-01-07 22:19:31,433\tWARNING deprecation.py:50 -- DeprecationWarning: `worker_restore_timeout_s` has been deprecated. Use `AlgorithmConfig.env_runner_restore_timeout_s` instead. This will raise an error in the future!\n","2025-01-07 22:20:11,208\tWARNING ppo.py:295 -- You are running PPO on the new API stack! This is the new default behavior for this algorithm. If you don't want to use the new API stack, set `config.api_stack(enable_rl_module_and_learner=False,enable_env_runner_and_connector_v2=False)`. For a detailed migration guide, see here: https://docs.ray.io/en/master/rllib/new-api-stack-migration-guide.html\n","d:\\programs\\miniconda3\\Lib\\site-packages\\ray\\rllib\\algorithms\\algorithm.py:569: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n","`UnifiedLogger` will be removed in Ray 2.7.\n","  return UnifiedLogger(config, logdir, loggers=None)\n","d:\\programs\\miniconda3\\Lib\\site-packages\\ray\\tune\\logger\\unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n","The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n","  self._loggers.append(cls(self.config, self.logdir, self.trial))\n","d:\\programs\\miniconda3\\Lib\\site-packages\\ray\\tune\\logger\\unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n","The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n","  self._loggers.append(cls(self.config, self.logdir, self.trial))\n","d:\\programs\\miniconda3\\Lib\\site-packages\\ray\\tune\\logger\\unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n","The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n","  self._loggers.append(cls(self.config, self.logdir, self.trial))\n","2025-01-07 22:26:12,658\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n","2025-01-07 22:26:49,369\tWARNING ppo.py:295 -- You are running PPO on the new API stack! This is the new default behavior for this algorithm. If you don't want to use the new API stack, set `config.api_stack(enable_rl_module_and_learner=False,enable_env_runner_and_connector_v2=False)`. For a detailed migration guide, see here: https://docs.ray.io/en/master/rllib/new-api-stack-migration-guide.html\n","d:\\programs\\miniconda3\\Lib\\site-packages\\gymnasium\\envs\\registration.py:642: UserWarning: \u001b[33mWARN: Overriding environment rllib-single-agent-env-v0 already in registry.\u001b[0m\n","  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n","2025-01-07 22:36:46,781\tWARNING rl_module.py:427 -- Could not create a Catalog object for your RLModule! If you are not using the new API stack yet, make sure to switch it off in your config: `config.api_stack(enable_rl_module_and_learner=False, enable_env_runner_and_connector_v2=False)`. Some algos already use the new stack by default. Ignore this message, if your RLModule does not use a Catalog to build its sub-components.\n"]}],"source":["import os\n","from dl_helper.rl.rl_utils import simplify_rllib_metrics\n","\n","checkpoint_base_dir = rf'C:/Users/lh/Desktop/temp/RLlib_{algo}_cartpole'\n","os.makedirs(checkpoint_base_dir, exist_ok=True)\n","\n","# 构建算法\n","algo = config.build()\n","\n","# 模型\n","algo.get_module()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 训练循环\n","rounds = 10 * 3\n","for i in range(rounds):\n","    print(f\"\\nTraining iteration {i+1}/{rounds}\")\n","    result = algo.train()\n","    simplify_rllib_metrics(result)\n","    \n","    if (i + 1) % 5 == 0:\n","        checkpoint_dir = algo.save_to_path(\n","            os.path.join(os.path.abspath(checkpoint_base_dir), f\"checkpoint_{i+1}\")\n","        )\n","        print(f\"Checkpoint saved in directory {checkpoint_dir}\")\n","\n","# 保存最终模型\n","final_checkpoint = algo.save_to_path(\n","    os.path.join(os.path.abspath(checkpoint_base_dir), f\"final\")\n",")\n","print(f\"Final checkpoint saved in directory {final_checkpoint}\")\n","\n","# 清理\n","algo.stop()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["NumPy 版本: 1.26.4\n"]}],"source":["import numpy as np\n","print(f\"NumPy 版本: {np.__version__}\")"]},{"cell_type":"markdown","metadata":{},"source":["# DQN 默认配置\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 基类\n","from ray.rllib.core.rl_module.rl_module import RLModule\n","from ray.rllib.core.rl_module.torch.torch_rl_module import TorchRLModule\n","\n","# API 类\n","from ray.rllib.core.rl_module.apis import InferenceOnlyAPI, TargetNetworkAPI"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# DQN 模型\n","from ray.rllib.algorithms.dqn.dqn_rainbow_catalog import DQNRainbowCatalog\n","\"\"\"\n","\"DQN Rainbow 目录类用于构建模型。\n","\n","`DQNRainbowCatalog` 提供以下模型：\n","    - 编码器（Encoder）：用于对观察结果进行编码。\n","    - 目标编码器（Target_Encoder）：用于对目标网络的观察结果进行编码。\n","    - Af Head：\n","        如果使用 dueling 架构，则是优势流的头部；如果使用 Q 函数，则是 Q 函数的头部。\n","        这是一个多节点头部，在期望学习的情况下有 `action_space.n` 个节点，在分布式 Q 学习的情况下有 `action_space.n` 倍于支持原子数（`num_atoms`）的节点数。\n","    - Vf Head（可选）：\n","        在选择 dueling 架构时，值函数的头部。这是一个单一节点头部。如果不使用 dueling 架构，则不存在此头部。\n","\n","所有网络都可以包括嘈杂层（noisy layers），如果 `noisy` 为 `True`。在这种情况下，不使用 epsilon 贪心探索。\n","\n","可以通过重写 `build_af_head()` 和 `build_vf_head()` 来构建任何自定义头部。另外，可以重写 `AfHeadConfig` 或 `VfHeadConfig` 来在 `RLModule` 运行时构建自定义逻辑。\n","\n","所有头部可以选择使用分布式学习。在这种情况下，输出神经元的数量对应于离散分布的支持原子数乘以动作数。\n","\n","任何为探索或推断构建的模块都使用标志 `inference_only=True`，不包含任何目标网络。可以通过 `SingleAgentModuleSpec` 中的 `inference_only` 布尔标志来设置此标志。\"\n","\"\"\"\n","\n","from ray.rllib.algorithms.dqn.torch.dqn_rainbow_torch_rl_module import (\n","    DQNRainbowTorchRLModule,\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dqn_config = {\n","    \"exploration_config\": {},\n","    \"algo_class\": \"ray.rllib.algorithms.dqn.dqn.DQN\",\n","    \"extra_python_environs_for_driver\": {},\n","    \"extra_python_environs_for_worker\": {},\n","    \"placement_strategy\": \"PACK\",\n","    \"num_gpus\": 0,\n","    \"_fake_gpus\": False,\n","    \"num_cpus_for_main_process\": 1,\n","    \"framework_str\": \"torch\",\n","    \"eager_tracing\": True,\n","    \"eager_max_retraces\": 20,\n","    \"tf_session_args\": {\n","        \"intra_op_parallelism_threads\": 2,\n","        \"inter_op_parallelism_threads\": 2,\n","        \"gpu_options\": {\"allow_growth\": True},\n","        \"log_device_placement\": False,\n","        \"device_count\": {\"CPU\": 1},\n","        \"allow_soft_placement\": True\n","    },\n","    \"local_tf_session_args\": {\n","        \"intra_op_parallelism_threads\": 8,\n","        \"inter_op_parallelism_threads\": 8\n","    },\n","    \"torch_compile_learner\": False,\n","    \"torch_compile_learner_what_to_compile\": \"TorchCompileWhatToCompile.FORWARD_TRAIN\",\n","    \"torch_compile_learner_dynamo_backend\": \"inductor\",\n","    \"torch_compile_learner_dynamo_mode\": None,\n","    \"torch_compile_worker\": False,\n","    \"torch_compile_worker_dynamo_backend\": \"onnxrt\",\n","    \"torch_compile_worker_dynamo_mode\": None,\n","    \"torch_ddp_kwargs\": {},\n","    \"torch_skip_nan_gradients\": False,\n","    \"env\": \"CartPole-v1\",\n","    \"env_config\": {},\n","    \"observation_space\": None,\n","    \"action_space\": None,\n","    \"clip_rewards\": None,\n","    \"normalize_actions\": True,\n","    \"clip_actions\": False,\n","    \"_is_atari\": False,\n","    \"disable_env_checking\": False,\n","    \"env_task_fn\": None,\n","    \"render_env\": False,\n","    \"action_mask_key\": \"action_mask\",\n","    \"env_runner_cls\": None,\n","    \"num_env_runners\": 0,\n","    \"num_envs_per_env_runner\": 1,\n","    \"num_cpus_per_env_runner\": 1,\n","    \"num_gpus_per_env_runner\": 0,\n","    \"custom_resources_per_env_runner\": {},\n","    \"validate_env_runners_after_construction\": True,\n","    \"max_requests_in_flight_per_env_runner\": 1,\n","    \"sample_timeout_s\": 60.0,\n","    \"create_env_on_local_worker\": False,\n","    \"_env_to_module_connector\": None,\n","    \"add_default_connectors_to_env_to_module_pipeline\": True,\n","    \"_module_to_env_connector\": None,\n","    \"add_default_connectors_to_module_to_env_pipeline\": True,\n","    \"episode_lookback_horizon\": 1,\n","    \"rollout_fragment_length\": \"auto\",\n","    \"batch_mode\": \"truncate_episodes\",\n","    \"compress_observations\": False,\n","    \"remote_worker_envs\": False,\n","    \"remote_env_batch_wait_ms\": 0,\n","    \"enable_tf1_exec_eagerly\": False,\n","    \"sample_collector\": \"ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector\",\n","    \"preprocessor_pref\": \"deepmind\",\n","    \"observation_filter\": \"NoFilter\",\n","    \"update_worker_filter_stats\": True,\n","    \"use_worker_filter_stats\": True,\n","    \"sampler_perf_stats_ema_coef\": None,\n","    \"num_learners\": 0,\n","    \"num_gpus_per_learner\": 0,\n","    \"num_cpus_per_learner\": 1,\n","    \"local_gpu_idx\": 0,\n","    \"max_requests_in_flight_per_learner\": 3,\n","    \"gamma\": 0.99,\n","    \"lr\": 0.0005,\n","    \"grad_clip\": 40.0,\n","    \"grad_clip_by\": \"global_norm\",\n","    \"train_batch_size_per_learner\": None,\n","    \"train_batch_size\": 32,\n","    \"num_epochs\": 1,\n","    \"minibatch_size\": None,\n","    \"shuffle_batch_per_epoch\": False,\n","    \"model\": {\n","        \"fcnet_hiddens\": [256, 256],\n","        \"fcnet_activation\": \"tanh\",\n","        \"fcnet_weights_initializer\": None,\n","        \"fcnet_weights_initializer_config\": None,\n","        \"fcnet_bias_initializer\": None,\n","        \"fcnet_bias_initializer_config\": None,\n","        \"conv_filters\": None,\n","        \"conv_activation\": \"relu\",\n","        \"conv_kernel_initializer\": None,\n","        \"conv_kernel_initializer_config\": None,\n","        \"conv_bias_initializer\": None,\n","        \"conv_bias_initializer_config\": None,\n","        \"conv_transpose_kernel_initializer\": None,\n","        \"conv_transpose_kernel_initializer_config\": None,\n","        \"conv_transpose_bias_initializer\": None,\n","        \"conv_transpose_bias_initializer_config\": None,\n","        \"post_fcnet_hiddens\": [],\n","        \"post_fcnet_activation\": \"relu\",\n","        \"post_fcnet_weights_initializer\": None,\n","        \"post_fcnet_weights_initializer_config\": None,\n","        \"post_fcnet_bias_initializer\": None,\n","        \"post_fcnet_bias_initializer_config\": None,\n","        \"free_log_std\": False,\n","        \"log_std_clip_param\": 20.0,\n","        \"no_final_linear\": False,\n","        \"vf_share_layers\": True,\n","        \"use_lstm\": False,\n","        \"max_seq_len\": 20,\n","        \"lstm_cell_size\": 256,\n","        \"lstm_use_prev_action\": False,\n","        \"lstm_use_prev_reward\": False,\n","        \"lstm_weights_initializer\": None,\n","        \"lstm_weights_initializer_config\": None,\n","        \"lstm_bias_initializer\": None,\n","        \"lstm_bias_initializer_config\": None,\n","        \"_time_major\": False,\n","        \"use_attention\": False,\n","        \"attention_num_transformer_units\": 1,\n","        \"attention_dim\": 64,\n","        \"attention_num_heads\": 1,\n","        \"attention_head_dim\": 32,\n","        \"attention_memory_inference\": 50,\n","        \"attention_memory_training\": 50,\n","        \"attention_position_wise_mlp_dim\": 32,\n","        \"attention_init_gru_gate_bias\": 2.0,\n","        \"attention_use_n_prev_actions\": 0,\n","        \"attention_use_n_prev_rewards\": 0,\n","        \"framestack\": True,\n","        \"dim\": 84,\n","        \"grayscale\": False,\n","        \"zero_mean\": True,\n","        \"custom_model\": None,\n","        \"custom_model_config\": {},\n","        \"custom_action_dist\": None,\n","        \"custom_preprocessor\": None,\n","        \"encoder_latent_dim\": None,\n","        \"always_check_shapes\": False,\n","        \"lstm_use_prev_action_reward\": -1,\n","        \"_use_default_native_models\": -1,\n","        \"_disable_preprocessor_api\": False,\n","        \"_disable_action_flattening\": False\n","    },\n","    \"_learner_connector\": None,\n","    \"add_default_connectors_to_learner_pipeline\": True,\n","    \"learner_config_dict\": {},\n","    \"optimizer\": {},\n","    \"_learner_class\": None,\n","    \"callbacks_class\": \"ray.rllib.algorithms.callbacks.DefaultCallbacks\",\n","    \"explore\": True,\n","    \"enable_rl_module_and_learner\": True,\n","    \"enable_env_runner_and_connector_v2\": True,\n","    \"_prior_exploration_config\": {\n","        \"type\": \"EpsilonGreedy\",\n","        \"initial_epsilon\": 1.0,\n","        \"final_epsilon\": 0.02,\n","        \"epsilon_timesteps\": 10000\n","    },\n","    \"count_steps_by\": \"env_steps\",\n","    \"policies\": {\"default_policy\": [None, None, None, None]},\n","    \"policy_map_capacity\": 100,\n","    \"policy_mapping_fn\": \"AlgorithmConfig.DEFAULT_POLICY_MAPPING_FN\",\n","    \"policies_to_train\": None,\n","    \"policy_states_are_swappable\": False,\n","    \"observation_fn\": None,\n","    \"input_\": \"sampler\",\n","    \"input_read_method\": \"read_parquet\",\n","    \"input_read_method_kwargs\": {},\n","    \"input_read_schema\": {},\n","    \"input_read_episodes\": False,\n","    \"input_read_sample_batches\": False,\n","    \"input_read_batch_size\": None,\n","    \"input_filesystem\": None,\n","    \"input_filesystem_kwargs\": {},\n","    \"input_compress_columns\": [\"obs\", \"new_obs\"],\n","    \"input_spaces_jsonable\": True,\n","    \"materialize_data\": False,\n","    \"materialize_mapped_data\": True,\n","    \"map_batches_kwargs\": {},\n","    \"iter_batches_kwargs\": {},\n","    \"prelearner_class\": None,\n","    \"prelearner_buffer_class\": None,\n","    \"prelearner_buffer_kwargs\": {},\n","    \"prelearner_module_synch_period\": 10,\n","    \"dataset_num_iters_per_learner\": None,\n","    \"input_config\": {},\n","    \"actions_in_input_normalized\": False,\n","    \"postprocess_inputs\": False,\n","    \"shuffle_buffer_size\": 0,\n","    \"output\": None,\n","    \"output_config\": {},\n","    \"output_compress_columns\": [\"obs\", \"new_obs\"],\n","    \"output_max_file_size\": 67108864,\n","    \"output_max_rows_per_file\": None,\n","    \"output_write_method\": \"write_parquet\",\n","    \"output_write_method_kwargs\": {},\n","    \"output_filesystem\": None,\n","    \"output_filesystem_kwargs\": {},\n","    \"output_write_episodes\": True,\n","    \"offline_sampling\": False,\n","    \"evaluation_interval\": 10,\n","    \"evaluation_duration\": 3,\n","    \"evaluation_duration_unit\": \"episodes\",\n","    \"evaluation_sample_timeout_s\": 120.0,\n","    \"evaluation_parallel_to_training\": False,\n","    \"evaluation_force_reset_envs_before_iteration\": True,\n","    \"evaluation_config\": {\"explore\": False},\n","    \"off_policy_estimation_methods\": {},\n","    \"ope_split_batch_by_episode\": True,\n","    \"evaluation_num_env_runners\": 0,\n","    \"custom_evaluation_function\": None,\n","    \"in_evaluation\": False,\n","    \"sync_filters_on_rollout_workers_timeout_s\": 10.0,\n","    \"keep_per_episode_custom_metrics\": False,\n","    \"metrics_episode_collection_timeout_s\": 60.0,\n","    \"metrics_num_episodes_for_smoothing\": 100,\n","    \"min_time_s_per_iteration\": None,\n","    \"min_train_timesteps_per_iteration\": 0,\n","    \"min_sample_timesteps_per_iteration\": 1000,\n","    \"log_gradients\": True,\n","    \"export_native_model_files\": False,\n","    \"checkpoint_trainable_policies_only\": False,\n","    \"logger_creator\": None,\n","    \"logger_config\": None,\n","    \"log_level\": \"WARN\",\n","    \"log_sys_usage\": True,\n","    \"fake_sampler\": False,\n","    \"seed\": None,\n","    \"_run_training_always_in_thread\": False,\n","    \"_evaluation_parallel_to_training_wo_thread\": False,\n","    \"restart_failed_env_runners\": True,\n","    \"ignore_env_runner_failures\": False,\n","    \"max_num_env_runner_restarts\": 1000,\n","    \"delay_between_env_runner_restarts_s\": 60.0,\n","    \"restart_failed_sub_environments\": False,\n","    \"num_consecutive_env_runner_failures_tolerance\": 100,\n","    \"env_runner_health_probe_timeout_s\": 30.0,\n","    \"env_runner_restore_timeout_s\": 1800.0,\n","    \"_model_config\": {},\n","    \"_rl_module_spec\": None,\n","    \"algorithm_config_overrides_per_module\": {},\n","    \"_per_module_overrides\": {},\n","    \"_torch_grad_scaler_class\": None,\n","    \"_torch_lr_scheduler_classes\": None,\n","    \"_tf_policy_handles_more_than_one_loss\": False,\n","    \"_disable_preprocessor_api\": False,\n","    \"_disable_action_flattening\": False,\n","    \"_disable_initialize_loss_from_dummy_batch\": False,\n","    \"_dont_auto_sync_env_runner_states\": False,\n","    \"_is_frozen\": False,\n","    \"enable_connectors\": -1,\n","    \"simple_optimizer\": -1,\n","    \"monitor\": -1,\n","    \"evaluation_num_episodes\": -1,\n","    \"metrics_smoothing_episodes\": -1,\n","    \"timesteps_per_iteration\": -1,\n","    \"min_iter_time_s\": -1,\n","    \"collect_metrics_timeout\": -1,\n","    \"min_time_s_per_reporting\": -1,\n","    \"min_train_timesteps_per_reporting\": -1,\n","    \"min_sample_timesteps_per_reporting\": -1,\n","    \"input_evaluation\": -1,\n","    \"policy_map_cache\": -1,\n","    \"worker_cls\": -1,\n","    \"synchronize_filters\": -1,\n","    \"enable_async_evaluation\": -1,\n","    \"custom_async_evaluation_function\": -1,\n","    \"_enable_rl_module_api\": -1,\n","    \"auto_wrap_old_gym_envs\": -1,\n","    \"always_attach_evaluation_results\": -1,\n","    \"buffer_size\": -1,\n","    \"prioritized_replay\": -1,\n","    \"learning_starts\": -1,\n","    \"replay_batch_size\": -1,\n","    \"replay_sequence_length\": None,\n","    \"replay_mode\": -1,\n","    \"prioritized_replay_alpha\": -1,\n","    \"prioritized_replay_beta\": -1,\n","    \"prioritized_replay_eps\": -1,\n","    \"_disable_execution_plan_api\": -1,\n","    \"epsilon\": [(0, 1.0), (10000, 0.05)],\n","    \"target_network_update_freq\": 500,\n","    \"num_steps_sampled_before_learning_starts\": 1000,\n","    \"store_buffer_in_checkpoints\": False,\n","    \"adam_epsilon\": 1e-08,\n","    \"tau\": 1.0,\n","    \"num_atoms\": 1,\n","    \"v_min\": -10.0,\n","    \"v_max\": 10.0,\n","    \"noisy\": False,\n","    \"sigma0\": 0.5,\n","    \"dueling\": True,\n","    \"hiddens\": [256],\n","    \"double_q\": True,\n","    \"n_step\": 1,\n","    \"before_learn_on_batch\": None,\n","    \"training_intensity\": None,\n","    \"td_error_loss_fn\": \"huber\",\n","    \"categorical_distribution_temperature\": 1.0,\n","    \"replay_buffer_config\": {\n","        \"type\": \"PrioritizedEpisodeReplayBuffer\",\n","        \"capacity\": 50000,\n","        \"alpha\": 0.6,\n","        \"beta\": 0.4\n","    },\n","    \"lr_schedule\": None\n","}"]},{"cell_type":"markdown","metadata":{},"source":["# PPO 默认配置（IMPALA/APPO）\n"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["# 基类\n","from ray.rllib.core.rl_module.rl_module import RLModule\n","from ray.rllib.core.rl_module.torch import TorchRLModule\n","from ray.rllib.algorithms.ppo.ppo_rl_module import PPORLModule\n","from ray.rllib.algorithms.ppo.torch.ppo_torch_rl_module import PPOTorchRLModule\n","\n","# API 类\n","from ray.rllib.core.rl_module.apis import InferenceOnlyAPI, ValueFunctionAPI"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["TorchCNNEncoder(\n","  (net): Sequential(\n","    (0): TorchCNN(\n","      (cnn): Sequential(\n","        (0): ZeroPad2d((2, 2, 2, 2))\n","        (1): Conv2d(3, 16, kernel_size=(8, 8), stride=(4, 4))\n","        (2): ReLU()\n","        (3): ZeroPad2d((1, 2, 1, 2))\n","        (4): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2))\n","        (5): ReLU()\n","      )\n","    )\n","    (1): Flatten(start_dim=1, end_dim=-1)\n","  )\n",")\n","torch.Size([32, 84, 84, 3])\n","{'encoder_out': tensor([[0.0000, 0.0233, 0.0000,  ..., 0.0138, 0.0000, 0.0348],\n","        [0.0000, 0.0264, 0.0046,  ..., 0.0392, 0.0000, 0.3358],\n","        [0.0000, 0.0000, 0.0159,  ..., 0.2468, 0.0000, 0.1673],\n","        ...,\n","        [0.0000, 0.0000, 0.0000,  ..., 0.1377, 0.0000, 0.2146],\n","        [0.0000, 0.0000, 0.0000,  ..., 0.1896, 0.0000, 0.0087],\n","        [0.0000, 0.0000, 0.1356,  ..., 0.1044, 0.0000, 0.0186]],\n","       grad_fn=<ViewBackward0>)}\n"]}],"source":["# 默认的编码器\n","from ray.rllib.core.models.configs import (\n","    CNNEncoderConfig,\n","    MLPEncoderConfig,\n","    RecurrentEncoderConfig,\n",")\n","\n","# config = MLPEncoderConfig(\n","#     input_dims=[2],\n","#     hidden_layer_dims=[8, 8],\n","#     hidden_layer_activation=\"silu\",\n","#     hidden_layer_use_layernorm=True,\n","#     hidden_layer_use_bias=False,\n","#     output_layer_dim=4,\n","#     output_layer_activation=\"tanh\",\n","#     output_layer_use_bias=False,\n","# )\n","# model = config.build(framework=\"torch\")\n","# print(model)\n","\n","config = CNNEncoderConfig(\n","    input_dims=[84, 84, 3],  # must be 3D tensor (image: w x h x C)\n","    cnn_filter_specifiers=[\n","        [16, [8, 8], 4],\n","        [32, [4, 4], 2],\n","    ],\n","    cnn_activation=\"relu\",\n","    cnn_use_layernorm=False,\n","    cnn_use_bias=True,\n",")\n","model = config.build(framework=\"torch\")\n","print(model)\n","\n","# 创建一个batch的输入数据\n","batch_size = 32\n","import torch\n","input_tensor = torch.randn(batch_size, 84, 84, 3)  # PyTorch格式\n","print(input_tensor.shape)\n","\n","# 前向传播\n","output = model({'obs': input_tensor})\n","print(output)\n"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["ConvFCNet(\n","  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (fc): Linear(in_features=65536, out_features=10, bias=True)\n",")\n","torch.Size([64, 3, 64, 64])\n"]},{"ename":"TypeError","evalue":"conv2d() received an invalid combination of arguments - got (dict, Parameter, Parameter, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, tuple of ints padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!dict!, !Parameter!, !Parameter!, !tuple of (int, int)!, !tuple of (int, int)!, !tuple of (int, int)!, int)\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, str padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!dict!, !Parameter!, !Parameter!, !tuple of (int, int)!, !tuple of (int, int)!, !tuple of (int, int)!, int)\n","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[1;32mIn[6], line 58\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28mprint\u001b[39m(input_tensor\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# 前向传播\u001b[39;00m\n\u001b[1;32m---> 58\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[43mColumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOBS\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28mprint\u001b[39m(output)\n","File \u001b[1;32md:\\programs\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32md:\\programs\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[1;32mIn[6], line 28\u001b[0m, in \u001b[0;36mConvFCNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 28\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     29\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Flatten the output of convolutional layer\u001b[39;00m\n\u001b[0;32m     30\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(x)\n","File \u001b[1;32md:\\programs\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32md:\\programs\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[1;32md:\\programs\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32md:\\programs\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[1;31mTypeError\u001b[0m: conv2d() received an invalid combination of arguments - got (dict, Parameter, Parameter, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, tuple of ints padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!dict!, !Parameter!, !Parameter!, !tuple of (int, int)!, !tuple of (int, int)!, !tuple of (int, int)!, int)\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, str padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!dict!, !Parameter!, !Parameter!, !tuple of (int, int)!, !tuple of (int, int)!, !tuple of (int, int)!, int)\n"]}],"source":["# 自定义 ModelConfig\n","from ray.rllib.core.models.configs import ModelConfig\n","from ray.rllib.core.models.torch.encoder import TorchModel, Encoder\n","from dataclasses import dataclass\n","from ray.rllib.core.models.base import ENCODER_OUT\n","from ray.rllib.core.columns import Columns\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class ConvFCNet(TorchModel, Encoder):\n","    def __init__(self, config) -> None:\n","        TorchModel.__init__(self, config)\n","        Encoder.__init__(self, config)\n","    \n","        self.conv1 = nn.Conv2d(in_channels=config.input_dims[0], out_channels=16, kernel_size=3, stride=1, padding=1)\n","        conv1_out = (config.input_dims[1] // 1) * (config.input_dims[2] // 1) * 16  # 64 * 64 * 16\n","        self.fc = nn.Linear(conv1_out, config.out_dim)\n","\n","    def _forward(self, inputs: dict, **kwargs) -> dict:\n","        x = F.relu(self.conv1(inputs[Columns.OBS]))\n","        x = x.view(x.size(0), -1)  # Flatten the output of convolutional layer\n","        x = self.fc(x)\n","\n","        return {ENCODER_OUT: x}\n","\n","@dataclass\n","class test_EncoderConfig(ModelConfig):\n","    input_dims = None\n","    out_dim = 10\n","    def build(self, framework: str = \"torch\"):\n","        if framework == \"torch\":\n","            # 一个卷积层 + 全连接层\n","            return ConvFCNet(self)\n","\n","        else:\n","            raise ValueError(f'only torch ModelConfig')\n","\n","    @property\n","    def output_dims(self):\n","        \"\"\"Read-only `output_dims` are inferred automatically from other settings.\"\"\"\n","        return self.out_dim\n","    \n","net = test_EncoderConfig([3, 64, 64], 10).build()\n","print(net)\n","\n","batch_size = 64\n","input_tensor = torch.randn(batch_size, 3, 64, 64)  # PyTorch格式\n","print(input_tensor.shape)\n","\n","# 前向传播\n","output = net({Columns.OBS: input_tensor})\n","print(output)"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["# 用于自定义 PPO 模型\n","from ray.rllib.algorithms.ppo.ppo_catalog import PPOCatalog\n","\n","\"\"\"\n","1. 自定义编码器a \n","    - 继承 PPOCatalog 类\n","        - 重写 Catalog.build_encoder方法\n","        - 应该根据自定义的 encoder 模型 覆盖属性 `Catalog.latent_dims`，以便可以使用该信息来构建头部。\n","    - 继承重写算法配置类(PPOConfig) \n","        @override(AlgorithmConfig)\n","        def get_default_rl_module_spec(self) -> RLModuleSpec:\n","            from ray.rllib.algorithms.ppo.ppo_catalog import PPOCatalog\n","\n","            if self.framework_str == \"torch\":\n","                from ray.rllib.algorithms.ppo.torch.ppo_torch_rl_module import (\n","                    PPOTorchRLModule,\n","                )\n","\n","                return RLModuleSpec(module_class=PPOTorchRLModule, catalog_class=PPOCatalog) << return RLModuleSpec(module_class=PPOTorchRLModule, catalog_class=Custom_Catalog)\n","            elif self.framework_str == \"tf2\":\n","                from ray.rllib.algorithms.ppo.tf.ppo_tf_rl_module import PPOTfRLModule\n","\n","                return RLModuleSpec(module_class=PPOTfRLModule, catalog_class=PPOCatalog)\n","            else:\n","                raise ValueError(\n","                    f\"The framework {self.framework_str} is not supported. \"\n","                    \"Use either 'torch' or 'tf2'.\"\n","                ) \n","\n","2. 自定义编码器b\n","    - 继承 ModelConfig，自定义 ModelConfig\n","        from ray.rllib.core.models.configs import ModelConfig\n","\n","3. 调整编码器\n","    CNNEncoderConfig\n","    MLPEncoderConfig\n","    RecurrentEncoderConfig(循环神经网络)\n","\n","    通过配置调整\n","    config.rl_module(\n","        model_config={\n","            # MLPEncoderConfig\n","            \"fcnet_hiddens\": [5, 3, 3],\n","            \"fcnet_kernel_initializer\": None,\n","            \"fcnet_kernel_initializer_kwargs\": {},\n","            \"fcnet_bias_initializer\": None,\n","            \"fcnet_bias_initializer_kwargs\": {},\n","\n","            # CNNEncoderConfig\n","            \"conv_filters\": [\n","                [32, [8, 8], 4],  # [输出通道数, [kernel_size_h, kernel_size_w], stride]\n","                [64, [4, 4], 2],  # [64个通道, 4x4卷积核, stride=2]\n","                [64, [3, 3], 1],  # [64个通道, 3x3卷积核, stride=1] \n","            ],\n","\n","            # ...\n","        },\n","    )\n","\n","            \"adam_epsilon\": 1e-8,        grad_lp\":40.0,\n","            \"num_sp_saple_before_nng_sat\":10000,\n","        \"ta\": 1,   \"\"b ectoue\"num_ iRms\":M51, \"v_mn\":-10.0,\n","          \"v_mx\": 100,\"osy\":Tre,\n","         \"iga0\": 0.5,\n","\n","    RLlib's native RLModules get their Models from a Catalog object.\n","    By default, that Catalog builds the configs it has as attributes.\n","    This component was build to be hackable and extensible. You can inject custom\n","    components into RL Modules by overriding the `build_xxx` methods of this class.\n","    Note that it is recommended to write a custom RL Module for a single use-case.\n","    Modifications to Catalogs mostly make sense if you want to reuse the same\n","    Catalog for different RL Modules. For example if you have written a custom\n","    encoder and want to inject it into different RL Modules (e.g. for PPO, DQN, etc.).\n","    You can influence the decision tree that determines the sub-components by modifying\n","    `Catalog._determine_components_hook`.\n","\n","    Usage example:\n","\n","    # Define a custom catalog\n","\n","    .. testcode::\n","\n","        import torch\n","        import gymnasium as gym\n","        from ray.rllib.core.models.configs import MLPHeadConfig\n","        from ray.rllib.core.models.catalog import Catalog\n","\n","        class MyCatalog(Catalog):\n","            def __init__(\n","                self,\n","                observation_space: gym.Space,\n","                action_space: gym.Space,\n","                model_config_dict: dict,\n","            ):\n","                super().__init__(observation_space, action_space, model_config_dict)\n","                self.my_model_config = MLPHeadConfig(\n","                    hidden_layer_dims=[64, 32],\n","                    input_dims=[self.observation_space.shape[0]],\n","                )\n","\n","            def build_my_head(self, framework: str):\n","                return self.my_model_config.build(framework=framework)\n","\n","        # With that, RLlib can build and use models from this catalog like this:\n","        catalog = MyCatalog(gym.spaces.Box(0, 1), gym.spaces.Box(0, 1), {})\n","        my_head = catalog.build_my_head(framework=\"torch\")\n","\n","        # Make a call to the built model.\n","        out = my_head(torch.Tensor([[1]]))\n","    \"\"\"\n","    \"\"\"描述用于 RL 模块的子模块架构。\n","\n","    RLlib 的原生 RL 模块从 Catalog 对象获取其模型。\n","    默认情况下，该 Catalog 会构建其作为属性拥有的配置。\n","    此组件被构建为可hack和可扩展的。您可以通过重写此类的 `build_xxx` 方法，\n","    将自定义组件注入到 RL 模块中。\n","    请注意，建议为单个用例编写自定义 RL 模块。\n","    对 Catalog 的修改主要在您想要为不同的 RL 模块重用相同的 Catalog 时才有意义。\n","    例如，如果您编写了一个自定义编码器并希望将其注入到不同的 RL 模块\n","    （例如，PPO、DQN 等）。您可以通过修改\n","    `Catalog._determine_components_hook` 来影响决定子组件的决策树。\n","\n","    使用示例：\n","\n","    # 定义一个自定义的 catalog\n","\n","    .. testcode::\n","\n","        import torch\n","        import gymnasium as gym\n","        from ray.rllib.core.models.configs import MLPHeadConfig\n","        from ray.rllib.core.models.catalog import Catalog\n","\n","        class MyCatalog(Catalog):\n","            def __init__(\n","                self,\n","                observation_space: gym.Space,\n","                action_space: gym.Space,\n","                model_config_dict: dict,\n","            ):\n","                super().__init__(observation_space, action_space, model_config_dict)\n","                self.my_model_config = MLPHeadConfig(\n","                    hidden_layer_dims=[64, 32],\n","                    input_dims=[self.observation_space.shape[0]],\n","                )\n","\n","            def build_my_head(self, framework: str):\n","                return self.my_model_config.build(framework=framework)\n","\n","        # 有了这个，RLlib 可以像这样从这个 catalog 构建和使用模型：\n","        catalog = MyCatalog(gym.spaces.Box(0, 1), gym.spaces.Box(0, 1), {})\n","        my_head = catalog.build_my_head(framework=\"torch\")\n","\n","        # 对构建的模型进行调用。\n","        out = my_head(torch.Tensor([[1]]))\n","    \"\"\"\n","\n","    # TODO (Sven): Add `framework` arg to c'tor and remove this arg from `build`\n","    #  methods. This way, we can already know in the c'tor of Catalog, what the exact\n","    #  action distibution objects are and thus what the output dims for e.g. a pi-head\n","    #  will be.\n","    def __init__(\n","        self,\n","        observation_space: gym.Space,\n","        action_space: gym.Space,\n","        model_config_dict: dict,\n","        # deprecated args.\n","        view_requirements=DEPRECATED_VALUE,\n","    ):\n","        \"\"\"Initializes a Catalog with a default encoder config.\n","\n","        Args:\n","            observation_space: The observation space of the environment.\n","            action_space: The action space of the environment.\n","            model_config_dict: The model config that specifies things like hidden\n","                dimensions and activations functions to use in this Catalog.\n","        \"\"\"\n","        if view_requirements != DEPRECATED_VALUE:\n","            deprecation_warning(old=\"Catalog(view_requirements=..)\", error=True)\n","\n","        # TODO (sven): The following logic won't be needed anymore, once we get rid of\n","        #  Catalogs entirely. We will assert directly inside the algo's DefaultRLModule\n","        #  class that the `model_config` is a DefaultModelConfig. Thus users won't be\n","        #  able to pass in partial config dicts into a default model (alternatively, we\n","        #  could automatically augment the user provided dict by the default config\n","        #  dataclass object only(!) for default modules).\n","        # TODO (sven): 一旦我们完全摆脱 Catalog，以下逻辑将不再需要。\n","        # 我们将直接在算法的 DefaultRLModule 类中断言 `model_config` 是一个 DefaultModelConfig。\n","        # 因此，用户将无法将部分配置字典传递到默认模型中（或者，我们可以仅为默认模块自动\n","        # 通过默认配置数据类对象(!)来增强用户提供的字典）。\n","        if dataclasses.is_dataclass(model_config_dict):\n","            model_config_dict = dataclasses.asdict(model_config_dict)\n","        default_config = dataclasses.asdict(DefaultModelConfig())\n","        # end: TODO\n","\n","        self.observation_space = observation_space\n","        self.action_space = action_space\n","\n","        self._model_config_dict = default_config | model_config_dict\n","        self._latent_dims = None\n","\n","        self._determine_components_hook()\n","\n","    @OverrideToImplementCustomLogic_CallToSuperRecommended\n","    def _determine_components_hook(self):\n","        \"\"\"Decision tree hook for subclasses to override.\n","\n","        By default, this method executes the decision tree that determines the\n","        components that a Catalog builds. You can extend the components by overriding\n","        this or by adding to the constructor of your subclass.\n","\n","        Override this method if you don't want to use the default components\n","        determined here. If you want to use them but add additional components, you\n","        should call `super()._determine_components()` at the beginning of your\n","        implementation.\n","\n","        This makes it so that subclasses are not forced to create an encoder config\n","        if the rest of their catalog is not dependent on it or if it breaks.\n","        At the end of this method, an attribute `Catalog.latent_dims`\n","        should be set so that heads can be built using that information.\n","        \"\"\"\n","        \"\"\"子类可以重写的决策树钩子。\n","\n","        默认情况下，此方法执行决策树，该决策树确定 Catalog 构建的组件。\n","        您可以通过重写此方法或在子类的构造函数中添加内容来扩展组件。\n","\n","        如果您不想使用此处确定的默认组件，请重写此方法。\n","        如果您想使用它们但添加额外的组件，您应该在实现开始时调用\n","        `super()._determine_components()`。\n","\n","        这使得子类不必创建编码器配置，如果它们 Catalog 的其余部分不依赖于它，或者它会破坏逻辑。\n","        在此方法的最后，应该设置属性 `Catalog.latent_dims`，以便可以使用该信息来构建头部。\n","        \"\"\"\n","\n","        self._encoder_config = self._get_encoder_config(\n","            observation_space=self.observation_space,\n","            action_space=self.action_space,\n","            model_config_dict=self._model_config_dict,\n","        )\n","\n","        # Create a function that can be called when framework is known to retrieve the\n","        # class type for action distributions\n","        self._action_dist_class_fn = functools.partial(\n","            self._get_dist_cls_from_action_space, action_space=self.action_space\n","        )\n","\n","        # The dimensions of the latent vector that is output by the encoder and fed\n","        # to the heads.\n","        self.latent_dims = self._encoder_config.output_dims\n","\n","    @property\n","    def latent_dims(self):\n","        \"\"\"Returns the latent dimensions of the encoder.\n","\n","        This establishes an agreement between encoder and heads about the latent\n","        dimensions. Encoders can be built to output a latent tensor with\n","        `latent_dims` dimensions, and heads can be built with tensors of\n","        `latent_dims` dimensions as inputs. This can be safely ignored if this\n","        agreement is not needed in case of modifications to the Catalog.\n","\n","        Returns:\n","            The latent dimensions of the encoder.\n","        \"\"\"\n","        \n","        \"\"\"返回编码器的潜在维度。\n","\n","        这在编码器和头部之间建立了关于潜在维度的一致性。\n","        可以构建编码器以输出具有 latent_dims 维度的潜在张量，并且可以构建头部，\n","        将具有 latent_dims 维度的张量作为输入。\n","        如果不需要对目录进行修改的情况下达成此一致性，则可以安全地忽略此内容。\n","\n","        返回：\n","        编码器的潜在维度。\n","        \"\"\"\n","        return self._latent_dims\n","\n","    @latent_dims.setter\n","    def latent_dims(self, value):\n","        self._latent_dims = value\n","\n","    @OverrideToImplementCustomLogic\n","    def build_encoder(self, framework: str) -> Encoder:\n","        \"\"\"Builds the encoder.\n","\n","        By default, this method builds an encoder instance from Catalog._encoder_config.\n","\n","        You should override this if you want to use RLlib's default RL Modules but\n","        only want to change the encoder. For example, if you want to use a custom\n","        encoder, but want to use RLlib's default heads, action distribution and how\n","        tensors are routed between them. If you want to have full control over the\n","        RL Module, we recommend writing your own RL Module by inheriting from one of\n","        RLlib's RL Modules instead.\n","\n","        Args:\n","            framework: The framework to use. Either \"torch\" or \"tf2\".\n","\n","        Returns:\n","            The encoder.\n","        \"\"\"\n","        \"\"\"构建编码器。\n","\n","        默认情况下，此方法从 Catalog._encoder_config 构建一个编码器实例。\n","\n","        如果您想使用 RLlib 的默认 RL 模块，但只想更改编码器，则应重写此方法。\n","        例如，如果您想使用自定义编码器，但想使用 RLlib 的默认头、动作分布以及张量如何在它们之间路由。\n","        如果您想完全控制 RL 模块，\n","        我们建议您通过继承 RLlib 的 RL 模块之一来编写自己的 RL 模块。\n","\n","        Args:\n","        framework: 要使用的框架。可以是 \"torch\" 或 \"tf2\"。\n","\n","        Returns:\n","        编码器。\n","        \"\"\"\n","        assert hasattr(self, \"_encoder_config\"), (\n","            \"You must define a `Catalog._encoder_config` attribute in your Catalog \"\n","            \"subclass or override the `Catalog.build_encoder` method. By default, \"\n","            \"an encoder_config is created in the __post_init__ method.\"\n","        )\n","        return self._encoder_config.build(framework=framework)\n","\n","    @OverrideToImplementCustomLogic\n","    def get_action_dist_cls(self, framework: str):\n","        \"\"\"Get the action distribution class.\n","\n","        The default behavior is to get the action distribution from the\n","        `Catalog._action_dist_class_fn`.\n","\n","        You should override this to have RLlib build your custom action\n","        distribution instead of the default one. For example, if you don't want to\n","        use RLlib's default RLModules with their default models, but only want to\n","        change the distribution that Catalog returns.\n","\n","        Args:\n","            framework: The framework to use. Either \"torch\" or \"tf2\".\n","\n","        Returns:\n","            The action distribution.\n","        \"\"\"\n","        \"\"\"获取动作分布类。\n","\n","        默认行为是从 Catalog._action_dist_class_fn 获取动作分布。\n","\n","        您应该重写此方法，以便让 RLlib 构建您的自定义动作分布，而不是默认的动作分布。\n","        例如，如果您不想使用 RLlib 的默认 RL 模块及其默认模型，而只想更改 Catalog 返回的分布。\n","\n","        Args:\n","        framework: 要使用的框架。可以是 \"torch\" 或 \"tf2\"。\n","\n","        Returns:\n","        动作分布。\n","        \"\"\"\n","        assert hasattr(self, \"_action_dist_class_fn\"), (\n","            \"You must define a `Catalog._action_dist_class_fn` attribute in your \"\n","            \"Catalog subclass or override the `Catalog.action_dist_class_fn` method. \"\n","            \"By default, an action_dist_class_fn is created in the __post_init__ \"\n","            \"method.\"\n","        )\n","        return self._action_dist_class_fn(framework=framework)\n","\n","    @classmethod\n","    def _get_encoder_config(\n","        cls,\n","        observation_space: gym.Space,\n","        model_config_dict: dict,\n","        action_space: gym.Space = None,\n","    ) -> ModelConfig:\n","        \"\"\"Returns an EncoderConfig for the given input_space and model_config_dict.\n","\n","        Encoders are usually used in RLModules to transform the input space into a\n","        latent space that is then fed to the heads. The returned EncoderConfig\n","        objects correspond to the built-in Encoder classes in RLlib.\n","        For example, for a simple 1D-Box input_space, RLlib offers an\n","        MLPEncoder, hence this method returns the MLPEncoderConfig. You can overwrite\n","        this method to produce specific EncoderConfigs for your custom Models.\n","\n","        The following input spaces lead to the following configs:\n","        - 1D-Box: MLPEncoderConfig\n","        - 3D-Box: CNNEncoderConfig\n","        # TODO (Artur): Support more spaces here\n","        # ...\n","\n","        Args:\n","            observation_space: The observation space to use.\n","            model_config_dict: The model config to use.\n","            action_space: The action space to use if actions are to be encoded. This\n","                is commonly the case for LSTM models.\n","\n","        Returns:\n","            The encoder config.\n","        \"\"\"\n","        \"\"\"返回给定 input_space 和 model_config_dict 的 EncoderConfig。\n","\n","        编码器通常在 RLModules 中使用，用于将输入空间转换为一个潜在空间，然后将其传递给头部。\n","        返回的 EncoderConfig 对象对应于 RLlib 中的内置编码器类。\n","        例如，对于一个简单的 1D-Box 输入空间，RLlib 提供了 MLPEncoder\n","        ，因此此方法返回 MLPEncoderConfig。\n","        您可以重写此方法以生成特定的 EncoderConfig 以用于您的自定义模型。\n","\n","        以下输入空间会导致以下配置：\n","\n","        1D-Box: MLPEncoderConfig\n","        3D-Box: CNNEncoderConfig\n","        TODO (Artur): 在此处支持更多空间\n","        ...\n","        Args:\n","        observation_space: 要使用的观测空间。\n","        model_config_dict: 要使用的模型配置。\n","        action_space: 如果动作需要编码，则要使用的动作空间。这在 LSTM 模型的情况下通常是这样。\n","\n","        Returns:\n","        编码器配置。\n","        \"\"\"\n","\n","        activation = model_config_dict[\"fcnet_activation\"]\n","        output_activation = model_config_dict[\"fcnet_activation\"]\n","        use_lstm = model_config_dict[\"use_lstm\"]\n","\n","        if use_lstm:\n","            encoder_config = RecurrentEncoderConfig(\n","                input_dims=observation_space.shape,\n","                recurrent_layer_type=\"lstm\",\n","                hidden_dim=model_config_dict[\"lstm_cell_size\"],\n","                hidden_weights_initializer=model_config_dict[\"lstm_kernel_initializer\"],\n","                hidden_weights_initializer_config=model_config_dict[\n","                    \"lstm_kernel_initializer_kwargs\"\n","                ],\n","                hidden_bias_initializer=model_config_dict[\"lstm_bias_initializer\"],\n","                hidden_bias_initializer_config=model_config_dict[\n","                    \"lstm_bias_initializer_kwargs\"\n","                ],\n","                batch_major=True,\n","                num_layers=1,\n","                tokenizer_config=cls.get_tokenizer_config(\n","                    observation_space,\n","                    model_config_dict,\n","                ),\n","            )\n","        else:\n","            # TODO (Artur): Maybe check for original spaces here\n","            # input_space is a 1D Box\n","            if isinstance(observation_space, Box) and len(observation_space.shape) == 1:\n","                # In order to guarantee backward compatability with old configs,\n","                # we need to check if no latent dim was set and simply reuse the last\n","                # fcnet hidden dim for that purpose.\n","                hidden_layer_dims = model_config_dict[\"fcnet_hiddens\"][:-1]\n","                encoder_latent_dim = model_config_dict[\"fcnet_hiddens\"][-1]\n","                encoder_config = MLPEncoderConfig(\n","                    input_dims=observation_space.shape,\n","                    hidden_layer_dims=hidden_layer_dims,\n","                    hidden_layer_activation=activation,\n","                    hidden_layer_weights_initializer=model_config_dict[\n","                        \"fcnet_kernel_initializer\"\n","                    ],\n","                    hidden_layer_weights_initializer_config=model_config_dict[\n","                        \"fcnet_kernel_initializer_kwargs\"\n","                    ],\n","                    hidden_layer_bias_initializer=model_config_dict[\n","                        \"fcnet_bias_initializer\"\n","                    ],\n","                    hidden_layer_bias_initializer_config=model_config_dict[\n","                        \"fcnet_bias_initializer_kwargs\"\n","                    ],\n","                    output_layer_dim=encoder_latent_dim,\n","                    output_layer_activation=output_activation,\n","                    output_layer_weights_initializer=model_config_dict[\n","                        \"fcnet_kernel_initializer\"\n","                    ],\n","                    output_layer_weights_initializer_config=model_config_dict[\n","                        \"fcnet_kernel_initializer_kwargs\"\n","                    ],\n","                    output_layer_bias_initializer=model_config_dict[\n","                        \"fcnet_bias_initializer\"\n","                    ],\n","                    output_layer_bias_initializer_config=model_config_dict[\n","                        \"fcnet_bias_initializer_kwargs\"\n","                    ],\n","                )\n","\n","            # input_space is a 3D Box\n","            elif (\n","                isinstance(observation_space, Box) and len(observation_space.shape) == 3\n","            ):\n","                if not model_config_dict.get(\"conv_filters\"):\n","                    model_config_dict[\"conv_filters\"] = get_filter_config(\n","                        observation_space.shape\n","                    )\n","\n","                encoder_config = CNNEncoderConfig(\n","                    input_dims=observation_space.shape,\n","                    cnn_filter_specifiers=model_config_dict[\"conv_filters\"],\n","                    cnn_activation=model_config_dict[\"conv_activation\"],\n","                    cnn_kernel_initializer=model_config_dict[\"conv_kernel_initializer\"],\n","                    cnn_kernel_initializer_config=model_config_dict[\n","                        \"conv_kernel_initializer_kwargs\"\n","                    ],\n","                    cnn_bias_initializer=model_config_dict[\"conv_bias_initializer\"],\n","                    cnn_bias_initializer_config=model_config_dict[\n","                        \"conv_bias_initializer_kwargs\"\n","                    ],\n","                )\n","            # input_space is a 2D Box\n","            elif (\n","                isinstance(observation_space, Box) and len(observation_space.shape) == 2\n","            ):\n","                # RLlib used to support 2D Box spaces by silently flattening them\n","                raise ValueError(\n","                    f\"No default encoder config for obs space={observation_space},\"\n","                    f\" lstm={use_lstm} found. 2D Box \"\n","                    f\"spaces are not supported. They should be either flattened to a \"\n","                    f\"1D Box space or enhanced to be a 3D box space.\"\n","                )\n","            # input_space is a possibly nested structure of spaces.\n","            else:\n","                # NestedModelConfig\n","                raise ValueError(\n","                    f\"No default encoder config for obs space={observation_space},\"\n","                    f\" lstm={use_lstm} found.\"\n","                )\n","\n","        return encoder_config\n","\n","    @classmethod\n","    @OverrideToImplementCustomLogic\n","    def get_tokenizer_config(\n","        cls,\n","        observation_space: gym.Space,\n","        model_config_dict: dict,\n","        # deprecated args.\n","        view_requirements=DEPRECATED_VALUE,\n","    ) -> ModelConfig:\n","        \"\"\"Returns a tokenizer config for the given space.\n","\n","        This is useful for recurrent / transformer models that need to tokenize their\n","        inputs. By default, RLlib uses the models supported by Catalog out of the box to\n","        tokenize.\n","\n","        You should override this method if you want to change the custom tokenizer\n","        inside current encoders that Catalog returns without providing the recurrent\n","        network as a whole. For example, if you want to define some custom CNN layers\n","        as a tokenizer for a recurrent encoder that already includes the recurrent\n","        layers and handles the state.\n","\n","        Args:\n","            observation_space: The observation space to use.\n","            model_config_dict: The model config to use.\n","        \"\"\"\n","        \"\"\"返回给定空间的 tokenizer 配置。\n","\n","        这对于需要对其输入进行标记化的循环/transformer 模型非常有用。\n","        默认情况下，RLlib 使用 Catalog 开箱即用的模型进行标记化。\n","\n","        如果您想更改 Catalog 返回的当前编码器内的自定义 tokenizer，\n","        而无需提供整个循环网络，则应重写此方法。\n","        例如，如果您想将一些自定义 CNN 层定义为循环编码器的 tokenizer，\n","        该循环编码器已经包含循环层并处理状态。\n","\n","        Args:\n","        observation_space: 要使用的观测空间。\n","        model_config_dict: 要使用的模型配置。\n","        \"\"\"\n","        if view_requirements != DEPRECATED_VALUE:\n","            deprecation_warning(old=\"Catalog(view_requirements=..)\", error=True)\n","\n","        return cls._get_encoder_config(\n","            observation_space=observation_space,\n","            # Use model_config_dict without flags that would end up in complex models\n","            model_config_dict={\n","                **model_config_dict,\n","                **{\"use_lstm\": False, \"use_attention\": False},\n","            },\n","        )\n","\n","    @classmethod\n","    def _get_dist_cls_from_action_space(\n","        cls,\n","        action_space: gym.Space,\n","        *,\n","        framework: Optional[str] = None,\n","    ) -> Distribution:\n","        \"\"\"Returns a distribution class for the given action space.\n","\n","        You can get the required input dimension for the distribution by calling\n","        `action_dict_cls.required_input_dim(action_space)`\n","        on the retrieved class. This is useful, because the Catalog needs to find out\n","        about the required input dimension for the distribution before the model that\n","        outputs these inputs is configured.\n","\n","        Args:\n","            action_space: Action space of the target gym env.\n","            framework: The framework to use.\n","\n","        Returns:\n","            The distribution class for the given action space.\n","        \"\"\"\n","        \n","        \"\"\"返回给定动作空间的分布类。\n","\n","        您可以通过在检索到的类上调用 action_dict_cls.required_input_dim(action_space) 获取分布所需的输入维度。\n","        这对于 Catalog 在配置输出这些输入的模型之前需要了解分布所需的输入维度非常有用。\n","\n","        Args:\n","        action_space: 目标 Gym 环境的动作空间。\n","        framework: 要使用的框架。\n","\n","        Returns:\n","        给定动作空间的分布类。\n","        \"\"\"\n","        # If no framework provided, return no action distribution class (None).\n","        if framework is None:\n","            return None\n","        # This method is structured in two steps:\n","        # Firstly, construct a dictionary containing the available distribution classes.\n","        # Secondly, return the correct distribution class for the given action space.\n","\n","        # Step 1: Construct the dictionary.\n","\n","        class DistEnum(enum.Enum):\n","            Categorical = \"Categorical\"\n","            DiagGaussian = \"Gaussian\"\n","            Deterministic = \"Deterministic\"\n","            MultiDistribution = \"MultiDistribution\"\n","            MultiCategorical = \"MultiCategorical\"\n","\n","        if framework == \"torch\":\n","            from ray.rllib.models.torch.torch_distributions import (\n","                TorchCategorical,\n","                TorchDeterministic,\n","                TorchDiagGaussian,\n","            )\n","\n","            distribution_dicts = {\n","                DistEnum.Deterministic: TorchDeterministic,\n","                DistEnum.DiagGaussian: TorchDiagGaussian,\n","                DistEnum.Categorical: TorchCategorical,\n","            }\n","        elif framework == \"tf2\":\n","            from ray.rllib.models.tf.tf_distributions import (\n","                TfCategorical,\n","                TfDeterministic,\n","                TfDiagGaussian,\n","            )\n","\n","            distribution_dicts = {\n","                DistEnum.Deterministic: TfDeterministic,\n","                DistEnum.DiagGaussian: TfDiagGaussian,\n","                DistEnum.Categorical: TfCategorical,\n","            }\n","        else:\n","            raise ValueError(\n","                f\"Unknown framework: {framework}. Only 'torch' and 'tf2' are \"\n","                \"supported for RLModule Catalogs.\"\n","            )\n","\n","        # Only add a MultiAction distribution class to the dict if we can compute its\n","        # components (we need a Tuple/Dict space for this).\n","        if isinstance(action_space, (Tuple, Dict)):\n","            partial_multi_action_distribution_cls = _multi_action_dist_partial_helper(\n","                catalog_cls=cls,\n","                action_space=action_space,\n","                framework=framework,\n","            )\n","\n","            distribution_dicts[\n","                DistEnum.MultiDistribution\n","            ] = partial_multi_action_distribution_cls\n","\n","        # Only add a MultiCategorical distribution class to the dict if we can compute\n","        # its components (we need a MultiDiscrete space for this).\n","        if isinstance(action_space, MultiDiscrete):\n","            partial_multi_categorical_distribution_cls = (\n","                _multi_categorical_dist_partial_helper(\n","                    action_space=action_space,\n","                    framework=framework,\n","                )\n","            )\n","\n","            distribution_dicts[\n","                DistEnum.MultiCategorical\n","            ] = partial_multi_categorical_distribution_cls\n","\n","        # Step 2: Return the correct distribution class for the given action space.\n","\n","        # Box space -> DiagGaussian OR Deterministic.\n","        if isinstance(action_space, Box):\n","            if action_space.dtype.char in np.typecodes[\"AllInteger\"]:\n","                raise ValueError(\n","                    \"Box(..., `int`) action spaces are not supported. \"\n","                    \"Use MultiDiscrete  or Box(..., `float`).\"\n","                )\n","            else:\n","                if len(action_space.shape) > 1:\n","                    raise UnsupportedSpaceException(\n","                        f\"Action space has multiple dimensions {action_space.shape}. \"\n","                        f\"Consider reshaping this into a single dimension, using a \"\n","                        f\"custom action distribution, using a Tuple action space, \"\n","                        f\"or the multi-agent API.\"\n","                    )\n","                return distribution_dicts[DistEnum.DiagGaussian]\n","\n","        # Discrete Space -> Categorical.\n","        elif isinstance(action_space, Discrete):\n","            return distribution_dicts[DistEnum.Categorical]\n","\n","        # Tuple/Dict Spaces -> MultiAction.\n","        elif isinstance(action_space, (Tuple, Dict)):\n","            return distribution_dicts[DistEnum.MultiDistribution]\n","\n","        # Simplex -> Dirichlet.\n","        elif isinstance(action_space, Simplex):\n","            # TODO(Artur): Supported Simplex (in torch).\n","            raise NotImplementedError(\"Simplex action space not yet supported.\")\n","\n","        # MultiDiscrete -> MultiCategorical.\n","        elif isinstance(action_space, MultiDiscrete):\n","            return distribution_dicts[DistEnum.MultiCategorical]\n","\n","        # Unknown type -> Error.\n","        else:\n","            raise NotImplementedError(f\"Unsupported action space: `{action_space}`\")\n","\n","    @staticmethod\n","    def get_preprocessor(observation_space: gym.Space, **kwargs) -> Preprocessor:\n","        \"\"\"Returns a suitable preprocessor for the given observation space.\n","\n","        Args:\n","            observation_space: The input observation space.\n","            **kwargs: Forward-compatible kwargs.\n","\n","        Returns:\n","            preprocessor: Preprocessor for the observations.\n","        \"\"\"\n","        \"\"\"返回适用于给定观测空间的预处理器。\n","\n","        Args:\n","            observation_space: 输入的观测空间。\n","            **kwargs: 向前兼容的 kwargs。\n","\n","        Returns:\n","            preprocessor: 观测的预处理器。\n","        \"\"\"\n","        # TODO(Artur): Since preprocessors have long been @PublicAPI with the options\n","        #  kwarg as part of their constructor, we fade out support for this,\n","        #  beginning with this entrypoint.\n","        # Next, we should deprecate the `options` kwarg from the Preprocessor itself,\n","        # after deprecating the old catalog and other components that still pass this.\n","        # TODO(Artur): 由于预处理器长期以来一直是 @PublicAPI，并且 options kwarg 是其构造函数的一部分，\n","        # 我们正在逐渐淘汰对此的支持，\n","        # 从这个入口点开始。\n","        # 接下来，在弃用旧的 catalog 和其他仍然传递此参数的组件之后，\n","        # 我们应该弃用 Preprocessor 本身的 `options` kwarg。\n","        options = kwargs.get(\"options\", {})\n","        if options:\n","            deprecation_warning(\n","                old=\"get_preprocessor_for_space(..., options={...})\",\n","                help=\"Override `Catalog.get_preprocessor()` \"\n","                \"in order to implement custom behaviour.\",\n","                error=False,\n","            )\n","\n","        if options.get(\"custom_preprocessor\"):\n","            deprecation_warning(\n","                old=\"model_config['custom_preprocessor']\",\n","                help=\"Custom preprocessors are deprecated, \"\n","                \"since they sometimes conflict with the built-in \"\n","                \"preprocessors for handling complex observation spaces. \"\n","                \"Please use wrapper classes around your environment \"\n","                \"instead.\",\n","                error=True,\n","            )\n","        else:\n","            # TODO(Artur): Inline the get_preprocessor() call here once we have\n","            #  deprecated the old model catalog.\n","            cls = get_preprocessor(observation_space)\n","            prep = cls(observation_space, options)\n","            return prep\n","\n","class PPOCatalog(Catalog):\n","    \"\"\"The Catalog class used to build models for PPO.\n","\n","    PPOCatalog provides the following models:\n","        - ActorCriticEncoder: The encoder used to encode the observations.\n","        - Pi Head: The head used to compute the policy logits.\n","        - Value Function Head: The head used to compute the value function.\n","\n","    The ActorCriticEncoder is a wrapper around Encoders to produce separate outputs\n","    for the policy and value function. See implementations of PPORLModule for\n","    more details.\n","\n","    Any custom ActorCriticEncoder can be built by overriding the\n","    build_actor_critic_encoder() method. Alternatively, the ActorCriticEncoderConfig\n","    at PPOCatalog.actor_critic_encoder_config can be overridden to build a custom\n","    ActorCriticEncoder during RLModule runtime.\n","\n","    Any custom head can be built by overriding the build_pi_head() and build_vf_head()\n","    methods. Alternatively, the PiHeadConfig and VfHeadConfig can be overridden to\n","    build custom heads during RLModule runtime.\n","\n","    Any module built for exploration or inference is built with the flag\n","    `ìnference_only=True` and does not contain a value network. This flag can be set\n","    in the `SingleAgentModuleSpec` through the `inference_only` boolean flag.\n","    In case that the actor-critic-encoder is not shared between the policy and value\n","    function, the inference-only module will contain only the actor encoder network.\n","    \"\"\"\n","    \"\"\"\n","    PPO 目录类用于构建模型。\n","\n","    PPOCatalog 提供以下模型：\n","        - ActorCriticEncoder（演员-评论员编码器）：用于对观察结果进行编码。\n","        - Pi Head（策略头部）：用于计算策略的对数（logits）。\n","        - Value Function Head（值函数头部）：用于计算值函数。\n","\n","    ActorCriticEncoder 是 Encoders 的包装器，用于为策略和值函数产生单独的输出。查看 PPORLModule 的实现以获取更多细节。\n","\n","    任何自定义的 ActorCriticEncoder 都可以通过重写 build_actor_critic_encoder() 方法进行构建。\n","    另外，可以重写 PPOCatalog.actor_critic_encoder_config 中的 ActorCriticEncoderConfig 来在 RLModule 运行时构建自定义 ActorCriticEncoder。\n","\n","    任何自定义的头部都可以通过重写 build_pi_head() 和 build_vf_head() 方法进行构建。\n","    另外，可以重写 PiHeadConfig 和 VfHeadConfig 来在 RLModule 运行时构建自定义头部。\n","\n","    任何为探索或推断构建的模块都使用标志 `inference_only=True`，不包含值网络。\n","    可以通过 `SingleAgentModuleSpec` 中的 `inference_only` 布尔标志来设置此标志。\n","\n","    如果 ActorCriticEncoder 在策略和值函数之间不共享，那么推断模块将仅包含演员编码器网络。 -> self.model_config.get(\"vf_share_layers\") \n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        observation_space: gym.Space,\n","        action_space: gym.Space,\n","        model_config_dict: dict,\n","    ):\n","        \"\"\"Initializes the PPOCatalog.\n","\n","        Args:\n","            observation_space: The observation space of the Encoder.\n","            action_space: The action space for the Pi Head.\n","            model_config_dict: The model config to use.\n","        \"\"\"\n","        super().__init__(\n","            observation_space=observation_space,\n","            action_space=action_space,\n","            model_config_dict=model_config_dict,\n","        )\n","        # Replace EncoderConfig by ActorCriticEncoderConfig\n","        self.actor_critic_encoder_config = ActorCriticEncoderConfig(\n","            base_encoder_config=self._encoder_config,\n","            shared=self._model_config_dict[\"vf_share_layers\"],\n","        )\n","\n","        self.pi_and_vf_head_hiddens = self._model_config_dict[\"head_fcnet_hiddens\"]\n","        self.pi_and_vf_head_activation = self._model_config_dict[\n","            \"head_fcnet_activation\"\n","        ]\n","\n","        # We don't have the exact (framework specific) action dist class yet and thus\n","        # cannot determine the exact number of output nodes (action space) required.\n","        # -> Build pi config only in the `self.build_pi_head` method.\n","        self.pi_head_config = None\n","\n","        self.vf_head_config = MLPHeadConfig(\n","            input_dims=self.latent_dims,\n","            hidden_layer_dims=self.pi_and_vf_head_hiddens,\n","            hidden_layer_activation=self.pi_and_vf_head_activation,\n","            output_layer_activation=\"linear\",\n","            output_layer_dim=1,\n","        )\n","\n","    @OverrideToImplementCustomLogic\n","    def build_actor_critic_encoder(self, framework: str) -> ActorCriticEncoder:\n","        \"\"\"Builds the ActorCriticEncoder.\n","\n","        The default behavior is to build the encoder from the encoder_config.\n","        This can be overridden to build a custom ActorCriticEncoder as a means of\n","        configuring the behavior of a PPORLModule implementation.\n","\n","        Args:\n","            framework: The framework to use. Either \"torch\" or \"tf2\".\n","\n","        Returns:\n","            The ActorCriticEncoder.\n","        \"\"\"\n","        return self.actor_critic_encoder_config.build(framework=framework)\n","\n","    @override(Catalog)\n","    def build_encoder(self, framework: str) -> Encoder:\n","        \"\"\"Builds the encoder.\n","\n","        Since PPO uses an ActorCriticEncoder, this method should not be implemented.\n","        \"\"\"\n","        raise NotImplementedError(\n","            \"Use PPOCatalog.build_actor_critic_encoder() instead for PPO.\"\n","        )\n","\n","    @OverrideToImplementCustomLogic\n","    def build_pi_head(self, framework: str) -> Model:\n","        \"\"\"Builds the policy head.\n","\n","        The default behavior is to build the head from the pi_head_config.\n","        This can be overridden to build a custom policy head as a means of configuring\n","        the behavior of a PPORLModule implementation.\n","\n","        Args:\n","            framework: The framework to use. Either \"torch\" or \"tf2\".\n","\n","        Returns:\n","            The policy head.\n","        \"\"\"\n","        # Get action_distribution_cls to find out about the output dimension for pi_head\n","        action_distribution_cls = self.get_action_dist_cls(framework=framework)\n","        if self._model_config_dict[\"free_log_std\"]:\n","            _check_if_diag_gaussian(\n","                action_distribution_cls=action_distribution_cls, framework=framework\n","            )\n","            is_diag_gaussian = True\n","        else:\n","            is_diag_gaussian = _check_if_diag_gaussian(\n","                action_distribution_cls=action_distribution_cls,\n","                framework=framework,\n","                no_error=True,\n","            )\n","        required_output_dim = action_distribution_cls.required_input_dim(\n","            space=self.action_space, model_config=self._model_config_dict\n","        )\n","        # Now that we have the action dist class and number of outputs, we can define\n","        # our pi-config and build the pi head.\n","        pi_head_config_class = (\n","            FreeLogStdMLPHeadConfig\n","            if self._model_config_dict[\"free_log_std\"]\n","            else MLPHeadConfig\n","        )\n","        self.pi_head_config = pi_head_config_class(\n","            input_dims=self.latent_dims,\n","            hidden_layer_dims=self.pi_and_vf_head_hiddens,\n","            hidden_layer_activation=self.pi_and_vf_head_activation,\n","            output_layer_dim=required_output_dim,\n","            output_layer_activation=\"linear\",\n","            clip_log_std=is_diag_gaussian,\n","            log_std_clip_param=self._model_config_dict.get(\"log_std_clip_param\", 20),\n","        )\n","\n","        return self.pi_head_config.build(framework=framework)\n","\n","    @OverrideToImplementCustomLogic\n","    def build_vf_head(self, framework: str) -> Model:\n","        \"\"\"Builds the value function head.\n","\n","        The default behavior is to build the head from the vf_head_config.\n","        This can be overridden to build a custom value function head as a means of\n","        configuring the behavior of a PPORLModule implementation.\n","\n","        Args:\n","            framework: The framework to use. Either \"torch\" or \"tf2\".\n","\n","        Returns:\n","            The value function head.\n","        \"\"\"\n","        return self.vf_head_config.build(framework=framework)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["@PublicAPI(stability=\"alpha\")\n","class RLModule(Checkpointable, abc.ABC):\n","    \"\"\"Base class for RLlib modules.\n","\n","    Subclasses should call super().__init__(config) in their __init__ method.\n","    Here is the pseudocode for how the forward methods are called:\n","\n","    Example for creating a sampling loop:\n","\n","    .. testcode::\n","\n","        from ray.rllib.algorithms.ppo.torch.ppo_torch_rl_module import (\n","            PPOTorchRLModule\n","        )\n","        from ray.rllib.algorithms.ppo.ppo_catalog import PPOCatalog\n","        import gymnasium as gym\n","        import torch\n","\n","        env = gym.make(\"CartPole-v1\")\n","\n","        # Create a single agent RL module spec.\n","        module_spec = RLModuleSpec(\n","            module_class=PPOTorchRLModule,\n","            observation_space=env.observation_space,\n","            action_space=env.action_space,\n","            model_config=DefaultModelConfig(fcnet_hiddens=[128, 128]),\n","            catalog_class=PPOCatalog,\n","        )\n","        module = module_spec.build()\n","        action_dist_class = module.get_inference_action_dist_cls()\n","        obs, info = env.reset()\n","        terminated = False\n","\n","        while not terminated:\n","            fwd_ins = {\"obs\": torch.Tensor([obs])}\n","            fwd_outputs = module.forward_exploration(fwd_ins)\n","            # this can be either deterministic or stochastic distribution\n","            action_dist = action_dist_class.from_logits(\n","                fwd_outputs[\"action_dist_inputs\"]\n","            )\n","            action = action_dist.sample()[0].numpy()\n","            obs, reward, terminated, truncated, info = env.step(action)\n","\n","\n","    Example for training:\n","\n","    .. testcode::\n","\n","        from ray.rllib.algorithms.ppo.torch.ppo_torch_rl_module import (\n","            PPOTorchRLModule\n","        )\n","        from ray.rllib.algorithms.ppo.ppo_catalog import PPOCatalog\n","        import gymnasium as gym\n","        import torch\n","\n","        env = gym.make(\"CartPole-v1\")\n","\n","        # Create a single agent RL module spec.\n","        module_spec = RLModuleSpec(\n","            module_class=PPOTorchRLModule,\n","            observation_space=env.observation_space,\n","            action_space=env.action_space,\n","            model_config=DefaultModelConfig(fcnet_hiddens=[128, 128]),\n","            catalog_class=PPOCatalog,\n","        )\n","        module = module_spec.build()\n","\n","        fwd_ins = {\"obs\": torch.Tensor([obs])}\n","        fwd_outputs = module.forward_train(fwd_ins)\n","        # loss = compute_loss(fwd_outputs, fwd_ins)\n","        # update_params(module, loss)\n","\n","    Example for inference:\n","\n","    .. testcode::\n","\n","        from ray.rllib.algorithms.ppo.torch.ppo_torch_rl_module import (\n","            PPOTorchRLModule\n","        )\n","        from ray.rllib.algorithms.ppo.ppo_catalog import PPOCatalog\n","        import gymnasium as gym\n","        import torch\n","\n","        env = gym.make(\"CartPole-v1\")\n","\n","        # Create a single agent RL module spec.\n","        module_spec = RLModuleSpec(\n","            module_class=PPOTorchRLModule,\n","            observation_space=env.observation_space,\n","            action_space=env.action_space,\n","            model_config=DefaultModelConfig(fcnet_hiddens=[128, 128]),\n","            catalog_class=PPOCatalog,\n","        )\n","        module = module_spec.build()\n","\n","        while not terminated:\n","            fwd_ins = {\"obs\": torch.Tensor([obs])}\n","            fwd_outputs = module.forward_inference(fwd_ins)\n","            # this can be either deterministic or stochastic distribution\n","            action_dist = action_dist_class.from_logits(\n","                fwd_outputs[\"action_dist_inputs\"]\n","            )\n","            action = action_dist.sample()[0].numpy()\n","            obs, reward, terminated, truncated, info = env.step(action)\n","\n","\n","    Args:\n","        config: The config for the RLModule.\n","\n","    Abstract Methods:\n","        ``~_forward_train``: Forward pass during training.\n","\n","        ``~_forward_exploration``: Forward pass during training for exploration.\n","\n","        ``~_forward_inference``: Forward pass during inference.\n","\n","\n","    Note:\n","        There is a reason that the specs are not written as abstract properties.\n","        The reason is that torch overrides `__getattr__` and `__setattr__`. This means\n","        that if we define the specs as properties, then any error in the property will\n","        be interpreted as a failure to retrieve the attribute and will invoke\n","        `__getattr__` which will give a confusing error about the attribute not found.\n","        More details here: https://github.com/pytorch/pytorch/issues/49726.\n","    \"\"\"\n","\n","    \"\"\"RLlib模块的基础类。\n","\n","    子类应在它们的__init__方法中调用super().__init__(config)。\n","    以下是如何调用forward方法的伪代码：\n","\n","    创建采样循环的示例：\n","\n","    .. testcode::\n","\n","        from ray.rllib.algorithms.ppo.torch.ppo_torch_rl_module import (\n","            PPOTorchRLModule\n","        )\n","        from ray.rllib.algorithms.ppo.ppo_catalog import PPOCatalog\n","        import gymnasium as gym\n","        import torch\n","\n","        env = gym.make(\"CartPole-v1\")\n","\n","        # 创建单个智能体的RL模块规范。\n","        module_spec = RLModuleSpec(\n","            module_class=PPOTorchRLModule,\n","            observation_space=env.observation_space,\n","            action_space=env.action_space,\n","            model_config=DefaultModelConfig(fcnet_hiddens=[128, 128]),\n","            catalog_class=PPOCatalog,\n","        )\n","        module = module_spec.build()\n","        action_dist_class = module.get_inference_action_dist_cls()\n","        obs, info = env.reset()\n","        terminated = False\n","\n","        while not terminated:\n","            fwd_ins = {\"obs\": torch.Tensor([obs])}\n","            fwd_outputs = module.forward_exploration(fwd_ins)\n","            # 这可以是确定性或随机分布\n","            action_dist = action_dist_class.from_logits(\n","                fwd_outputs[\"action_dist_inputs\"]\n","            )\n","            action = action_dist.sample()[0].numpy()\n","            obs, reward, terminated, truncated, info = env.step(action)\n","\n","\n","    训练的示例：\n","\n","    .. testcode::\n","\n","        from ray.rllib.algorithms.ppo.torch.ppo_torch_rl_module import (\n","            PPOTorchRLModule\n","        )\n","        from ray.rllib.algorithms.ppo.ppo_catalog import PPOCatalog\n","        import gymnasium as gym\n","        import torch\n","\n","        env = gym.make(\"CartPole-v1\")\n","\n","        # 创建单个智能体的RL模块规范。\n","        module_spec = RLModuleSpec(\n","            module_class=PPOTorchRLModule,\n","            observation_space=env.observation_space,\n","            action_space=env.action_space,\n","            model_config=DefaultModelConfig(fcnet_hiddens=[128, 128]),\n","            catalog_class=PPOCatalog,\n","        )\n","        module = module_spec.build()\n","\n","        fwd_ins = {\"obs\": torch.Tensor([obs])}\n","        fwd_outputs = module.forward_train(fwd_ins)\n","        # loss = compute_loss(fwd_outputs, fwd_ins)\n","        # update_params(module, loss)\n","\n","    推理的示例：\n","\n","    .. testcode::\n","\n","        from ray.rllib.algorithms.ppo.torch.ppo_torch_rl_module import (\n","            PPOTorchRLModule\n","        )\n","        from ray.rllib.algorithms.ppo.ppo_catalog import PPOCatalog\n","        import gymnasium as gym\n","        import torch\n","\n","        env = gym.make(\"CartPole-v1\")\n","\n","        # 创建单个智能体的RL模块规范。\n","        module_spec = RLModuleSpec(\n","            module_class=PPOTorchRLModule,\n","            observation_space=env.observation_space,\n","            action_space=env.action_space,\n","            model_config=DefaultModelConfig(fcnet_hiddens=[128, 128]),\n","            catalog_class=PPOCatalog,\n","        )\n","        module = module_spec.build()\n","\n","        while not terminated:\n","            fwd_ins = {\"obs\": torch.Tensor([obs])}\n","            fwd_outputs = module.forward_inference(fwd_ins)\n","            # 这可以是确定性或随机分布\n","            action_dist = action_dist_class.from_logits(\n","                fwd_outputs[\"action_dist_inputs\"]\n","            )\n","            action = action_dist.sample()[0].numpy()\n","            obs, reward, terminated, truncated, info = env.step(action)\n","\n","\n","    参数:\n","        config: RLModule的配置。\n","\n","    抽象方法:\n","        ``~_forward_train``: 训练期间的前向传递。\n","\n","        ``~_forward_exploration``: 训练期间用于探索的前向传递。\n","\n","        ``~_forward_inference``: 推理期间的前向传递。\n","\n","    注意:\n","        规格没有被写成抽象属性的原因是torch重载了`__getattr__`和`__setattr__`。\n","        这意味着如果我们把规格定义为属性，那么属性中的任何错误都会被解释为获取属性的失败，并会调用`__getattr__`，\n","        这将给出一个关于找不到属性的令人困惑的错误。\n","        更多细节请参见：https://github.com/pytorch/pytorch/issues/49726。\n","    \"\"\"\n","\n","    framework: str = None\n","\n","    STATE_FILE_NAME = \"module_state.pkl\"\n","\n","    def __init__(\n","        self,\n","        config=DEPRECATED_VALUE,\n","        *,\n","        observation_space: Optional[gym.Space] = None,\n","        action_space: Optional[gym.Space] = None,\n","        inference_only: Optional[bool] = None,\n","        learner_only: bool = False,\n","        model_config: Optional[Union[dict, DefaultModelConfig]] = None,\n","        catalog_class=None,\n","    ):\n","        # TODO (sven): Deprecate Catalog and replace with utility functions to create\n","        #  primitive components based on obs- and action spaces.\n","        self.catalog = None\n","        self._catalog_ctor_error = None\n","\n","        # Deprecated\n","        self.config = config\n","        if self.config != DEPRECATED_VALUE:\n","            deprecation_warning(\n","                old=\"RLModule(config=[RLModuleConfig])\",\n","                new=\"RLModule(observation_space=.., action_space=.., inference_only=..,\"\n","                \" learner_only=.., model_config=..)\",\n","                help=\"See https://github.com/ray-project/ray/blob/master/rllib/examples/rl_modules/custom_cnn_rl_module.py \"  # noqa\n","                \"for how to write a custom RLModule.\",\n","                error=True,\n","            )\n","        else:\n","            self.observation_space = observation_space\n","            self.action_space = action_space\n","            self.inference_only = inference_only\n","            self.learner_only = learner_only\n","            self.model_config = model_config\n","            try:\n","                self.catalog = catalog_class(\n","                    observation_space=self.observation_space,\n","                    action_space=self.action_space,\n","                    model_config_dict=self.model_config,\n","                )\n","            except Exception as e:\n","                logger.warning(\n","                    \"Could not create a Catalog object for your RLModule! If you are \"\n","                    \"not using the new API stack yet, make sure to switch it off in \"\n","                    \"your config: `config.api_stack(enable_rl_module_and_learner=False\"\n","                    \", enable_env_runner_and_connector_v2=False)`. Some algos already \"\n","                    \"use the new stack by default. Ignore this message, if your \"\n","                    \"RLModule does not use a Catalog to build its sub-components.\"\n","                )\n","                self._catalog_ctor_error = e\n","\n","        # TODO (sven): Deprecate this. We keep it here for now in case users\n","        #  still have custom models (or subclasses of RLlib default models)\n","        #  into which they pass in a `config` argument.\n","        # TODO (sven): 弃用此方法。我们暂时保留它，\n","        # 以防用户仍在使用自定义模型（或RLlib默认模型的子类）并传递`config`参数。\n","        self.config = RLModuleConfig(\n","            observation_space=self.observation_space,\n","            action_space=self.action_space,\n","            inference_only=self.inference_only,\n","            learner_only=self.learner_only,\n","            model_config_dict=self.model_config,\n","            catalog_class=catalog_class,\n","        )\n","\n","        self.action_dist_cls = None\n","        if self.catalog is not None:\n","            self.action_dist_cls = self.catalog.get_action_dist_cls(\n","                framework=self.framework\n","            )\n","\n","        # Make sure, `setup()` is only called once, no matter what.\n","        if hasattr(self, \"_is_setup\") and self._is_setup:\n","            raise RuntimeError(\n","                \"`RLModule.setup()` called twice within your RLModule implementation \"\n","                f\"{self}! Make sure you are using the proper inheritance order \"\n","                \"(TorchRLModule before [Algo]RLModule) or (TfRLModule before \"\n","                \"[Algo]RLModule) and that you are NOT overriding the constructor, but \"\n","                \"only the `setup()` method of your subclass.\"\n","            )\n","        self.setup()\n","        self._is_setup = True\n","\n","    @OverrideToImplementCustomLogic\n","    def setup(self):\n","        \"\"\"Sets up the components of the module.\n","\n","        This is called automatically during the __init__ method of this class,\n","        therefore, the subclass should call super.__init__() in its constructor. This\n","        abstraction can be used to create any components (e.g. NN layers) that your\n","        RLModule needs.\n","        \"\"\"\n","        \"\"\"设置模块的组件。\n","\n","        这个方法会在该类的__init__方法中自动调用，\n","        因此，子类应该在其构造函数中调用super().__init__()。这个抽象方法可以用来创建你\n","        的RLModule所需的任何组件（例如，NN层）。\n","        \"\"\"\n","        return None\n","\n","    @OverrideToImplementCustomLogic\n","    def get_exploration_action_dist_cls(self) -> Type[Distribution]:\n","        \"\"\"Returns the action distribution class for this RLModule used for exploration.\n","\n","        This class is used to create action distributions from outputs of the\n","        forward_exploration method. If the case that no action distribution class is\n","        needed, this method can return None.\n","\n","        Note that RLlib's distribution classes all implement the `Distribution`\n","        interface. This requires two special methods: `Distribution.from_logits()` and\n","        `Distribution.to_deterministic()`. See the documentation of the\n","        :py:class:`~ray.rllib.models.distributions.Distribution` class for more details.\n","        \"\"\"\n","        \"\"\"返回用于探索的此RLModule的动作分布类。\n","\n","        这个类用于从forward_exploration方法的输出创建动作分布。如果不需要动作分布类，\n","        此方法可以返回None。\n","\n","        注意，RLlib的所有分布类都实现了`Distribution`接口。这需要两个特殊方法：\n","        `Distribution.from_logits()`和`Distribution.to_deterministic()`。更多详情请参见\n","        :py:class:`~ray.rllib.models.distributions.Distribution`类的文档。\n","        \"\"\"\n","        raise NotImplementedError\n","\n","    @OverrideToImplementCustomLogic\n","    def get_inference_action_dist_cls(self) -> Type[Distribution]:\n","        \"\"\"Returns the action distribution class for this RLModule used for inference.\n","\n","        This class is used to create action distributions from outputs of the forward\n","        inference method. If the case that no action distribution class is needed,\n","        this method can return None.\n","\n","        Note that RLlib's distribution classes all implement the `Distribution`\n","        interface. This requires two special methods: `Distribution.from_logits()` and\n","        `Distribution.to_deterministic()`. See the documentation of the\n","        :py:class:`~ray.rllib.models.distributions.Distribution` class for more details.\n","        \"\"\"\n","        \"\"\"返回用于推理的此RLModule的动作分布类。\n","\n","        此类用于从forward inference方法的输出创建动作分布。如果不需要动作分布类，\n","        此方法可以返回None。\n","\n","        请注意，RLlib的所有分布类都实现了Distribution接口。这需要两个特殊方法：\n","        Distribution.from_logits()和Distribution.to_deterministic()。有关更多详细信息，\n","        请参阅:py:class:~ray.rllib.models.distributions.Distribution类的文档。\n","        \"\"\"\n","        raise NotImplementedError\n","\n","    @OverrideToImplementCustomLogic\n","    def get_train_action_dist_cls(self) -> Type[Distribution]:\n","        \"\"\"Returns the action distribution class for this RLModule used for training.\n","\n","        This class is used to get the correct action distribution class to be used by\n","        the training components. In case that no action distribution class is needed,\n","        this method can return None.\n","\n","        Note that RLlib's distribution classes all implement the `Distribution`\n","        interface. This requires two special methods: `Distribution.from_logits()` and\n","        `Distribution.to_deterministic()`. See the documentation of the\n","        :py:class:`~ray.rllib.models.distributions.Distribution` class for more details.\n","        \"\"\"\n","        \"\"\"返回用于训练的此RLModule的动作分布类。\n","\n","        此类用于获取训练组件使用的正确动作分布类。如果不需要动作分布类，\n","        此方法可以返回None。\n","\n","        请注意，RLlib的所有分布类都实现了Distribution接口。这需要两个特殊方法：\n","        Distribution.from_logits()和Distribution.to_deterministic()。有关更多详细信息，\n","        请参阅:py:class:~ray.rllib.models.distributions.Distribution类的文档。\n","        \"\"\"\n","        raise NotImplementedError\n","\n","    @OverrideToImplementCustomLogic\n","    def _forward(self, batch: Dict[str, Any], **kwargs) -> Dict[str, Any]:\n","        \"\"\"Generic forward pass method, used in all phases of training and evaluation.\n","\n","        If you need a more nuanced distinction between forward passes in the different\n","        phases of training and evaluation, override the following methods instead:\n","        For distinct action computation logic w/o exploration, override the\n","        `self._forward_inference()` method.\n","        For distinct action computation logic with exploration, override the\n","        `self._forward_exploration()` method.\n","        For distinct forward pass logic before loss computation, override the\n","        `self._forward_train()` method.\n","\n","        Args:\n","            batch: The input batch.\n","            **kwargs: Additional keyword arguments.\n","\n","        Returns:\n","            The output of the forward pass.\n","        \"\"\"\n","        \"\"\"通用的前向传递方法，用于训练和评估的所有阶段。\n","\n","        如果您需要在训练和评估的不同阶段之间进行更细致的前向传递区分，请改写以下方法：\n","        对于无探索的不同动作计算逻辑，请重写self._forward_inference()方法。\n","        对于带有探索的不同动作计算逻辑，请重写self._forward_exploration()方法。\n","        对于损失计算前的不同前向传递逻辑，请重写self._forward_train()方法。\n","\n","        参数:\n","            batch: 输入批次。\n","            **kwargs: 额外的关键字参数。\n","\n","        返回:\n","            前向传递的输出。\n","        \"\"\"\n","\n","        return {}\n","\n","    def forward_inference(self, batch: Dict[str, Any], **kwargs) -> Dict[str, Any]:\n","        \"\"\"DO NOT OVERRIDE! Forward-pass during evaluation, called from the sampler.\n","\n","        This method should not be overridden. Override the `self._forward_inference()`\n","        method instead.\n","\n","        Args:\n","            batch: The input batch. This input batch should comply with\n","                input_specs_inference().\n","            **kwargs: Additional keyword arguments.\n","\n","        Returns:\n","            The output of the forward pass. This output should comply with the\n","            ouptut_specs_inference().\n","        \"\"\"\n","        \"\"\"不要重写！在评估期间的前向传递，由采样器调用。\n","\n","        此方法不应被重写。请改写self._forward_inference()方法。\n","\n","        参数:\n","            batch: 输入批次。该输入批次应符合input_specs_inference()。\n","            **kwargs: 额外的关键字参数。\n","\n","        返回:\n","            前向传递的输出。该输出应符合output_specs_inference()。\n","        \"\"\"\n","        return self._forward_inference(batch, **kwargs)\n","\n","    @OverrideToImplementCustomLogic\n","    def _forward_inference(self, batch: Dict[str, Any], **kwargs) -> Dict[str, Any]:\n","        \"\"\"Forward-pass used for action computation without exploration behavior.\n","\n","        Override this method only, if you need specific behavior for non-exploratory\n","        action computation behavior. If you have only one generic behavior for all\n","        phases of training and evaluation, override `self._forward()` instead.\n","\n","        By default, this calls the generic `self._forward()` method.\n","        \"\"\"\n","        \"\"\"用于无探索行为的动作计算的前向传递。\n","\n","        仅当您需要特定于非探索动作计算行为的特定行为时才重写此方法。如果您对训练和评估的所有阶段只有一个通用行为，请改写self._forward()方法。\n","        默认情况下，此方法调用通用的self._forward()方法。\n","        \"\"\"\n","        return self._forward(batch, **kwargs)\n","\n","    def forward_exploration(self, batch: Dict[str, Any], **kwargs) -> Dict[str, Any]:\n","        \"\"\"DO NOT OVERRIDE! Forward-pass during exploration, called from the sampler.\n","\n","        This method should not be overridden. Override the `self._forward_exploration()`\n","        method instead.\n","\n","        Args:\n","            batch: The input batch. This input batch should comply with\n","                input_specs_exploration().\n","            **kwargs: Additional keyword arguments.\n","\n","        Returns:\n","            The output of the forward pass. This output should comply with the\n","            output_specs_exploration().\n","        \"\"\"\n","        return self._forward_exploration(batch, **kwargs)\n","\n","    @OverrideToImplementCustomLogic\n","    def _forward_exploration(self, batch: Dict[str, Any], **kwargs) -> Dict[str, Any]:\n","        \"\"\"Forward-pass used for action computation with exploration behavior.\n","\n","        Override this method only, if you need specific behavior for exploratory\n","        action computation behavior. If you have only one generic behavior for all\n","        phases of training and evaluation, override `self._forward()` instead.\n","\n","        By default, this calls the generic `self._forward()` method.\n","        \"\"\"\n","        return self._forward(batch, **kwargs)\n","\n","    def forward_train(self, batch: Dict[str, Any], **kwargs) -> Dict[str, Any]:\n","        \"\"\"DO NOT OVERRIDE! Forward-pass during training called from the learner.\n","\n","        This method should not be overridden. Override the `self._forward_train()`\n","        method instead.\n","\n","        Args:\n","            batch: The input batch. This input batch should comply with\n","                input_specs_train().\n","            **kwargs: Additional keyword arguments.\n","\n","        Returns:\n","            The output of the forward pass. This output should comply with the\n","            output_specs_train().\n","        \"\"\"\n","        if self.inference_only:\n","            raise RuntimeError(\n","                \"Calling `forward_train` on an inference_only module is not allowed! \"\n","                \"Set the `inference_only=False` flag in the RLModule's config when \"\n","                \"building the module.\"\n","            )\n","        return self._forward_train(batch, **kwargs)\n","\n","    @OverrideToImplementCustomLogic\n","    def _forward_train(self, batch: Dict[str, Any], **kwargs) -> Dict[str, Any]:\n","        \"\"\"Forward-pass used before the loss computation (training).\n","\n","        Override this method only, if you need specific behavior and outputs for your\n","        loss computations. If you have only one generic behavior for all\n","        phases of training and evaluation, override `self._forward()` instead.\n","\n","        By default, this calls the generic `self._forward()` method.\n","        \"\"\"\n","        return self._forward(batch, **kwargs)\n","\n","    @OverrideToImplementCustomLogic\n","    def get_initial_state(self) -> Any:\n","        \"\"\"Returns the initial state of the RLModule, in case this is a stateful module.\n","\n","        Returns:\n","            A tensor or any nested struct of tensors, representing an initial state for\n","            this (stateful) RLModule.\n","        \"\"\"\n","        \n","        \"\"\"返回RLModule的初始状态，如果这是一个有状态的模块。\n","\n","        返回:\n","            一个张量或任何嵌套的张量结构，表示此（有状态的）RLModule的初始状态。\n","        \"\"\"\n","        return {}\n","\n","    @OverrideToImplementCustomLogic\n","    def is_stateful(self) -> bool:\n","        \"\"\"By default, returns False if the initial state is an empty dict (or None).\n","\n","        By default, RLlib assumes that the module is non-recurrent, if the initial\n","        state is an empty dict and recurrent otherwise.\n","        This behavior can be customized by overriding this method.\n","        \"\"\"\n","        \"\"\"默认情况下，如果初始状态是一个空字典（或None），则返回False。\n","        \n","        默认情况下，如果初始状态是一个空字典，RLlib假定此模块是非循环的，它们没有像循环神经网络（RNN）、长短期记忆网络（LSTM）或门控循环单元（GRU）那样的反馈回路或记忆单元。\n","        否则为循环的。\n","        这种行为可以通过重写此方法来定制。\n","        \"\"\"\n","        initial_state = self.get_initial_state()\n","        assert isinstance(initial_state, dict), (\n","            \"The initial state of an RLModule must be a dict, but is \"\n","            f\"{type(initial_state)} instead.\"\n","        )\n","        return bool(initial_state)\n","\n","    @OverrideToImplementCustomLogic\n","    @override(Checkpointable)\n","    def get_state(\n","        self,\n","        components: Optional[Union[str, Collection[str]]] = None,\n","        *,\n","        not_components: Optional[Union[str, Collection[str]]] = None,\n","        inference_only: bool = False,\n","        **kwargs,\n","    ) -> StateDict:\n","        \"\"\"Returns the state dict of the module.\n","\n","        Args:\n","            inference_only: Whether the returned state should be an inference-only\n","                state (w/o those model components that are not needed for action\n","                computations, such as a value function or a target network).\n","                Note that setting this to `False` might raise an error if\n","                `self.inference_only` is True.\n","\n","        Returns:\n","            This RLModule's state dict.\n","        \"\"\"\n","        \"\"\"返回模块的状态字典。\n","\n","        参数:\n","            inference_only: 是否返回仅用于推理状态的状态字典（不包括那些对动作计算不必要的模型组件，如值函数或目标网络）。\n","                请注意，如果self.inference_only为True，将其设置为False可能会引发错误。\n","\n","        返回:\n","            此RLModule的状态字典。\n","        \"\"\"\n","        if components is not None or not_components is not None:\n","            raise ValueError(\n","                \"`component` arg and `not_component` arg not supported in \"\n","                \"`RLModule.get_state()` base implementation! Override this method in \"\n","                \"your custom RLModule subclass.\"\n","            )\n","        return {}\n","\n","    @OverrideToImplementCustomLogic\n","    @override(Checkpointable)\n","    def set_state(self, state: StateDict) -> None:\n","        pass\n","\n","    @override(Checkpointable)\n","    def get_ctor_args_and_kwargs(self):\n","        return (\n","            (),  # *args\n","            {\n","                \"observation_space\": self.observation_space,\n","                \"action_space\": self.action_space,\n","                \"inference_only\": self.inference_only,\n","                \"learner_only\": self.learner_only,\n","                \"model_config\": self.model_config,\n","                \"catalog_class\": (\n","                    type(self.catalog) if self.catalog is not None else None\n","                ),\n","            },  # **kwargs\n","        )\n","\n","    def as_multi_rl_module(self) -> \"MultiRLModule\":\n","        \"\"\"Returns a multi-agent wrapper around this module.\"\"\"\n","        from ray.rllib.core.rl_module.multi_rl_module import MultiRLModule\n","\n","        multi_rl_module = MultiRLModule(\n","            rl_module_specs={DEFAULT_MODULE_ID: RLModuleSpec.from_module(self)}\n","        )\n","        return multi_rl_module\n","\n","    def unwrapped(self) -> \"RLModule\":\n","        \"\"\"Returns the underlying module if this module is a wrapper.\n","\n","        An example of a wrapped is the TorchDDPRLModule class, which wraps\n","        a TorchRLModule.\n","\n","        Returns:\n","            The underlying module.\n","        \"\"\"\n","        return self\n","\n","    @Deprecated(new=\"RLModule.as_multi_rl_module()\", error=True)\n","    def as_multi_agent(self, *args, **kwargs):\n","        pass\n","\n","    @Deprecated(new=\"RLModule.save_to_path(...)\", error=True)\n","    def save_state(self, *args, **kwargs):\n","        pass\n","\n","    @Deprecated(new=\"RLModule.restore_from_path(...)\", error=True)\n","    def load_state(self, *args, **kwargs):\n","        pass\n","\n","    @Deprecated(new=\"RLModule.save_to_path(...)\", error=True)\n","    def save_to_checkpoint(self, *args, **kwargs):\n","        pass\n","\n","    def output_specs_inference(self) -> SpecType:\n","        return [Columns.ACTION_DIST_INPUTS]\n","\n","    def output_specs_exploration(self) -> SpecType:\n","        return [Columns.ACTION_DIST_INPUTS]\n","\n","    def output_specs_train(self) -> SpecType:\n","        \"\"\"Returns the output specs of the forward_train method.\"\"\"\n","        return {}\n","\n","    def input_specs_inference(self) -> SpecType:\n","        \"\"\"Returns the input specs of the forward_inference method.\"\"\"\n","        return self._default_input_specs()\n","\n","    def input_specs_exploration(self) -> SpecType:\n","        \"\"\"Returns the input specs of the forward_exploration method.\"\"\"\n","        return self._default_input_specs()\n","\n","    def input_specs_train(self) -> SpecType:\n","        \"\"\"Returns the input specs of the forward_train method.\"\"\"\n","        return self._default_input_specs()\n","\n","    def _default_input_specs(self) -> SpecType:\n","        \"\"\"Returns the default input specs.\"\"\"\n","        return [Columns.OBS]\n","\n","class TorchRLModule(nn.Module, RLModule):\n","    \"\"\"A base class for RLlib PyTorch RLModules.\n","\n","    Note that the `_forward` methods of this class can be 'torch.compiled' individually:\n","        - `TorchRLModule._forward_train()`\n","        - `TorchRLModule._forward_inference()`\n","        - `TorchRLModule._forward_exploration()`\n","\n","    As a rule of thumb, they should only contain torch-native tensor manipulations,\n","    or otherwise they may yield wrong outputs. In particular, the creation of RLlib\n","    distributions inside these methods should be avoided when using `torch.compile`.\n","    When in doubt, you can use `torch.dynamo.explain()` to check whether a compiled\n","    method has broken up into multiple sub-graphs.\n","\n","    Compiling these methods can bring speedups under certain conditions.\n","    \"\"\"\n","    \"\"\"\n","    RLlib PyTorch RL模块的基类。\n","\n","    注意这个类的 `_forward` 方法可以单独进行 'torch.compile' 编译:\n","        - `TorchRLModule._forward_train()`\n","        - `TorchRLModule._forward_inference()`\n","        - `TorchRLModule._forward_exploration()`\n","\n","    作为经验法则，这些方法应该只包含torch原生的张量操作，\n","    否则可能会产生错误的输出。特别是在使用 `torch.compile` 时，\n","    应当避免在这些方法内创建 RLlib 分布。当有疑问时，可以使用\n","    `torch.dynamo.explain()` 来检查一个编译过的方法是否分解成了多个子图。\n","\n","    在某些条件下，编译这些方法可以带来速度提升。\n","    \"\"\"\n","\n","    framework: str = \"torch\"\n","\n","    # Stick with torch default.\n","    STATE_FILE_NAME = \"module_state.pt\"\n","\n","    def __init__(self, *args, **kwargs) -> None:\n","        nn.Module.__init__(self)\n","        RLModule.__init__(self, *args, **kwargs)\n","\n","        # If an inference-only class AND self.inference_only is True,\n","        # remove all attributes that are returned by\n","        # `self.get_non_inference_attributes()`.\n","        if self.inference_only and isinstance(self, InferenceOnlyAPI):\n","            for attr in self.get_non_inference_attributes():\n","                parts = attr.split(\".\")\n","                if not hasattr(self, parts[0]):\n","                    continue\n","                target = getattr(self, parts[0])\n","                # Traverse from the next part on (if nested).\n","                for part in parts[1:]:\n","                    if not hasattr(target, part):\n","                        target = None\n","                        break\n","                    target = getattr(target, part)\n","                # Delete, if target is valid.\n","                if target is not None:\n","                    del target\n","\n","    def compile(self, compile_config: TorchCompileConfig):\n","        \"\"\"Compile the forward methods of this module.\n","\n","        This is a convenience method that calls `compile_wrapper` with the given\n","        compile_config.\n","\n","        Args:\n","            compile_config: The compile config to use.\n","        \"\"\"\n","        return compile_wrapper(self, compile_config)\n","\n","    @OverrideToImplementCustomLogic\n","    def _forward_inference(self, batch: Dict[str, Any], **kwargs) -> Dict[str, Any]:\n","        # By default, calls the generic `_forward()` method, but with a no-grad context\n","        # for performance reasons.\n","        with torch.no_grad():\n","            return self._forward(batch, **kwargs)\n","\n","    @OverrideToImplementCustomLogic\n","    def _forward_exploration(self, batch: Dict[str, Any], **kwargs) -> Dict[str, Any]:\n","        # By default, calls the generic `_forward()` method, but with a no-grad context\n","        # for performance reasons.\n","        with torch.no_grad():\n","            return self._forward(batch, **kwargs)\n","\n","    @OverrideToImplementCustomLogic\n","    @override(RLModule)\n","    def get_state(\n","        self,\n","        components: Optional[Union[str, Collection[str]]] = None,\n","        *,\n","        not_components: Optional[Union[str, Collection[str]]] = None,\n","        inference_only: bool = False,\n","        **kwargs,\n","    ) -> StateDict:\n","        state_dict = self.state_dict()\n","        # Filter out `inference_only` keys from the state dict if `inference_only` and\n","        # this RLModule is NOT `inference_only` (but does implement the\n","        # InferenceOnlyAPI).\n","        if (\n","            inference_only\n","            and not self.inference_only\n","            and isinstance(self, InferenceOnlyAPI)\n","        ):\n","            attr = self.get_non_inference_attributes()\n","            for key in list(state_dict.keys()):\n","                if any(\n","                    key.startswith(a) and (len(key) == len(a) or key[len(a)] == \".\")\n","                    for a in attr\n","                ):\n","                    del state_dict[key]\n","        return convert_to_numpy(state_dict)\n","\n","    @OverrideToImplementCustomLogic\n","    @override(RLModule)\n","    def set_state(self, state: StateDict) -> None:\n","        # If state contains more keys than `self.state_dict()`, then we simply ignore\n","        # these keys (strict=False). This is most likely due to `state` coming from\n","        # an `inference_only=False` RLModule, while `self` is an `inference_only=True`\n","        # RLModule.\n","        self.load_state_dict(convert_to_torch_tensor(state), strict=False)\n","\n","    @OverrideToImplementCustomLogic\n","    @override(RLModule)\n","    def get_inference_action_dist_cls(self) -> Type[TorchDistribution]:\n","        if self.action_dist_cls is not None:\n","            return self.action_dist_cls\n","        elif isinstance(self.action_space, gym.spaces.Discrete):\n","            return TorchCategorical\n","        elif isinstance(self.action_space, gym.spaces.Box):\n","            return TorchDiagGaussian\n","        else:\n","            raise ValueError(\n","                f\"Default action distribution for action space \"\n","                f\"{self.action_space} not supported! Either set the \"\n","                f\"`self.action_dist_cls` property in your RLModule's `setup()` method \"\n","                f\"to a subclass of `ray.rllib.models.torch.torch_distributions.\"\n","                f\"TorchDistribution` or - if you need different distributions for \"\n","                f\"inference and training - override the three methods: \"\n","                f\"`get_inference_action_dist_cls`, `get_exploration_action_dist_cls`, \"\n","                f\"and `get_train_action_dist_cls` in your RLModule.\"\n","            )\n","\n","    @OverrideToImplementCustomLogic\n","    @override(RLModule)\n","    def get_exploration_action_dist_cls(self) -> Type[TorchDistribution]:\n","        return self.get_inference_action_dist_cls()\n","\n","    @OverrideToImplementCustomLogic\n","    @override(RLModule)\n","    def get_train_action_dist_cls(self) -> Type[TorchDistribution]:\n","        return self.get_inference_action_dist_cls()\n","\n","    @override(nn.Module)\n","    def forward(self, batch: Dict[str, Any], **kwargs) -> Dict[str, Any]:\n","        \"\"\"DO NOT OVERRIDE!\n","\n","        This is aliased to `self.forward_train` because Torch DDP requires a forward\n","        method to be implemented for backpropagation to work.\n","\n","        Instead, override:\n","        `_forward()` to define a generic forward pass for all phases (exploration,\n","        inference, training)\n","        `_forward_inference()` to define the forward pass for action inference in\n","        deployment/production (no exploration).\n","        `_forward_exploration()` to define the forward pass for action inference during\n","        training sample collection (w/ exploration behavior).\n","        `_forward_train()` to define the forward pass prior to loss computation.\n","        \"\"\"\n","        return self.forward_train(batch, **kwargs)\n","\n","\"\"\"\n","PPORLModule 需要使用一个 catalog 来构建模型\n","\n","# Build models from catalog.\n","self.encoder = self.catalog.build_actor_critic_encoder(framework=self.framework)\n","self.pi = self.catalog.build_pi_head(framework=self.framework)\n","self.vf = self.catalog.build_vf_head(framework=self.framework)\n","\"\"\"\n","@DeveloperAPI(stability=\"alpha\")\n","class PPORLModule(RLModule, InferenceOnlyAPI, ValueFunctionAPI, abc.ABC):\n","    @override(RLModule)\n","    def setup(self):\n","        if self.catalog is None and hasattr(self, \"_catalog_ctor_error\"):\n","            raise self._catalog_ctor_error\n","\n","        # __sphinx_doc_begin__\n","        # If we have a stateful model, states for the critic need to be collected\n","        # during sampling and `inference-only` needs to be `False`. Note, at this\n","        # point the encoder is not built, yet and therefore `is_stateful()` does\n","        # not work.\n","        # 如果我们有一个有状态的模型，评判器的状态需要在采样期间被收集，\n","        # 并且 `inference-only` 需要设置为 `False`。注意，此刻编码器还未构建，\n","        # 因此 `is_stateful()` 无法工作。\n","        is_stateful = isinstance(\n","            self.catalog.actor_critic_encoder_config.base_encoder_config,\n","            RecurrentEncoderConfig,\n","        )\n","        if is_stateful:\n","            self.inference_only = False\n","        # If this is an `inference_only` Module, we'll have to pass this information\n","        # to the encoder config as well.\n","        if self.inference_only and self.framework == \"torch\":\n","            self.catalog.actor_critic_encoder_config.inference_only = True\n","\n","        # Build models from catalog.\n","        self.encoder = self.catalog.build_actor_critic_encoder(framework=self.framework)\n","        self.pi = self.catalog.build_pi_head(framework=self.framework)\n","        self.vf = self.catalog.build_vf_head(framework=self.framework)\n","        # __sphinx_doc_end__\n","\n","    @override(RLModule)\n","    def get_initial_state(self) -> dict:\n","        if hasattr(self.encoder, \"get_initial_state\"):\n","            return self.encoder.get_initial_state()\n","        else:\n","            return {}\n","\n","    @OverrideToImplementCustomLogic_CallToSuperRecommended\n","    @override(InferenceOnlyAPI)\n","    def get_non_inference_attributes(self) -> List[str]:\n","        \"\"\"Return attributes, which are NOT inference-only (only used for training).\"\"\"\n","        return [\"vf\"] + (\n","            []\n","            if self.model_config.get(\"vf_share_layers\")\n","            else [\"encoder.critic_encoder\"]\n","        )\n","\n","class Columns:\n","    \"\"\"Definitions of common column names for RL data, e.g. 'obs', 'rewards', etc..\n","\n","    Note that this replaces the `SampleBatch` and `Postprocessing` columns (of the same\n","    name).\n","    \"\"\"\n","\n","    # Observation received from an environment after `reset()` or `step()`.\n","    OBS = \"obs\"\n","    # Infos received from an environment after `reset()` or `step()`.\n","    INFOS = \"infos\"\n","\n","    # Action computed/sampled by an RLModule.\n","    ACTIONS = \"actions\"\n","    # Action actually sent to the (gymnasium) `Env.step()` method.\n","    ACTIONS_FOR_ENV = \"actions_for_env\"\n","    # Reward returned by `env.step()`.\n","    REWARDS = \"rewards\"\n","    # Termination signal received from an environment after `step()`.\n","    TERMINATEDS = \"terminateds\"\n","    # Truncation signal received from an environment after `step()` (e.g. because\n","    # of a reached time limit).\n","    TRUNCATEDS = \"truncateds\"\n","\n","    # Next observation: Only used by algorithms that need to look at TD-data for\n","    # training, such as off-policy/DQN algos.\n","    NEXT_OBS = \"new_obs\"\n","\n","    # Uniquely identifies an episode\n","    EPS_ID = \"eps_id\"\n","    AGENT_ID = \"agent_id\"\n","    MODULE_ID = \"module_id\"\n","\n","    # The size of non-zero-padded data within a (e.g. LSTM) zero-padded\n","    # (B, T, ...)-style train batch.\n","    SEQ_LENS = \"seq_lens\"\n","    # Episode timestep counter.\n","    T = \"t\"\n","\n","    # Common extra RLModule output keys.\n","    STATE_IN = \"state_in\"\n","    STATE_OUT = \"state_out\"\n","    EMBEDDINGS = \"embeddings\"\n","    ACTION_DIST_INPUTS = \"action_dist_inputs\"\n","    ACTION_PROB = \"action_prob\"\n","    ACTION_LOGP = \"action_logp\"\n","\n","    # Value function predictions.\n","    VF_PREDS = \"vf_preds\"\n","    # Values, predicted at one timestep beyond the last timestep taken.\n","    # These are usually calculated via the value function network using the final\n","    # observation (and in case of an RNN: the last returned internal state).\n","    VALUES_BOOTSTRAPPED = \"values_bootstrapped\"\n","\n","    # Postprocessing columns.\n","    ADVANTAGES = \"advantages\"\n","    VALUE_TARGETS = \"value_targets\"\n","\n","    # Intrinsic rewards (learning with curiosity).\n","    INTRINSIC_REWARDS = \"intrinsic_rewards\"\n","\n","    # Loss mask. If provided in a train batch, a Learner's compute_loss_for_module\n","    # method should respect the False-set value in here and mask out the respective\n","    # items form the loss.\n","    LOSS_MASK = \"loss_mask\"\n","\n","class PPOTorchRLModule(TorchRLModule, PPORLModule):\n","    @override(RLModule)\n","    def _forward(self, batch: Dict[str, Any], **kwargs) -> Dict[str, Any]:\n","        \"\"\"Default forward pass (used for inference and exploration).\"\"\"\n","        output = {}\n","        # Encoder forward pass.\n","        encoder_outs = self.encoder(batch)\n","        # Stateful encoder?\n","        if Columns.STATE_OUT in encoder_outs:\n","            output[Columns.STATE_OUT] = encoder_outs[Columns.STATE_OUT]\n","        # Pi head.\n","        output[Columns.ACTION_DIST_INPUTS] = self.pi(encoder_outs[ENCODER_OUT][ACTOR])\n","        return output\n","\n","    @override(RLModule)\n","    def _forward_train(self, batch: Dict[str, Any], **kwargs) -> Dict[str, Any]:\n","        \"\"\"Train forward pass (keep embeddings for possible shared value func. call).\"\"\"\n","        output = {}\n","        encoder_outs = self.encoder(batch)\n","        output[Columns.EMBEDDINGS] = encoder_outs[ENCODER_OUT][CRITIC]\n","        if Columns.STATE_OUT in encoder_outs:\n","            output[Columns.STATE_OUT] = encoder_outs[Columns.STATE_OUT]\n","        output[Columns.ACTION_DIST_INPUTS] = self.pi(encoder_outs[ENCODER_OUT][ACTOR])\n","        return output\n","\n","    @override(ValueFunctionAPI)\n","    def compute_values(\n","        self,\n","        batch: Dict[str, Any],\n","        embeddings: Optional[Any] = None,\n","    ) -> TensorType:\n","        if embeddings is None:\n","            # Separate vf-encoder.\n","            if hasattr(self.encoder, \"critic_encoder\"):\n","                batch_ = batch\n","                if self.is_stateful():\n","                    # The recurrent encoders expect a `(state_in, h)`  key in the\n","                    # input dict while the key returned is `(state_in, critic, h)`.\n","                    batch_ = batch.copy()\n","                    batch_[Columns.STATE_IN] = batch[Columns.STATE_IN][CRITIC]\n","                embeddings = self.encoder.critic_encoder(batch_)[ENCODER_OUT]\n","            # Shared encoder.\n","            else:\n","                embeddings = self.encoder(batch)[ENCODER_OUT][CRITIC]\n","\n","        # Value head.\n","        vf_out = self.vf(embeddings)\n","        # Squeeze out last dimension (single node value head).\n","        return vf_out.squeeze(-1)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["ppo_config = {\n","    \"extra_python_environs_for_driver\": {},\n","    \"extra_python_environs_for_worker\": {},\n","    \"placement_strategy\": \"PACK\",\n","    \"num_gpus\": 0,\n","    \"_fake_gpus\": False,\n","    \"num_cpus_for_main_process\": 1,\n","    \"framework_str\": \"torch\",\n","    \"eager_tracing\": True,\n","    \"eager_max_retraces\": 20,\n","    \"tf_session_args\": {\n","        \"intra_op_parallelism_threads\": 2,\n","        \"inter_op_parallelism_threads\": 2,\n","        \"gpu_options\": {\"allow_growth\": True},\n","        \"log_device_placement\": False,\n","        \"device_count\": {\"CPU\": 1},\n","        \"allow_soft_placement\": True\n","    },\n","    \"local_tf_session_args\": {\n","        \"intra_op_parallelism_threads\": 8,\n","        \"inter_op_parallelism_threads\": 8\n","    },\n","    \"torch_compile_learner\": False,\n","    \"torch_compile_learner_what_to_compile\": \"TorchCompileWhatToCompile.FORWARD_TRAIN\",\n","    \"torch_compile_learner_dynamo_backend\": \"inductor\",\n","    \"torch_compile_learner_dynamo_mode\": None,\n","    \"torch_compile_worker\": False,\n","    \"torch_compile_worker_dynamo_backend\": \"onnxrt\",\n","    \"torch_compile_worker_dynamo_mode\": None,\n","    \"torch_ddp_kwargs\": {},\n","    \"torch_skip_nan_gradients\": False,\n","    \"env\": \"CartPole-v1\",\n","    \"env_config\": {},\n","    \"observation_space\": None,\n","    \"action_space\": None,\n","    \"clip_rewards\": None,\n","    \"normalize_actions\": True,\n","    \"clip_actions\": False,\n","    \"_is_atari\": None,\n","    \"disable_env_checking\": False,\n","    \"env_task_fn\": None,\n","    \"render_env\": False,\n","    \"action_mask_key\": \"action_mask\",\n","    \"env_runner_cls\": None,\n","    \"num_env_runners\": 0,\n","    \"num_envs_per_env_runner\": 1,\n","    \"num_cpus_per_env_runner\": 1,\n","    \"num_gpus_per_env_runner\": 0,\n","    \"custom_resources_per_env_runner\": {},\n","    \"validate_env_runners_after_construction\": True,\n","    \"max_requests_in_flight_per_env_runner\": 1,\n","    \"sample_timeout_s\": 60.0,\n","    \"create_env_on_local_worker\": False,\n","    \"_env_to_module_connector\": None,\n","    \"add_default_connectors_to_env_to_module_pipeline\": True,\n","    \"_module_to_env_connector\": None,\n","    \"add_default_connectors_to_module_to_env_pipeline\": True,\n","    \"episode_lookback_horizon\": 1,\n","    \"rollout_fragment_length\": \"auto\",\n","    \"batch_mode\": \"truncate_episodes\",\n","    \"compress_observations\": False,\n","    \"remote_worker_envs\": False,\n","    \"remote_env_batch_wait_ms\": 0,\n","    \"enable_tf1_exec_eagerly\": False,\n","    \"sample_collector\": \"<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>\",\n","    \"preprocessor_pref\": \"deepmind\",\n","    \"observation_filter\": \"NoFilter\",\n","    \"update_worker_filter_stats\": True,\n","    \"use_worker_filter_stats\": True,\n","    \"sampler_perf_stats_ema_coef\": None,\n","    \"num_learners\": 0,\n","    \"num_gpus_per_learner\": 0,\n","    \"num_cpus_per_learner\": 1,\n","    \"local_gpu_idx\": 0,\n","    \"max_requests_in_flight_per_learner\": 3,\n","    \"gamma\": 0.99,\n","    \"lr\": 5e-05,\n","    \"grad_clip\": None,\n","    \"grad_clip_by\": \"global_norm\",\n","    \"train_batch_size_per_learner\": None,\n","    \"train_batch_size\": 4000,\n","    \"num_epochs\": 30,\n","    \"minibatch_size\": 128,\n","    \"shuffle_batch_per_epoch\": True,\n","    \"model\": {\n","        \"fcnet_hiddens\": [256, 256],\n","        \"fcnet_activation\": \"tanh\",\n","        \"fcnet_weights_initializer\": None,\n","        \"fcnet_weights_initializer_config\": None,\n","        \"fcnet_bias_initializer\": None,\n","        \"fcnet_bias_initializer_config\": None,\n","        \"conv_filters\": None,\n","        \"conv_activation\": \"relu\",\n","        \"conv_kernel_initializer\": None,\n","        \"conv_kernel_initializer_config\": None,\n","        \"conv_bias_initializer\": None,\n","        \"conv_bias_initializer_config\": None,\n","        \"conv_transpose_kernel_initializer\": None,\n","        \"conv_transpose_kernel_initializer_config\": None,\n","        \"conv_transpose_bias_initializer\": None,\n","        \"conv_transpose_bias_initializer_config\": None,\n","        \"post_fcnet_hiddens\": [],\n","        \"post_fcnet_activation\": \"relu\",\n","        \"post_fcnet_weights_initializer\": None,\n","        \"post_fcnet_weights_initializer_config\": None,\n","        \"post_fcnet_bias_initializer\": None,\n","        \"post_fcnet_bias_initializer_config\": None,\n","        \"free_log_std\": False,\n","        \"log_std_clip_param\": 20.0,\n","        \"no_final_linear\": False,\n","        \"vf_share_layers\": False,\n","        \"use_lstm\": False,\n","        \"max_seq_len\": 20,\n","        \"lstm_cell_size\": 256,\n","        \"lstm_use_prev_action\": False,\n","        \"lstm_use_prev_reward\": False,\n","        \"lstm_weights_initializer\": None,\n","        \"lstm_weights_initializer_config\": None,\n","        \"lstm_bias_initializer\": None,\n","        \"lstm_bias_initializer_config\": None,\n","        \"_time_major\": False,\n","        \"use_attention\": False,\n","        \"attention_num_transformer_units\": 1,\n","        \"attention_dim\": 64,\n","        \"attention_num_heads\": 1,\n","        \"attention_head_dim\": 32,\n","        \"attention_memory_inference\": 50,\n","        \"attention_memory_training\": 50,\n","        \"attention_position_wise_mlp_dim\": 32,\n","        \"attention_init_gru_gate_bias\": 2.0,\n","        \"attention_use_n_prev_actions\": 0,\n","        \"attention_use_n_prev_rewards\": 0,\n","        \"framestack\": True,\n","        \"dim\": 84,\n","        \"grayscale\": False,\n","        \"zero_mean\": True,\n","        \"custom_model\": None,\n","        \"custom_model_config\": {},\n","        \"custom_action_dist\": None,\n","        \"custom_preprocessor\": None,\n","        \"encoder_latent_dim\": None,\n","        \"always_check_shapes\": False,\n","        \"lstm_use_prev_action_reward\": -1,\n","        \"_use_default_native_models\": -1,\n","        \"_disable_preprocessor_api\": False,\n","        \"_disable_action_flattening\": False\n","    },\n","    \"_learner_connector\": None,\n","    \"add_default_connectors_to_learner_pipeline\": True,\n","    \"learner_config_dict\": {},\n","    \"optimizer\": {},\n","    \"_learner_class\": None,\n","    \"callbacks_class\": \"<class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>\",\n","    \"explore\": True,\n","    \"enable_rl_module_and_learner\": True,\n","    \"enable_env_runner_and_connector_v2\": True,\n","    \"_prior_exploration_config\": {\"type\": \"StochasticSampling\"},\n","    \"count_steps_by\": \"env_steps\",\n","    \"policies\": {\"default_policy\": [None, None, None, None]},\n","    \"policy_map_capacity\": 100,\n","    \"policy_mapping_fn\": \"<function AlgorithmConfig.DEFAULT_POLICY_MAPPING_FN at 0x0000000036439DA0>\",\n","    \"policies_to_train\": None,\n","    \"policy_states_are_swappable\": False,\n","    \"observation_fn\": None,\n","    \"input_\": \"sampler\",\n","    \"input_read_method\": \"read_parquet\",\n","    \"input_read_method_kwargs\": {},\n","    \"input_read_schema\": {},\n","    \"input_read_episodes\": False,\n","    \"input_read_sample_batches\": False,\n","    \"input_read_batch_size\": None,\n","    \"input_filesystem\": None,\n","    \"input_filesystem_kwargs\": {},\n","    \"input_compress_columns\": [\"obs\", \"new_obs\"],\n","    \"input_spaces_jsonable\": True,\n","    \"materialize_data\": False,\n","    \"materialize_mapped_data\": True,\n","    \"map_batches_kwargs\": {},\n","    \"iter_batches_kwargs\": {},\n","    \"prelearner_class\": None,\n","    \"prelearner_buffer_class\": None,\n","    \"prelearner_buffer_kwargs\": {},\n","    \"prelearner_module_synch_period\": 10,\n","    \"dataset_num_iters_per_learner\": None,\n","    \"input_config\": {},\n","    \"actions_in_input_normalized\": False,\n","    \"postprocess_inputs\": False,\n","    \"shuffle_buffer_size\": 0,\n","    \"output\": None,\n","    \"output_config\": {},\n","    \"output_compress_columns\": [\"obs\", \"new_obs\"],\n","    \"output_max_file_size\": 67108864,\n","    \"output_max_rows_per_file\": None,\n","    \"output_write_method\": \"write_parquet\",\n","    \"output_write_method_kwargs\": {},\n","    \"output_filesystem\": None,\n","    \"output_filesystem_kwargs\": {},\n","    \"output_write_episodes\": True,\n","    \"offline_sampling\": False,\n","    \"evaluation_interval\": 10,\n","    \"evaluation_duration\": 3,\n","    \"evaluation_duration_unit\": \"episodes\",\n","    \"evaluation_sample_timeout_s\": 120.0,\n","    \"evaluation_parallel_to_training\": False,\n","    \"evaluation_force_reset_envs_before_iteration\": True,\n","    \"evaluation_config\": None,\n","    \"off_policy_estimation_methods\": {},\n","    \"ope_split_batch_by_episode\": True,\n","    \"evaluation_num_env_runners\": 0,\n","    \"custom_evaluation_function\": None,\n","    \"in_evaluation\": False,\n","    \"sync_filters_on_rollout_workers_timeout_s\": 10.0,\n","    \"keep_per_episode_custom_metrics\": False,\n","    \"metrics_episode_collection_timeout_s\": 60.0,\n","    \"metrics_num_episodes_for_smoothing\": 100,\n","    \"min_time_s_per_iteration\": None,\n","    \"min_train_timesteps_per_iteration\": 0,\n","    \"min_sample_timesteps_per_iteration\": 0,\n","    \"log_gradients\": True,\n","    \"export_native_model_files\": False,\n","    \"checkpoint_trainable_policies_only\": False,\n","    \"logger_creator\": None,\n","    \"logger_config\": None,\n","    \"log_level\": \"WARN\",\n","    \"log_sys_usage\": True,\n","    \"fake_sampler\": False,\n","    \"seed\": None,\n","    \"_run_training_always_in_thread\": False,\n","    \"_evaluation_parallel_to_training_wo_thread\": False,\n","    \"restart_failed_env_runners\": True,\n","    \"ignore_env_runner_failures\": False,\n","    \"max_num_env_runner_restarts\": 1000,\n","    \"delay_between_env_runner_restarts_s\": 60.0,\n","    \"restart_failed_sub_environments\": False,\n","    \"num_consecutive_env_runner_failures_tolerance\": 100,\n","    \"env_runner_health_probe_timeout_s\": 30.0,\n","    \"env_runner_restore_timeout_s\": 1800.0,\n","    \"_model_config\": {},\n","    \"_rl_module_spec\": None,\n","    \"algorithm_config_overrides_per_module\": {},\n","    \"_per_module_overrides\": {},\n","    \"_torch_grad_scaler_class\": None,\n","    \"_torch_lr_scheduler_classes\": None,\n","    \"_tf_policy_handles_more_than_one_loss\": False,\n","    \"_disable_preprocessor_api\": False,\n","    \"_disable_action_flattening\": False,\n","    \"_disable_initialize_loss_from_dummy_batch\": False,\n","    \"_dont_auto_sync_env_runner_states\": False,\n","    \"_is_frozen\": True,\n","    \"enable_connectors\": -1,\n","    \"simple_optimizer\": False,\n","    \"monitor\": -1,\n","    \"evaluation_num_episodes\": -1,\n","    \"metrics_smoothing_episodes\": -1,\n","    \"timesteps_per_iteration\": -1,\n","    \"min_iter_time_s\": -1,\n","    \"collect_metrics_timeout\": -1,\n","    \"min_time_s_per_reporting\": -1,\n","    \"min_train_timesteps_per_reporting\": -1,\n","    \"min_sample_timesteps_per_reporting\": -1,\n","    \"input_evaluation\": -1,\n","    \"policy_map_cache\": -1,\n","    \"worker_cls\": -1,\n","    \"synchronize_filters\": -1,\n","    \"enable_async_evaluation\": -1,\n","    \"custom_async_evaluation_function\": -1,\n","    \"_enable_rl_module_api\": -1,\n","    \"auto_wrap_old_gym_envs\": -1,\n","    \"always_attach_evaluation_results\": -1,\n","    \"buffer_size\": -1,\n","    \"prioritized_replay\": -1,\n","    \"learning_starts\": -1,\n","    \"replay_batch_size\": -1,\n","    \"replay_sequence_length\": None,\n","    \"replay_mode\": -1,\n","    \"prioritized_replay_alpha\": -1,\n","    \"prioritized_replay_beta\": -1,\n","    \"prioritized_replay_eps\": -1,\n","    \"_disable_execution_plan_api\": -1,\n","    \"use_critic\": True,\n","    \"use_gae\": True,\n","    \"lambda_\": 1.0,\n","    \"use_kl_loss\": True,\n","    \"kl_coeff\": 0.2,\n","    \"kl_target\": 0.01,\n","    \"vf_loss_coeff\": 1.0,\n","    \"entropy_coeff\": 0.0,\n","    \"clip_param\": 0.3,\n","    \"vf_clip_param\": 10.0,\n","    \"entropy_coeff_schedule\": None,\n","    \"lr_schedule\": None,\n","    \"sgd_minibatch_size\": -1,\n","    \"vf_share_layers\": -1\n","}\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30823,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"}},"nbformat":4,"nbformat_minor":4}
