{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r\"..\")\n",
    "from easy_helper import simplify_rllib_metrics\n",
    "from ray.rllib.algorithms.ppo import PPOConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自定义环境\n",
    "使用的数据分为 训练数据集/验证数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "class CartPoleEnv(gym.Env):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        CartPole 环境\n",
    "        \"\"\"\n",
    "        self.env = gym.make('CartPole-v1', *args, **kwargs)\n",
    "        self.observation_space = self.env.observation_space\n",
    "        self.action_space = self.env.action_space\n",
    "\n",
    "        # 数据类型 train/val\n",
    "        print('init data type train')\n",
    "        self.data_type = 'train'\n",
    "\n",
    "    def val(self):\n",
    "        print(f'set data type to val')\n",
    "        self.data_type = 'val'\n",
    "    \n",
    "    def train(self):\n",
    "        print(f'set data type to train')\n",
    "        self.data_type = 'train'\n",
    "\n",
    "    def reset(self, *args, **kwargs):\n",
    "        return self.env.reset(*args, **kwargs)\n",
    "\n",
    "    def step(self, *args, **kwargs):\n",
    "        return self.env.step(*args, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 注册环境\n",
    "from ray.tune.registry import register_env\n",
    "register_env('test_CartPole-v1', lambda config={}: CartPoleEnv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.callbacks import DefaultCallbacks\n",
    "\n",
    "def _get_env(env):\n",
    "    while not isinstance(env, CartPoleEnv):\n",
    "        env = env.unwrapped\n",
    "    return env\n",
    "\n",
    "class TestCallbacks(DefaultCallbacks):\n",
    "\n",
    "    def on_evaluate_start(self, *args, **kwargs):\n",
    "        print('on_evaluate_start')\n",
    "        # 获取 eval_env_runner\n",
    "        algo = kwargs['algorithm'] \n",
    "        if algo.eval_env_runner_group is None:\n",
    "            eval_env_runner = algo.env_runner_group.local_env_runner\n",
    "        else:\n",
    "            eval_env_runner = algo.eval_env_runner\n",
    "        # 切换环境到 val模式\n",
    "        for env in eval_env_runner.env.unwrapped.envs:\n",
    "            _env = _get_env(env)\n",
    "            _env.val()\n",
    "\n",
    "    def on_evaluate_end(self, *args, **kwargs):\n",
    "        print('on_evaluate_end')\n",
    "        # 获取 eval_env_runner\n",
    "        algo = kwargs['algorithm'] \n",
    "        if algo.eval_env_runner_group is None:\n",
    "            eval_env_runner = algo.env_runner_group.local_env_runner\n",
    "            # 只有本地 eval_env_runner 需要切换回 train模式\n",
    "            for env in eval_env_runner.env.unwrapped.envs:\n",
    "                _env = _get_env(env)\n",
    "                _env.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练\n",
    "再验证环节需要切换环境的数据类型 > 验证数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-14 12:16:57,575\tWARNING ppo.py:295 -- You are running PPO on the new API stack! This is the new default behavior for this algorithm. If you don't want to use the new API stack, set `config.api_stack(enable_rl_module_and_learner=False,enable_env_runner_and_connector_v2=False)`. For a detailed migration guide, see here: https://docs.ray.io/en/master/rllib/new-api-stack-migration-guide.html\n",
      "d:\\programs\\miniconda3\\Lib\\site-packages\\ray\\rllib\\algorithms\\algorithm.py:569: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "`UnifiedLogger` will be removed in Ray 2.7.\n",
      "  return UnifiedLogger(config, logdir, loggers=None)\n",
      "d:\\programs\\miniconda3\\Lib\\site-packages\\ray\\tune\\logger\\unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "d:\\programs\\miniconda3\\Lib\\site-packages\\ray\\tune\\logger\\unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "d:\\programs\\miniconda3\\Lib\\site-packages\\ray\\tune\\logger\\unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "d:\\programs\\miniconda3\\Lib\\site-packages\\gymnasium\\envs\\registration.py:642: UserWarning: \u001b[33mWARN: Overriding environment rllib-single-agent-env-v0 already in registry.\u001b[0m\n",
      "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init data type train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-14 12:16:57,851\tWARNING ppo.py:295 -- You are running PPO on the new API stack! This is the new default behavior for this algorithm. If you don't want to use the new API stack, set `config.api_stack(enable_rl_module_and_learner=False,enable_env_runner_and_connector_v2=False)`. For a detailed migration guide, see here: https://docs.ray.io/en/master/rllib/new-api-stack-migration-guide.html\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init data type train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-14 12:16:58,236\tWARNING rl_module.py:427 -- Could not create a Catalog object for your RLModule! If you are not using the new API stack yet, make sure to switch it off in your config: `config.api_stack(enable_rl_module_and_learner=False, enable_env_runner_and_connector_v2=False)`. Some algos already use the new stack by default. Ignore this message, if your RLModule does not use a Catalog to build its sub-components.\n",
      "2025-02-14 12:16:58,285\tWARNING util.py:61 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on_evaluate_start\n",
      "set data type to val\n",
      "on_evaluate_end\n",
      "--------- 训练迭代: 1 ---------\n",
      "环境运行器:\n",
      "  episode平均回报: 24.1700\n",
      "  episode最大回报: 70.0000\n",
      "  episode平均步数: 24.1700\n",
      "  episode最大步数: 70.0000\n",
      "  采样环境总步数: 4000.0000\n",
      "  episodes计数: 177.0000\n",
      "\n",
      "评估:\n",
      "  episode平均回报: 19.0000\n",
      "  episode最大回报: 19.0000\n",
      "  episode平均步数: 19.0000\n",
      "  episode最大步数: 19.0000\n",
      "  无评估数据\n",
      "\n",
      "学习者(默认策略):\n",
      "  熵: 0.6719\n",
      "  策略损失: -0.0457\n",
      "  值函数损失: 6.6426\n",
      "  总损失: 6.6014\n",
      "\n",
      "本轮时间: 23.1346\n",
      "每轮训练步数: 1\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'环境运行器': {'episode平均回报': 24.17,\n",
       "  'episode最大回报': 70.0,\n",
       "  'episode平均步数': 24.17,\n",
       "  'episode最大步数': 70,\n",
       "  '采样环境总步数': 4000,\n",
       "  'episodes计数': 177},\n",
       " '评估': {'episode平均回报': 19.0,\n",
       "  'episode最大回报': 19.0,\n",
       "  'episode平均步数': 19.0,\n",
       "  'episode最大步数': 19},\n",
       " '学习者': {'默认策略': {'熵': 0.6718525886535645,\n",
       "   '策略损失': -0.045668601989746094,\n",
       "   '值函数损失': 6.642553329467773,\n",
       "   '总损失': 6.601372241973877}},\n",
       " '本轮时间': 23.1346492767334,\n",
       " '每轮训练步数': 1,\n",
       " '训练迭代次数': 1}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = (\n",
    "    PPOConfig()\n",
    "    .api_stack(\n",
    "        enable_rl_module_and_learner=True,\n",
    "        enable_env_runner_and_connector_v2=True,\n",
    "    )\n",
    "    .environment(\"test_CartPole-v1\")\n",
    "    .evaluation(\n",
    "        evaluation_interval=1,\n",
    "        evaluation_duration=1,\n",
    "    )\n",
    "    .callbacks(TestCallbacks)\n",
    ")\n",
    "\n",
    "algo = config.build()\n",
    "simplify_rllib_metrics(algo.train())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
