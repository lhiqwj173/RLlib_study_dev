{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from dl_helper.rl.rl_env.breakout_env import BreakoutEnv# 自定义环境\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.tune.registry import get_trainable_cls, register_env\n",
    "\n",
    "# # 注册环境\n",
    "# register_env(\"breakout\", lambda config: BreakoutEnv())\n",
    "\n",
    "config = (\n",
    "    PPOConfig()\n",
    "    .api_stack(\n",
    "        enable_rl_module_and_learner=True,\n",
    "        enable_env_runner_and_connector_v2=True,\n",
    "    )\n",
    "    .environment(\"breakout\")\n",
    "    # .environment(\"CartPole-v1\")\n",
    "    .env_runners(num_env_runners=1)\n",
    "    .rl_module(\n",
    "        model_config={\n",
    "            \"conv_filters\": [\n",
    "                [32, [8, 8], 4],  # [输出通道数, [kernel_size_h, kernel_size_w], stride]\n",
    "                [64, [4, 4], 2],  # [64个通道, 4x4卷积核, stride=2]\n",
    "                [64, [3, 3], 1],  # [64个通道, 3x3卷积核, stride=1] \n",
    "            ],\n",
    "        },\n",
    "    )\n",
    "    .learners(    \n",
    "        num_learners=1,\n",
    "        num_gpus_per_learner=1,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mInit signature:\u001b[0m\n",
      "\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malgo_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mconfig\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malgorithms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malgorithm_config\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAlgorithmConfig\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0menv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mlogger_creator\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mCallable\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtune\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLogger\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDocstring:\u001b[0m     \n",
      "An RLlib algorithm responsible for optimizing one or more Policies.\n",
      "\n",
      "Algorithms contain a EnvRunnerGroup under `self.env_runner_group`. An EnvRunnerGroup\n",
      "is composed of a single local EnvRunner (`self.env_runner_group.local_env_runner`),\n",
      "serving as the reference copy of the NeuralNetwork(s) to be trained and optionally\n",
      "one or more remote EnvRunners used to generate environment samples in parallel.\n",
      "EnvRunnerGroup is fault-tolerant and elastic. It tracks health states for all\n",
      "the managed remote EnvRunner actors. As a result, Algorithm should never\n",
      "access the underlying actor handles directly. Instead, always access them\n",
      "via all the foreach APIs with assigned IDs of the underlying EnvRunners.\n",
      "\n",
      "Each EnvRunners (remotes or local) contains a PolicyMap, which itself\n",
      "may contain either one policy for single-agent training or one or more\n",
      "policies for multi-agent training. Policies are synchronized\n",
      "automatically from time to time using ray.remote calls. The exact\n",
      "synchronization logic depends on the specific algorithm used,\n",
      "but this usually happens from local worker to all remote workers and\n",
      "after each training update.\n",
      "\n",
      "You can write your own Algorithm classes by sub-classing from `Algorithm`\n",
      "or any of its built-in sub-classes.\n",
      "This allows you to override the `training_step` method to implement\n",
      "your own algorithm logic. You can find the different built-in\n",
      "algorithms' `training_step()` methods in their respective main .py files,\n",
      "e.g. rllib.algorithms.dqn.dqn.py or rllib.algorithms.impala.impala.py.\n",
      "\n",
      "The most important API methods a Algorithm exposes are `train()`,\n",
      "`evaluate()`, `save_to_path()` and `restore_from_path()`.\n",
      "\u001b[1;31mSource:\u001b[0m        \n",
      "\u001b[1;32mclass\u001b[0m \u001b[0mPPO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAlgorithm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;33m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;33m@\u001b[0m\u001b[0moverride\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAlgorithm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0mget_default_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mAlgorithmConfig\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;32mreturn\u001b[0m \u001b[0mPPOConfig\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;33m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;33m@\u001b[0m\u001b[0moverride\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAlgorithm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0mget_default_policy_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAlgorithmConfig\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mType\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mPolicy\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"framework\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"torch\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;32mfrom\u001b[0m \u001b[0mray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malgorithms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mppo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mppo_torch_policy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPPOTorchPolicy\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;32mreturn\u001b[0m \u001b[0mPPOTorchPolicy\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;32melif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"framework\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"tf\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;32mfrom\u001b[0m \u001b[0mray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malgorithms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mppo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mppo_tf_policy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPPOTF1Policy\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;32mreturn\u001b[0m \u001b[0mPPOTF1Policy\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;32mfrom\u001b[0m \u001b[0mray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malgorithms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mppo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mppo_tf_policy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPPOTF2Policy\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;32mreturn\u001b[0m \u001b[0mPPOTF2Policy\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;33m@\u001b[0m\u001b[0moverride\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAlgorithm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;31m# Old API stack (Policy, RolloutWorker, Connector).\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable_env_runner_and_connector_v2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_training_step_old_api_stack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;31m# Collect batches from sample workers until we have a full batch.\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_time\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTIMERS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mENV_RUNNER_SAMPLING_TIMER\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;31m# Sample in parallel from the workers.\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount_steps_by\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"agent_steps\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[0mepisodes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv_runner_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msynchronous_parallel_sample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                    \u001b[0mworker_set\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv_runner_group\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                    \u001b[0mmax_agent_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal_train_batch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                    \u001b[0msample_timeout_s\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_timeout_s\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                    \u001b[0m_uses_new_env_runners\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                        \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable_env_runner_and_connector_v2\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                    \u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                    \u001b[0m_return_metrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[0mepisodes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv_runner_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msynchronous_parallel_sample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                    \u001b[0mworker_set\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv_runner_group\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                    \u001b[0mmax_env_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal_train_batch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                    \u001b[0msample_timeout_s\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_timeout_s\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                    \u001b[0m_uses_new_env_runners\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                        \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable_env_runner_and_connector_v2\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                    \u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                    \u001b[0m_return_metrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;31m# Return early if all our workers failed.\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mepisodes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;32mreturn\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;31m# Reduce EnvRunner metrics over the n EnvRunners.\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge_and_log_n_dicts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[0menv_runner_results\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mENV_RUNNER_RESULTS\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;31m# Perform a learner update step on the collected episodes.\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_time\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTIMERS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLEARNER_UPDATE_TIMER\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0mlearner_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearner_group\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_from_episodes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[0mepisodes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepisodes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[0mtimesteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                    \u001b[0mNUM_ENV_STEPS_SAMPLED_LIFETIME\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                        \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpeek\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                            \u001b[1;33m(\u001b[0m\u001b[0mENV_RUNNER_RESULTS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNUM_ENV_STEPS_SAMPLED_LIFETIME\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                        \u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                    \u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[0mnum_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[0mminibatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminibatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[0mshuffle_batch_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshuffle_batch_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge_and_log_n_dicts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearner_results\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mLEARNER_RESULTS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;31m# Update weights - after learning on the local worker - on all remote\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;31m# workers.\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_time\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTIMERS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSYNCH_WORKER_WEIGHTS_TIMER\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;31m# The train results's loss keys are ModuleIDs to their loss values.\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;31m# But we also return a total_loss key at the same level as the ModuleID\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;31m# keys. So we need to subtract that to get the correct set of ModuleIDs to\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;31m# update.\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;31m# TODO (sven): We should also not be using `learner_results` as a messenger\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;31m#  to infer which modules to update. `policies_to_train` might also NOT work\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;31m#  as it might be a very large set (100s of Modules) vs a smaller Modules\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;31m#  set that's present in the current train batch.\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0mmodules_to_update\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearner_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mALL_MODULES\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv_runner_group\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msync_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;31m# Sync weights from learner_group to all EnvRunners.\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[0mfrom_worker_or_learner_group\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearner_group\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[0mpolicies\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodules_to_update\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[0minference_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;33m@\u001b[0m\u001b[0mOldAPIStack\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0m_training_step_old_api_stack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mResultDict\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;31m# Collect batches from sample workers until we have a full batch.\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_timers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mSAMPLE_TIMER\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount_steps_by\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"agent_steps\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[0mtrain_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msynchronous_parallel_sample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                    \u001b[0mworker_set\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv_runner_group\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                    \u001b[0mmax_agent_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal_train_batch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                    \u001b[0msample_timeout_s\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_timeout_s\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[0mtrain_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msynchronous_parallel_sample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                    \u001b[0mworker_set\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv_runner_group\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                    \u001b[0mmax_env_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal_train_batch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                    \u001b[0msample_timeout_s\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_timeout_s\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;31m# Return early if all our workers failed.\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtrain_batch\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;32mreturn\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0mtrain_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_batch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_multi_agent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_counters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mNUM_AGENT_STEPS_SAMPLED\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtrain_batch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magent_steps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_counters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mNUM_ENV_STEPS_SAMPLED\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtrain_batch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv_steps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;31m# Standardize advantages.\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0mtrain_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstandardize_fields\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"advantages\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimple_optimizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0mtrain_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_one_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0mtrain_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmulti_gpu_train_one_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[0mpolicies_to_update\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_results\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[0mglobal_vars\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;34m\"timestep\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_counters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mNUM_AGENT_STEPS_SAMPLED\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;31m# TODO (sven): num_grad_updates per each policy should be\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;31m#  accessible via `train_results` (and get rid of global_vars).\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;34m\"num_grad_updates_per_policy\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[0mpid\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv_runner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolicy_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpid\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_grad_updates\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;32mfor\u001b[0m \u001b[0mpid\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpolicies_to_update\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;33m}\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;31m# Update weights - after learning on the local worker - on all remote\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;31m# workers.\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_timers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mSYNCH_WORKER_WEIGHTS_TIMER\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv_runner_group\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_remote_workers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[0mfrom_worker_or_learner_group\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv_runner_group\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msync_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                    \u001b[0mfrom_worker_or_learner_group\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfrom_worker_or_learner_group\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                    \u001b[0mpolicies\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpolicies_to_update\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                    \u001b[0mglobal_vars\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mglobal_vars\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;31m# For each policy: Update KL scale and warn about possible issues\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;32mfor\u001b[0m \u001b[0mpolicy_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicy_info\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_results\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;31m# Update KL loss with dynamic scaling\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;31m# for each (possibly multiagent) policy we are training\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0mkl_divergence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpolicy_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mLEARNER_STATS_KEY\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"kl\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_policy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpolicy_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_kl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkl_divergence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;31m# Warn about excessively high value function loss\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0mscaled_vf_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvf_loss_coeff\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mpolicy_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mLEARNER_STATS_KEY\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"vf_loss\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0mpolicy_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpolicy_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mLEARNER_STATS_KEY\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"policy_loss\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[0mlog_once\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ppo_warned_lr_ratio\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"model\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"vf_share_layers\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;32mand\u001b[0m \u001b[0mscaled_vf_loss\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                    \u001b[1;34m\"The magnitude of your value function loss for policy: {} is \"\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                    \u001b[1;34m\"extremely large ({}) compared to the policy loss ({}). This \"\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                    \u001b[1;34m\"can prevent the policy from learning. Consider scaling down \"\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                    \u001b[1;34m\"the VF loss by reducing vf_loss_coeff, or disabling \"\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                    \u001b[1;34m\"vf_share_layers.\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpolicy_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscaled_vf_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicy_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;31m# Warn about bad clipping configs.\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0mtrain_batch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolicy_batches\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpolicy_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_get_interceptor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0mmean_reward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_batch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolicy_batches\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpolicy_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"rewards\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[0mlog_once\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ppo_warned_vf_clip\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;32mand\u001b[0m \u001b[0mmean_reward\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvf_clip_param\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarned_vf_clip\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                    \u001b[1;33mf\"\u001b[0m\u001b[1;33mThe mean reward returned from the environment is \u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mmean_reward\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\"\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                    \u001b[1;33mf\"\u001b[0m\u001b[1;33m but the vf_clip_param is set to \u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'vf_clip_param'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\"\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                    \u001b[1;33mf\"\u001b[0m\u001b[1;33m Consider increasing it for policy: \u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mpolicy_id\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m to improve\u001b[0m\u001b[1;33m\"\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                    \u001b[1;34m\" value function convergence.\"\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;31m# Update global vars on local worker as well.\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;31m# TODO (simon): At least in RolloutWorker obsolete I guess as called in\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;31m#  `sync_weights()` called above if remote workers. Can we call this\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;31m#  where `set_weights()` is called on the local_worker?\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv_runner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_global_vars\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mglobal_vars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;32mreturn\u001b[0m \u001b[0mtrain_results\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFile:\u001b[0m           d:\\programs\\miniconda3\\lib\\site-packages\\ray\\rllib\\algorithms\\ppo\\ppo.py\n",
      "\u001b[1;31mType:\u001b[0m           ABCMeta\n",
      "\u001b[1;31mSubclasses:\u001b[0m     "
     ]
    }
   ],
   "source": [
    "config.algo_class??\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DEFAULT_AGENT_TO_MODULE_MAPPING_FN',\n",
       " 'DEFAULT_POLICY_MAPPING_FN',\n",
       " '__abstractmethods__',\n",
       " '__class__',\n",
       " '__contains__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setitem__',\n",
       " '__sizeof__',\n",
       " '__slots__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_abc_impl',\n",
       " '_check_if_correct_nn_framework_installed',\n",
       " '_disable_action_flattening',\n",
       " '_disable_execution_plan_api',\n",
       " '_disable_initialize_loss_from_dummy_batch',\n",
       " '_disable_preprocessor_api',\n",
       " '_dont_auto_sync_env_runner_states',\n",
       " '_enable_new_api_stack',\n",
       " '_enable_rl_module_api',\n",
       " '_env_to_module_connector',\n",
       " '_evaluation_parallel_to_training_wo_thread',\n",
       " '_fake_gpus',\n",
       " '_is_atari',\n",
       " '_is_frozen',\n",
       " '_learner_class',\n",
       " '_learner_connector',\n",
       " '_model_config',\n",
       " '_model_config_auto_includes',\n",
       " '_module_to_env_connector',\n",
       " '_per_module_overrides',\n",
       " '_prior_exploration_config',\n",
       " '_resolve_tf_settings',\n",
       " '_rl_module_spec',\n",
       " '_run_training_always_in_thread',\n",
       " '_serialize_dict',\n",
       " '_tf_policy_handles_more_than_one_loss',\n",
       " '_torch_grad_scaler_class',\n",
       " '_torch_lr_scheduler_classes',\n",
       " '_translate_special_keys',\n",
       " '_validate_evaluation_settings',\n",
       " '_validate_framework_settings',\n",
       " '_validate_input_settings',\n",
       " '_validate_multi_agent_settings',\n",
       " '_validate_new_api_stack_settings',\n",
       " '_validate_offline_settings',\n",
       " '_validate_resources_settings',\n",
       " '_validate_to_be_deprecated_settings',\n",
       " 'action_mask_key',\n",
       " 'action_space',\n",
       " 'actions_in_input_normalized',\n",
       " 'add_default_connectors_to_env_to_module_pipeline',\n",
       " 'add_default_connectors_to_learner_pipeline',\n",
       " 'add_default_connectors_to_module_to_env_pipeline',\n",
       " 'algo_class',\n",
       " 'algorithm_config_overrides_per_module',\n",
       " 'always_attach_evaluation_results',\n",
       " 'api_stack',\n",
       " 'auto_wrap_old_gym_envs',\n",
       " 'batch_mode',\n",
       " 'buffer_size',\n",
       " 'build',\n",
       " 'build_env_to_module_connector',\n",
       " 'build_learner',\n",
       " 'build_learner_connector',\n",
       " 'build_learner_group',\n",
       " 'build_module_to_env_connector',\n",
       " 'callbacks',\n",
       " 'callbacks_class',\n",
       " 'checkpoint_trainable_policies_only',\n",
       " 'checkpointing',\n",
       " 'clip_actions',\n",
       " 'clip_param',\n",
       " 'clip_rewards',\n",
       " 'collect_metrics_timeout',\n",
       " 'compress_observations',\n",
       " 'copy',\n",
       " 'count_steps_by',\n",
       " 'create_env_on_local_worker',\n",
       " 'custom_async_evaluation_function',\n",
       " 'custom_evaluation_function',\n",
       " 'custom_resources_per_env_runner',\n",
       " 'custom_resources_per_worker',\n",
       " 'dataset_num_iters_per_learner',\n",
       " 'debugging',\n",
       " 'delay_between_env_runner_restarts_s',\n",
       " 'delay_between_worker_restarts_s',\n",
       " 'disable_env_checking',\n",
       " 'eager_max_retraces',\n",
       " 'eager_tracing',\n",
       " 'enable_async_evaluation',\n",
       " 'enable_connectors',\n",
       " 'enable_env_runner_and_connector_v2',\n",
       " 'enable_rl_module_and_learner',\n",
       " 'enable_tf1_exec_eagerly',\n",
       " 'entropy_coeff',\n",
       " 'entropy_coeff_schedule',\n",
       " 'env',\n",
       " 'env_config',\n",
       " 'env_runner_cls',\n",
       " 'env_runner_health_probe_timeout_s',\n",
       " 'env_runner_restore_timeout_s',\n",
       " 'env_runners',\n",
       " 'env_task_fn',\n",
       " 'environment',\n",
       " 'episode_lookback_horizon',\n",
       " 'evaluation',\n",
       " 'evaluation_config',\n",
       " 'evaluation_duration',\n",
       " 'evaluation_duration_unit',\n",
       " 'evaluation_force_reset_envs_before_iteration',\n",
       " 'evaluation_interval',\n",
       " 'evaluation_num_env_runners',\n",
       " 'evaluation_num_episodes',\n",
       " 'evaluation_num_workers',\n",
       " 'evaluation_parallel_to_training',\n",
       " 'evaluation_sample_timeout_s',\n",
       " 'experimental',\n",
       " 'exploration',\n",
       " 'exploration_config',\n",
       " 'explore',\n",
       " 'export_native_model_files',\n",
       " 'extra_python_environs_for_driver',\n",
       " 'extra_python_environs_for_worker',\n",
       " 'fake_sampler',\n",
       " 'fault_tolerance',\n",
       " 'framework',\n",
       " 'framework_str',\n",
       " 'freeze',\n",
       " 'from_dict',\n",
       " 'from_state',\n",
       " 'gamma',\n",
       " 'get',\n",
       " 'get_config_for_module',\n",
       " 'get_default_learner_class',\n",
       " 'get_default_rl_module_spec',\n",
       " 'get_evaluation_config_object',\n",
       " 'get_marl_module_spec',\n",
       " 'get_multi_agent_setup',\n",
       " 'get_multi_rl_module_spec',\n",
       " 'get_rl_module_spec',\n",
       " 'get_rollout_fragment_length',\n",
       " 'get_state',\n",
       " 'get_torch_compile_worker_config',\n",
       " 'grad_clip',\n",
       " 'grad_clip_by',\n",
       " 'ignore_env_runner_failures',\n",
       " 'ignore_worker_failures',\n",
       " 'in_evaluation',\n",
       " 'input_',\n",
       " 'input_compress_columns',\n",
       " 'input_config',\n",
       " 'input_evaluation',\n",
       " 'input_filesystem',\n",
       " 'input_filesystem_kwargs',\n",
       " 'input_read_batch_size',\n",
       " 'input_read_episodes',\n",
       " 'input_read_method',\n",
       " 'input_read_method_kwargs',\n",
       " 'input_read_sample_batches',\n",
       " 'input_read_schema',\n",
       " 'input_spaces_jsonable',\n",
       " 'is_atari',\n",
       " 'is_multi_agent',\n",
       " 'items',\n",
       " 'iter_batches_kwargs',\n",
       " 'keep_per_episode_custom_metrics',\n",
       " 'keys',\n",
       " 'kl_coeff',\n",
       " 'kl_target',\n",
       " 'lambda_',\n",
       " 'learner_class',\n",
       " 'learner_config_dict',\n",
       " 'learners',\n",
       " 'learning_starts',\n",
       " 'local_gpu_idx',\n",
       " 'local_tf_session_args',\n",
       " 'log_gradients',\n",
       " 'log_level',\n",
       " 'log_sys_usage',\n",
       " 'logger_config',\n",
       " 'logger_creator',\n",
       " 'lr',\n",
       " 'lr_schedule',\n",
       " 'map_batches_kwargs',\n",
       " 'materialize_data',\n",
       " 'materialize_mapped_data',\n",
       " 'max_num_env_runner_restarts',\n",
       " 'max_num_worker_restarts',\n",
       " 'max_requests_in_flight_per_env_runner',\n",
       " 'max_requests_in_flight_per_learner',\n",
       " 'metrics_episode_collection_timeout_s',\n",
       " 'metrics_num_episodes_for_smoothing',\n",
       " 'metrics_smoothing_episodes',\n",
       " 'min_iter_time_s',\n",
       " 'min_sample_timesteps_per_iteration',\n",
       " 'min_sample_timesteps_per_reporting',\n",
       " 'min_time_s_per_iteration',\n",
       " 'min_time_s_per_reporting',\n",
       " 'min_train_timesteps_per_iteration',\n",
       " 'min_train_timesteps_per_reporting',\n",
       " 'minibatch_size',\n",
       " 'model',\n",
       " 'model_config',\n",
       " 'monitor',\n",
       " 'multi_agent',\n",
       " 'normalize_actions',\n",
       " 'num_consecutive_env_runner_failures_tolerance',\n",
       " 'num_consecutive_worker_failures_tolerance',\n",
       " 'num_cpus_for_local_worker',\n",
       " 'num_cpus_for_main_process',\n",
       " 'num_cpus_per_env_runner',\n",
       " 'num_cpus_per_learner',\n",
       " 'num_cpus_per_learner_worker',\n",
       " 'num_cpus_per_worker',\n",
       " 'num_env_runners',\n",
       " 'num_envs_per_env_runner',\n",
       " 'num_envs_per_worker',\n",
       " 'num_epochs',\n",
       " 'num_gpus',\n",
       " 'num_gpus_per_env_runner',\n",
       " 'num_gpus_per_learner',\n",
       " 'num_gpus_per_learner_worker',\n",
       " 'num_gpus_per_worker',\n",
       " 'num_learner_workers',\n",
       " 'num_learners',\n",
       " 'num_rollout_workers',\n",
       " 'observation_filter',\n",
       " 'observation_fn',\n",
       " 'observation_space',\n",
       " 'off_policy_estimation_methods',\n",
       " 'offline_data',\n",
       " 'offline_sampling',\n",
       " 'ope_split_batch_by_episode',\n",
       " 'optimizer',\n",
       " 'output',\n",
       " 'output_compress_columns',\n",
       " 'output_config',\n",
       " 'output_filesystem',\n",
       " 'output_filesystem_kwargs',\n",
       " 'output_max_file_size',\n",
       " 'output_max_rows_per_file',\n",
       " 'output_write_episodes',\n",
       " 'output_write_method',\n",
       " 'output_write_method_kwargs',\n",
       " 'overrides',\n",
       " 'placement_strategy',\n",
       " 'policies',\n",
       " 'policies_to_train',\n",
       " 'policy_map_cache',\n",
       " 'policy_map_capacity',\n",
       " 'policy_mapping_fn',\n",
       " 'policy_states_are_swappable',\n",
       " 'pop',\n",
       " 'postprocess_inputs',\n",
       " 'prelearner_buffer_class',\n",
       " 'prelearner_buffer_kwargs',\n",
       " 'prelearner_class',\n",
       " 'prelearner_module_synch_period',\n",
       " 'preprocessor_pref',\n",
       " 'prioritized_replay',\n",
       " 'prioritized_replay_alpha',\n",
       " 'prioritized_replay_beta',\n",
       " 'prioritized_replay_eps',\n",
       " 'python_environment',\n",
       " 'recreate_failed_env_runners',\n",
       " 'recreate_failed_workers',\n",
       " 'remote_env_batch_wait_ms',\n",
       " 'remote_worker_envs',\n",
       " 'render_env',\n",
       " 'replay_batch_size',\n",
       " 'replay_mode',\n",
       " 'replay_sequence_length',\n",
       " 'reporting',\n",
       " 'resources',\n",
       " 'restart_failed_env_runners',\n",
       " 'restart_failed_sub_environments',\n",
       " 'rl_module',\n",
       " 'rl_module_spec',\n",
       " 'rollout_fragment_length',\n",
       " 'rollouts',\n",
       " 'sample_collector',\n",
       " 'sample_timeout_s',\n",
       " 'sampler_perf_stats_ema_coef',\n",
       " 'seed',\n",
       " 'serialize',\n",
       " 'sgd_minibatch_size',\n",
       " 'shuffle_batch_per_epoch',\n",
       " 'shuffle_buffer_size',\n",
       " 'simple_optimizer',\n",
       " 'sync_filters_on_rollout_workers_timeout_s',\n",
       " 'synchronize_filters',\n",
       " 'tf_session_args',\n",
       " 'timesteps_per_iteration',\n",
       " 'to_dict',\n",
       " 'torch_compile_learner',\n",
       " 'torch_compile_learner_dynamo_backend',\n",
       " 'torch_compile_learner_dynamo_mode',\n",
       " 'torch_compile_learner_what_to_compile',\n",
       " 'torch_compile_worker',\n",
       " 'torch_compile_worker_dynamo_backend',\n",
       " 'torch_compile_worker_dynamo_mode',\n",
       " 'torch_ddp_kwargs',\n",
       " 'torch_skip_nan_gradients',\n",
       " 'total_train_batch_size',\n",
       " 'train_batch_size',\n",
       " 'train_batch_size_per_learner',\n",
       " 'training',\n",
       " 'update_from_dict',\n",
       " 'update_worker_filter_stats',\n",
       " 'use_critic',\n",
       " 'use_gae',\n",
       " 'use_kl_loss',\n",
       " 'use_worker_filter_stats',\n",
       " 'uses_new_env_runners',\n",
       " 'validate',\n",
       " 'validate_env_runners_after_construction',\n",
       " 'validate_train_batch_size_vs_rollout_fragment_length',\n",
       " 'validate_workers_after_construction',\n",
       " 'values',\n",
       " 'vf_clip_param',\n",
       " 'vf_loss_coeff',\n",
       " 'vf_share_layers',\n",
       " 'worker_cls',\n",
       " 'worker_health_probe_timeout_s',\n",
       " 'worker_restore_timeout_s']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "\n",
    "temp_dir = tempfile.gettempdir()\n",
    "dump_file = os.path.join(temp_dir, \"RLlib_config.pkl\")\n",
    "\n",
    "with open(dump_file, \"wb\") as f:\n",
    "    pickle.dump(config, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\programs\\miniconda3\\Lib\\site-packages\\ray\\rllib\\core\\learner\\learner_group.py:846: SyntaxWarning: invalid escape sequence '\\*'\n",
      "  \"\"\"Calls the given function on each Learner L with the args: (L, \\*\\*kwargs).\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pickle\n",
    "import os\n",
    "import tempfile\n",
    "from dl_helper.rl.rl_env.breakout_env import BreakoutEnv# 自定义环境\n",
    "\n",
    "sys.path.append(r\"..\")\n",
    "from easy_helper import simplify_rllib_metrics\n",
    "\n",
    "temp_dir = tempfile.gettempdir()\n",
    "dump_file = os.path.join(temp_dir, \"RLlib_config.pkl\")\n",
    "\n",
    "config = pickle.load(open(dump_file, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DEFAULT_AGENT_TO_MODULE_MAPPING_FN',\n",
       " 'DEFAULT_POLICY_MAPPING_FN',\n",
       " '__abstractmethods__',\n",
       " '__class__',\n",
       " '__contains__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setitem__',\n",
       " '__sizeof__',\n",
       " '__slots__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_abc_impl',\n",
       " '_check_if_correct_nn_framework_installed',\n",
       " '_disable_action_flattening',\n",
       " '_disable_execution_plan_api',\n",
       " '_disable_initialize_loss_from_dummy_batch',\n",
       " '_disable_preprocessor_api',\n",
       " '_dont_auto_sync_env_runner_states',\n",
       " '_enable_new_api_stack',\n",
       " '_enable_rl_module_api',\n",
       " '_env_to_module_connector',\n",
       " '_evaluation_parallel_to_training_wo_thread',\n",
       " '_fake_gpus',\n",
       " '_is_atari',\n",
       " '_is_frozen',\n",
       " '_learner_class',\n",
       " '_learner_connector',\n",
       " '_model_config',\n",
       " '_model_config_auto_includes',\n",
       " '_module_to_env_connector',\n",
       " '_per_module_overrides',\n",
       " '_prior_exploration_config',\n",
       " '_resolve_tf_settings',\n",
       " '_rl_module_spec',\n",
       " '_run_training_always_in_thread',\n",
       " '_serialize_dict',\n",
       " '_tf_policy_handles_more_than_one_loss',\n",
       " '_torch_grad_scaler_class',\n",
       " '_torch_lr_scheduler_classes',\n",
       " '_translate_special_keys',\n",
       " '_validate_evaluation_settings',\n",
       " '_validate_framework_settings',\n",
       " '_validate_input_settings',\n",
       " '_validate_multi_agent_settings',\n",
       " '_validate_new_api_stack_settings',\n",
       " '_validate_offline_settings',\n",
       " '_validate_resources_settings',\n",
       " '_validate_to_be_deprecated_settings',\n",
       " 'action_mask_key',\n",
       " 'action_space',\n",
       " 'actions_in_input_normalized',\n",
       " 'add_default_connectors_to_env_to_module_pipeline',\n",
       " 'add_default_connectors_to_learner_pipeline',\n",
       " 'add_default_connectors_to_module_to_env_pipeline',\n",
       " 'algo_class',\n",
       " 'algorithm_config_overrides_per_module',\n",
       " 'always_attach_evaluation_results',\n",
       " 'api_stack',\n",
       " 'auto_wrap_old_gym_envs',\n",
       " 'batch_mode',\n",
       " 'buffer_size',\n",
       " 'build',\n",
       " 'build_env_to_module_connector',\n",
       " 'build_learner',\n",
       " 'build_learner_connector',\n",
       " 'build_learner_group',\n",
       " 'build_module_to_env_connector',\n",
       " 'callbacks',\n",
       " 'callbacks_class',\n",
       " 'checkpoint_trainable_policies_only',\n",
       " 'checkpointing',\n",
       " 'clip_actions',\n",
       " 'clip_param',\n",
       " 'clip_rewards',\n",
       " 'collect_metrics_timeout',\n",
       " 'compress_observations',\n",
       " 'copy',\n",
       " 'count_steps_by',\n",
       " 'create_env_on_local_worker',\n",
       " 'custom_async_evaluation_function',\n",
       " 'custom_evaluation_function',\n",
       " 'custom_resources_per_env_runner',\n",
       " 'custom_resources_per_worker',\n",
       " 'dataset_num_iters_per_learner',\n",
       " 'debugging',\n",
       " 'delay_between_env_runner_restarts_s',\n",
       " 'delay_between_worker_restarts_s',\n",
       " 'disable_env_checking',\n",
       " 'eager_max_retraces',\n",
       " 'eager_tracing',\n",
       " 'enable_async_evaluation',\n",
       " 'enable_connectors',\n",
       " 'enable_env_runner_and_connector_v2',\n",
       " 'enable_rl_module_and_learner',\n",
       " 'enable_tf1_exec_eagerly',\n",
       " 'entropy_coeff',\n",
       " 'entropy_coeff_schedule',\n",
       " 'env',\n",
       " 'env_config',\n",
       " 'env_runner_cls',\n",
       " 'env_runner_health_probe_timeout_s',\n",
       " 'env_runner_restore_timeout_s',\n",
       " 'env_runners',\n",
       " 'env_task_fn',\n",
       " 'environment',\n",
       " 'episode_lookback_horizon',\n",
       " 'evaluation',\n",
       " 'evaluation_config',\n",
       " 'evaluation_duration',\n",
       " 'evaluation_duration_unit',\n",
       " 'evaluation_force_reset_envs_before_iteration',\n",
       " 'evaluation_interval',\n",
       " 'evaluation_num_env_runners',\n",
       " 'evaluation_num_episodes',\n",
       " 'evaluation_num_workers',\n",
       " 'evaluation_parallel_to_training',\n",
       " 'evaluation_sample_timeout_s',\n",
       " 'experimental',\n",
       " 'exploration',\n",
       " 'exploration_config',\n",
       " 'explore',\n",
       " 'export_native_model_files',\n",
       " 'extra_python_environs_for_driver',\n",
       " 'extra_python_environs_for_worker',\n",
       " 'fake_sampler',\n",
       " 'fault_tolerance',\n",
       " 'framework',\n",
       " 'framework_str',\n",
       " 'freeze',\n",
       " 'from_dict',\n",
       " 'from_state',\n",
       " 'gamma',\n",
       " 'get',\n",
       " 'get_config_for_module',\n",
       " 'get_default_learner_class',\n",
       " 'get_default_rl_module_spec',\n",
       " 'get_evaluation_config_object',\n",
       " 'get_marl_module_spec',\n",
       " 'get_multi_agent_setup',\n",
       " 'get_multi_rl_module_spec',\n",
       " 'get_rl_module_spec',\n",
       " 'get_rollout_fragment_length',\n",
       " 'get_state',\n",
       " 'get_torch_compile_worker_config',\n",
       " 'grad_clip',\n",
       " 'grad_clip_by',\n",
       " 'ignore_env_runner_failures',\n",
       " 'ignore_worker_failures',\n",
       " 'in_evaluation',\n",
       " 'input_',\n",
       " 'input_compress_columns',\n",
       " 'input_config',\n",
       " 'input_evaluation',\n",
       " 'input_filesystem',\n",
       " 'input_filesystem_kwargs',\n",
       " 'input_read_batch_size',\n",
       " 'input_read_episodes',\n",
       " 'input_read_method',\n",
       " 'input_read_method_kwargs',\n",
       " 'input_read_sample_batches',\n",
       " 'input_read_schema',\n",
       " 'input_spaces_jsonable',\n",
       " 'is_atari',\n",
       " 'is_multi_agent',\n",
       " 'items',\n",
       " 'iter_batches_kwargs',\n",
       " 'keep_per_episode_custom_metrics',\n",
       " 'keys',\n",
       " 'kl_coeff',\n",
       " 'kl_target',\n",
       " 'lambda_',\n",
       " 'learner_class',\n",
       " 'learner_config_dict',\n",
       " 'learners',\n",
       " 'learning_starts',\n",
       " 'local_gpu_idx',\n",
       " 'local_tf_session_args',\n",
       " 'log_gradients',\n",
       " 'log_level',\n",
       " 'log_sys_usage',\n",
       " 'logger_config',\n",
       " 'logger_creator',\n",
       " 'lr',\n",
       " 'lr_schedule',\n",
       " 'map_batches_kwargs',\n",
       " 'materialize_data',\n",
       " 'materialize_mapped_data',\n",
       " 'max_num_env_runner_restarts',\n",
       " 'max_num_worker_restarts',\n",
       " 'max_requests_in_flight_per_env_runner',\n",
       " 'max_requests_in_flight_per_learner',\n",
       " 'metrics_episode_collection_timeout_s',\n",
       " 'metrics_num_episodes_for_smoothing',\n",
       " 'metrics_smoothing_episodes',\n",
       " 'min_iter_time_s',\n",
       " 'min_sample_timesteps_per_iteration',\n",
       " 'min_sample_timesteps_per_reporting',\n",
       " 'min_time_s_per_iteration',\n",
       " 'min_time_s_per_reporting',\n",
       " 'min_train_timesteps_per_iteration',\n",
       " 'min_train_timesteps_per_reporting',\n",
       " 'minibatch_size',\n",
       " 'model',\n",
       " 'model_config',\n",
       " 'monitor',\n",
       " 'multi_agent',\n",
       " 'normalize_actions',\n",
       " 'num_consecutive_env_runner_failures_tolerance',\n",
       " 'num_consecutive_worker_failures_tolerance',\n",
       " 'num_cpus_for_local_worker',\n",
       " 'num_cpus_for_main_process',\n",
       " 'num_cpus_per_env_runner',\n",
       " 'num_cpus_per_learner',\n",
       " 'num_cpus_per_learner_worker',\n",
       " 'num_cpus_per_worker',\n",
       " 'num_env_runners',\n",
       " 'num_envs_per_env_runner',\n",
       " 'num_envs_per_worker',\n",
       " 'num_epochs',\n",
       " 'num_gpus',\n",
       " 'num_gpus_per_env_runner',\n",
       " 'num_gpus_per_learner',\n",
       " 'num_gpus_per_learner_worker',\n",
       " 'num_gpus_per_worker',\n",
       " 'num_learner_workers',\n",
       " 'num_learners',\n",
       " 'num_rollout_workers',\n",
       " 'observation_filter',\n",
       " 'observation_fn',\n",
       " 'observation_space',\n",
       " 'off_policy_estimation_methods',\n",
       " 'offline_data',\n",
       " 'offline_sampling',\n",
       " 'ope_split_batch_by_episode',\n",
       " 'optimizer',\n",
       " 'output',\n",
       " 'output_compress_columns',\n",
       " 'output_config',\n",
       " 'output_filesystem',\n",
       " 'output_filesystem_kwargs',\n",
       " 'output_max_file_size',\n",
       " 'output_max_rows_per_file',\n",
       " 'output_write_episodes',\n",
       " 'output_write_method',\n",
       " 'output_write_method_kwargs',\n",
       " 'overrides',\n",
       " 'placement_strategy',\n",
       " 'policies',\n",
       " 'policies_to_train',\n",
       " 'policy_map_cache',\n",
       " 'policy_map_capacity',\n",
       " 'policy_mapping_fn',\n",
       " 'policy_states_are_swappable',\n",
       " 'pop',\n",
       " 'postprocess_inputs',\n",
       " 'prelearner_buffer_class',\n",
       " 'prelearner_buffer_kwargs',\n",
       " 'prelearner_class',\n",
       " 'prelearner_module_synch_period',\n",
       " 'preprocessor_pref',\n",
       " 'prioritized_replay',\n",
       " 'prioritized_replay_alpha',\n",
       " 'prioritized_replay_beta',\n",
       " 'prioritized_replay_eps',\n",
       " 'python_environment',\n",
       " 'recreate_failed_env_runners',\n",
       " 'recreate_failed_workers',\n",
       " 'remote_env_batch_wait_ms',\n",
       " 'remote_worker_envs',\n",
       " 'render_env',\n",
       " 'replay_batch_size',\n",
       " 'replay_mode',\n",
       " 'replay_sequence_length',\n",
       " 'reporting',\n",
       " 'resources',\n",
       " 'restart_failed_env_runners',\n",
       " 'restart_failed_sub_environments',\n",
       " 'rl_module',\n",
       " 'rl_module_spec',\n",
       " 'rollout_fragment_length',\n",
       " 'rollouts',\n",
       " 'sample_collector',\n",
       " 'sample_timeout_s',\n",
       " 'sampler_perf_stats_ema_coef',\n",
       " 'seed',\n",
       " 'serialize',\n",
       " 'sgd_minibatch_size',\n",
       " 'shuffle_batch_per_epoch',\n",
       " 'shuffle_buffer_size',\n",
       " 'simple_optimizer',\n",
       " 'sync_filters_on_rollout_workers_timeout_s',\n",
       " 'synchronize_filters',\n",
       " 'tf_session_args',\n",
       " 'timesteps_per_iteration',\n",
       " 'to_dict',\n",
       " 'torch_compile_learner',\n",
       " 'torch_compile_learner_dynamo_backend',\n",
       " 'torch_compile_learner_dynamo_mode',\n",
       " 'torch_compile_learner_what_to_compile',\n",
       " 'torch_compile_worker',\n",
       " 'torch_compile_worker_dynamo_backend',\n",
       " 'torch_compile_worker_dynamo_mode',\n",
       " 'torch_ddp_kwargs',\n",
       " 'torch_skip_nan_gradients',\n",
       " 'total_train_batch_size',\n",
       " 'train_batch_size',\n",
       " 'train_batch_size_per_learner',\n",
       " 'training',\n",
       " 'update_from_dict',\n",
       " 'update_worker_filter_stats',\n",
       " 'use_critic',\n",
       " 'use_gae',\n",
       " 'use_kl_loss',\n",
       " 'use_worker_filter_stats',\n",
       " 'uses_new_env_runners',\n",
       " 'validate',\n",
       " 'validate_env_runners_after_construction',\n",
       " 'validate_train_batch_size_vs_rollout_fragment_length',\n",
       " 'validate_workers_after_construction',\n",
       " 'values',\n",
       " 'vf_clip_param',\n",
       " 'vf_loss_coeff',\n",
       " 'vf_share_layers',\n",
       " 'worker_cls',\n",
       " 'worker_health_probe_timeout_s',\n",
       " 'worker_restore_timeout_s']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = config.build()\n",
    "# EnvError: The env string you provided ('breakout') is:\n",
    "# 自定义的环境需要重新注册\n",
    "\n",
    "# for i in range(3):\n",
    "#     print(f\"第{i}次训练\")\n",
    "#     result = algo.train()\n",
    "#     simplify_rllib_metrics(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mSignature:\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDocstring:\u001b[0m <no docstring>\n",
      "\u001b[1;31mSource:\u001b[0m            \u001b[0mregister_env\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBreakoutEnv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mREG_NAME\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mBreakoutEnv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFile:\u001b[0m      d:\\code\\dl_helper\\dl_helper\\rl\\rl_env\\__init__.py\n",
      "\u001b[1;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "from ray.tune.registry import _global_registry, ENV_CREATOR\n",
    "\n",
    "# 获取注册的环境生成器\n",
    "env = _global_registry.get(ENV_CREATOR, \"breakout\")\n",
    "env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
